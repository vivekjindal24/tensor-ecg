{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fdc1b5b4945a97f",
   "metadata": {},
   "source": [
    "# Master ECG Pipeline\n",
    "\n",
    "This notebook combines all project scripts and modules into one single runnable file.\n",
    "\n",
    "**Usage:** run cells top-to-bottom. For headless execution on Windows use:\n",
    "\n",
    "```python\n",
    "import asyncio, sys\n",
    "if sys.platform == 'win32':\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bbe030653ed395",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.145113Z",
     "start_time": "2025-12-01T15:29:05.130813Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:20:52.569274Z",
     "iopub.status.busy": "2025-12-02T04:20:52.569274Z",
     "iopub.status.idle": "2025-12-02T04:20:59.902544Z",
     "shell.execute_reply": "2025-12-02T04:20:59.902544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: D:\\ecg-research\\.venv1\\Scripts\\python.exe\n",
      "Torch: 2.9.1+cpu\n",
      "DEVICE cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Environment & imports - idempotent\n",
    "import os, sys, json, time, math, asyncio\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "print('Python:', sys.executable)\n",
    "print('Torch:', getattr(torch, '__version__', 'n/a'))\n",
    "# Windows asyncio fix for nbconvert headless runs\n",
    "import platform\n",
    "if platform.system() == 'Windows':\n",
    "    try:\n",
    "        import asyncio, sys\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Project root detection\n",
    "ROOT = Path(os.environ.get('ECG_ROOT', Path.cwd().resolve()))\n",
    "DATASET_DIR = ROOT / 'Dataset'\n",
    "ARTIFACTS_DIR = ROOT / 'artifacts'\n",
    "PROCESSED_DIR = ARTIFACTS_DIR / 'processed'\n",
    "LOGS_DIR = ROOT / 'logs'\n",
    "NOTEBOOKS_DIR = ROOT / 'notebooks'\n",
    "for p in (ARTIFACTS_DIR, PROCESSED_DIR, PROCESSED_DIR/'records', LOGS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# seeds for reproducibility\n",
    "SEED = int(os.environ.get('ECG_SEED', '42'))\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('DEVICE', DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1218a398b2d98",
   "metadata": {},
   "source": [
    "## Inlined: Source modules (src/)\n",
    "\n",
    "Files: __init__.py, dataloaders.py, eval.py, generate_candidate_unified_mapping.py, model.py, preprocessing.py, saver.py, serving.py, training.py, utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e6b692dcb7dbad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.179210Z",
     "start_time": "2025-12-01T15:29:05.173874Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:20:59.913268Z",
     "iopub.status.busy": "2025-12-02T04:20:59.909679Z",
     "iopub.status.idle": "2025-12-02T04:20:59.928574Z",
     "shell.execute_reply": "2025-12-02T04:20:59.924551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module __init__.py raised attempted relative import with no known parent package\n"
     ]
    }
   ],
   "source": [
    "# --- __init__.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    ECG Research Pipeline\n",
    "    A unified multi-dataset ECG preprocessing, training, and serving pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    __version__ = \"0.1.0\"\n",
    "    __author__ = \"ECG Research Team\"\n",
    "    \n",
    "    from . import dataloaders, eval, model, preprocessing, saver, training, utils\n",
    "    \n",
    "    __all__ = ['utils', 'preprocessing', 'dataloaders', 'model', 'training', 'eval', 'saver']\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', '__init__.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee80321d574c4f0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.225829Z",
     "start_time": "2025-12-01T15:29:05.211008Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:20:59.938981Z",
     "iopub.status.busy": "2025-12-02T04:20:59.936510Z",
     "iopub.status.idle": "2025-12-02T04:20:59.989544Z",
     "shell.execute_reply": "2025-12-02T04:20:59.987532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module dataloaders.py raised attempted relative import with no known parent package\n"
     ]
    }
   ],
   "source": [
    "# --- dataloaders.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    ECG Dataloaders Module\n",
    "    Implements PyTorch Dataset and DataLoader creation with lazy loading.\n",
    "    \"\"\"\n",
    "    \n",
    "    import logging\n",
    "    from pathlib import Path\n",
    "    from typing import Dict, Optional, Tuple\n",
    "    \n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "    \n",
    "    from .utils import read_json\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    \n",
    "    class ECGDataset(Dataset):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset for lazy loading of preprocessed ECG records.\n",
    "        Reads individual compressed .npz files on demand.\n",
    "        \"\"\"\n",
    "    \n",
    "        def __init__(\n",
    "            self,\n",
    "            manifest: list,\n",
    "            processed_dir: Path,\n",
    "            augment: bool = False,\n",
    "            noise_std: float = 0.01\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Initialize ECG Dataset.\n",
    "    \n",
    "            Args:\n",
    "                manifest: List of dicts with 'path' and 'label' keys\n",
    "                processed_dir: Root directory containing processed records\n",
    "                augment: Whether to apply augmentation\n",
    "                noise_std: Standard deviation for gaussian noise augmentation\n",
    "            \"\"\"\n",
    "            self.manifest = manifest\n",
    "            self.processed_dir = Path(processed_dir)\n",
    "            self.augment = augment\n",
    "            self.noise_std = noise_std\n",
    "    \n",
    "        def __len__(self) -> int:\n",
    "            return len(self.manifest)\n",
    "    \n",
    "        def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "            \"\"\"\n",
    "            Load and return a single record.\n",
    "    \n",
    "            Args:\n",
    "                idx: Index of record\n",
    "    \n",
    "            Returns:\n",
    "                Tuple of (signal_tensor, label_tensor)\n",
    "            \"\"\"\n",
    "            item = self.manifest[idx]\n",
    "            record_path = self.processed_dir / item['path']\n",
    "    \n",
    "            try:\n",
    "                data = np.load(record_path)\n",
    "                signal = data['signal'].astype(np.float32)\n",
    "                label = int(data['label'])\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load {record_path}: {e}\")\n",
    "                # Return zero signal and label\n",
    "                signal = np.zeros(5000, dtype=np.float32)\n",
    "                label = 0\n",
    "    \n",
    "            # Optional augmentation (on CPU for reproducibility)\n",
    "            if self.augment:\n",
    "                # Add small gaussian noise\n",
    "                if self.noise_std > 0:\n",
    "                    noise = np.random.normal(0, self.noise_std, signal.shape).astype(np.float32)\n",
    "                    signal = signal + noise\n",
    "    \n",
    "                # Random crop (if longer than needed, take random segment)\n",
    "                # Here we assume signal is already fixed length, so this is optional placeholder\n",
    "    \n",
    "            # Convert to tensor\n",
    "            signal_tensor = torch.from_numpy(signal).float()\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "            return signal_tensor, label_tensor\n",
    "    \n",
    "    \n",
    "    def collate_fn(batch: list) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Collate function to reshape signals to (batch, 1, samples).\n",
    "    \n",
    "        Args:\n",
    "            batch: List of (signal, label) tuples\n",
    "    \n",
    "        Returns:\n",
    "            Tuple of (signals_batch, labels_batch)\n",
    "        \"\"\"\n",
    "        signals = []\n",
    "        labels = []\n",
    "    \n",
    "        for signal, label in batch:\n",
    "            signals.append(signal.unsqueeze(0))  # Add channel dimension\n",
    "            labels.append(label)\n",
    "    \n",
    "        signals_batch = torch.stack(signals, dim=0)  # Shape: (batch, 1, samples)\n",
    "        labels_batch = torch.stack(labels, dim=0)\n",
    "    \n",
    "        return signals_batch, labels_batch\n",
    "    \n",
    "    \n",
    "    def create_dataloaders(\n",
    "        splits_json: Path,\n",
    "        processed_dir: Path,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 4,\n",
    "        pin_memory: bool = True,\n",
    "        augment_train: bool = False\n",
    "    ) -> Dict[str, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create train/val/test DataLoaders from splits.json.\n",
    "    \n",
    "        Args:\n",
    "            splits_json: Path to splits.json file\n",
    "            processed_dir: Root directory containing processed records\n",
    "            batch_size: Batch size for DataLoaders\n",
    "            num_workers: Number of worker processes\n",
    "            pin_memory: Whether to pin memory for faster GPU transfer\n",
    "            augment_train: Whether to augment training data\n",
    "    \n",
    "        Returns:\n",
    "            Dictionary with 'train', 'val', 'test' DataLoaders\n",
    "        \"\"\"\n",
    "        splits = read_json(splits_json)\n",
    "    \n",
    "        train_manifest = splits['train']\n",
    "        val_manifest = splits.get('val', [])\n",
    "        test_manifest = splits.get('test', [])\n",
    "    \n",
    "        logger.info(f\"Creating dataloaders: train={len(train_manifest)}, \"\n",
    "                    f\"val={len(val_manifest)}, test={len(test_manifest)}\")\n",
    "    \n",
    "        # Create datasets\n",
    "        train_dataset = ECGDataset(train_manifest, processed_dir, augment=augment_train)\n",
    "        val_dataset = ECGDataset(val_manifest, processed_dir, augment=False)\n",
    "        test_dataset = ECGDataset(test_manifest, processed_dir, augment=False)\n",
    "    \n",
    "        # Optional: Create weighted sampler for imbalanced classes\n",
    "        train_labels = [item['label'] for item in train_manifest]\n",
    "        label_counts = np.bincount(train_labels)\n",
    "        weights = 1.0 / (label_counts + 1e-6)\n",
    "        sample_weights = weights[train_labels]\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "    \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True\n",
    "        )\n",
    "    \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn\n",
    "        ) if len(val_manifest) > 0 else None\n",
    "    \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn\n",
    "        ) if len(test_manifest) > 0 else None\n",
    "    \n",
    "        return {\n",
    "            'train': train_loader,\n",
    "            'val': val_loader,\n",
    "            'test': test_loader\n",
    "        }\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'dataloaders.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543595e54ed6abf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.266128Z",
     "start_time": "2025-12-01T15:29:05.244604Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:20:59.997617Z",
     "iopub.status.busy": "2025-12-02T04:20:59.997617Z",
     "iopub.status.idle": "2025-12-02T04:21:07.516999Z",
     "shell.execute_reply": "2025-12-02T04:21:07.516220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module eval.py raised attempted relative import with no known parent package\n"
     ]
    }
   ],
   "source": [
    "# --- eval.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    ECG Evaluation Module\n",
    "    Implements evaluation metrics and visualization for trained models.\n",
    "    \"\"\"\n",
    "    \n",
    "    import logging\n",
    "    from pathlib import Path\n",
    "    from typing import Dict, List, Optional, Tuple\n",
    "    \n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Non-interactive backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import (\n",
    "        classification_report,\n",
    "        confusion_matrix,\n",
    "        f1_score,\n",
    "        precision_recall_fscore_support,\n",
    "    )\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    from .utils import safe_makedirs, write_json\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    \n",
    "    def predict_on_split(\n",
    "        model: nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        device: str,\n",
    "        use_amp: bool = True\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate predictions and probabilities for a data split.\n",
    "    \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            dataloader: DataLoader for the split\n",
    "            device: Device string\n",
    "            use_amp: Whether to use automatic mixed precision\n",
    "    \n",
    "        Returns:\n",
    "            Tuple of (predictions, probabilities, true_labels)\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for signals, labels in tqdm(dataloader, desc=\"Predicting\"):\n",
    "                signals = signals.to(device, non_blocking=True)\n",
    "    \n",
    "                if use_amp and device == 'cuda':\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(signals)\n",
    "                else:\n",
    "                    outputs = model(signals)\n",
    "    \n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                _, preds = outputs.max(1)\n",
    "    \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "    \n",
    "        return (\n",
    "            np.array(all_preds),\n",
    "            np.array(all_probs),\n",
    "            np.array(all_labels)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def compute_metrics(\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        label_names: Optional[List[str]] = None\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Compute classification metrics.\n",
    "    \n",
    "        Args:\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels\n",
    "            label_names: List of label names\n",
    "    \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        if label_names is None:\n",
    "            label_names = [f\"Class_{i}\" for i in range(max(y_true.max(), y_pred.max()) + 1)]\n",
    "    \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "        # Per-class metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=None, zero_division=0\n",
    "        )\n",
    "    \n",
    "        # Macro and micro averages\n",
    "        f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "        # Per-class results\n",
    "        per_class = []\n",
    "        for i, label in enumerate(label_names):\n",
    "            per_class.append({\n",
    "                'label': label,\n",
    "                'precision': float(precision[i]) if i < len(precision) else 0.0,\n",
    "                'recall': float(recall[i]) if i < len(recall) else 0.0,\n",
    "                'f1': float(f1[i]) if i < len(f1) else 0.0,\n",
    "                'support': int(support[i]) if i < len(support) else 0\n",
    "            })\n",
    "    \n",
    "        metrics = {\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'per_class': per_class,\n",
    "            'f1_macro': float(f1_macro),\n",
    "            'f1_micro': float(f1_micro),\n",
    "            'f1_weighted': float(f1_weighted),\n",
    "            'accuracy': float((y_true == y_pred).mean())\n",
    "        }\n",
    "    \n",
    "        # Classification report\n",
    "        report = classification_report(\n",
    "            y_true, y_pred, target_names=label_names, zero_division=0, output_dict=True\n",
    "        )\n",
    "        metrics['classification_report'] = report\n",
    "    \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def plot_class_f1(\n",
    "        metrics: Dict[str, any],\n",
    "        output_path: Path,\n",
    "        figsize: Tuple[int, int] = (10, 6)\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot per-class F1 scores as bar chart.\n",
    "    \n",
    "        Args:\n",
    "            metrics: Metrics dictionary from compute_metrics\n",
    "            output_path: Path to save figure\n",
    "            figsize: Figure size tuple\n",
    "        \"\"\"\n",
    "        per_class = metrics['per_class']\n",
    "        labels = [item['label'] for item in per_class]\n",
    "        f1_scores = [item['f1'] for item in per_class]\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        bars = ax.bar(range(len(labels)), f1_scores, color='steelblue', alpha=0.8)\n",
    "    \n",
    "        ax.set_xlabel('Class', fontsize=12)\n",
    "        ax.set_ylabel('F1 Score', fontsize=12)\n",
    "        ax.set_title('Per-Class F1 Scores', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(range(len(labels)))\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, f1_scores):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{score:.3f}',\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        safe_makedirs(output_path.parent)\n",
    "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved F1 bar plot to {output_path}\")\n",
    "    \n",
    "    \n",
    "    def evaluate_model(\n",
    "        model: nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        device: str,\n",
    "        label_names: List[str],\n",
    "        output_dir: Path,\n",
    "        split_name: str = 'test',\n",
    "        use_amp: bool = True\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Complete evaluation pipeline: predict, compute metrics, save results and plots.\n",
    "    \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            dataloader: DataLoader for evaluation\n",
    "            device: Device string\n",
    "            label_names: List of label names\n",
    "            output_dir: Directory to save outputs\n",
    "            split_name: Name of split being evaluated\n",
    "            use_amp: Whether to use mixed precision\n",
    "    \n",
    "        Returns:\n",
    "            Metrics dictionary\n",
    "        \"\"\"\n",
    "        logger.info(f\"Evaluating model on {split_name} split\")\n",
    "    \n",
    "        # Get predictions\n",
    "        preds, probs, labels = predict_on_split(model, dataloader, device, use_amp)\n",
    "    \n",
    "        # Compute metrics\n",
    "        metrics = compute_metrics(labels, preds, label_names)\n",
    "    \n",
    "        # Add split name\n",
    "        metrics['split'] = split_name\n",
    "    \n",
    "        # Save metrics\n",
    "        metrics_path = output_dir / f'{split_name}_metrics.json'\n",
    "        write_json(metrics_path, metrics)\n",
    "        logger.info(f\"Saved metrics to {metrics_path}\")\n",
    "    \n",
    "        # Plot F1 scores\n",
    "        figures_dir = output_dir.parent.parent / 'figures'\n",
    "        plot_path = figures_dir / f'{split_name}_f1_scores.png'\n",
    "        plot_class_f1(metrics, plot_path)\n",
    "    \n",
    "        # Log summary\n",
    "        logger.info(f\"{split_name.capitalize()} Results:\")\n",
    "        logger.info(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        logger.info(f\"  F1 Macro: {metrics['f1_macro']:.4f}\")\n",
    "        logger.info(f\"  F1 Micro: {metrics['f1_micro']:.4f}\")\n",
    "        logger.info(f\"  F1 Weighted: {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "        return metrics\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'eval.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8128c7646501a17b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.299512Z",
     "start_time": "2025-12-01T15:29:05.278145Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:07.525223Z",
     "iopub.status.busy": "2025-12-02T04:21:07.525223Z",
     "iopub.status.idle": "2025-12-02T04:21:07.584897Z",
     "shell.execute_reply": "2025-12-02T04:21:07.584897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote candidate mapping to D:\\ecg-research\\notebooks\\logs\\unified_label_mapping.candidate.csv\n",
      "Rows: 0\n"
     ]
    }
   ],
   "source": [
    "# --- generate_candidate_unified_mapping.py (inlined) ---\n",
    "try:\n",
    "    # generate_candidate_unified_mapping.py\n",
    "    import csv\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    \n",
    "    ROOT = Path.cwd().resolve()\n",
    "    DATASET_DIR = ROOT / \"dataset\"\n",
    "    LOGS_DIR = ROOT / \"logs\"\n",
    "    LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    OUT = LOGS_DIR / \"unified_label_mapping.candidate.csv\"\n",
    "    \n",
    "    # label heuristics: map various textual signals to our 5 classes\n",
    "    HEURISTICS = [\n",
    "        (re.compile(r\"\\b(myocard|infarct|mi|ischemi|ischemia)\\b\", re.I), \"MI\"),\n",
    "        (re.compile(r\"\\b(atrial fibrill|afib|a\\.fibrill|fibrillation)\\b\", re.I), \"AF\"),\n",
    "        (re.compile(r\"\\b(bundle branch block|bbb|left bundle|right bundle|bundle-branch)\\b\", re.I), \"BBB\"),\n",
    "        (re.compile(r\"\\b(normal|norm|sinus rhythm|no abnormalit)\\b\", re.I), \"NORM\"),\n",
    "    ]\n",
    "    \n",
    "    def apply_heuristics(text: str):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        for pat, lab in HEURISTICS:\n",
    "            if pat.search(text):\n",
    "                return lab\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_ptbxl(ptbxl_dir: Path, rows):\n",
    "        csv_path = ptbxl_dir / \"ptbxl_database.csv\"\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path, dtype=str).fillna(\"\")\n",
    "            for _, r in df.iterrows():\n",
    "                # filename_lr often like \"records100/00000/00001_lr\"\n",
    "                filename_lr = str(r.get(\"filename_lr\", \"\")).strip()\n",
    "                if filename_lr:\n",
    "                    rid = Path(filename_lr).name  # e.g. \"00001_lr\" but we'll preserve folder-less id\n",
    "                    rid = filename_lr.replace(\"\\\\\", \"/\").strip(\"/\")\n",
    "                else:\n",
    "                    rid = str(r.get(\"ecg_id\", \"\")).strip()\n",
    "                report = str(r.get(\"report\", \"\")).strip()\n",
    "                scp = str(r.get(\"scp_codes\", \"\")).strip()\n",
    "                combined = \" \".join([report, scp])\n",
    "                mapped = apply_heuristics(combined)\n",
    "                rows.append((\"PTBXL\", rid, combined, mapped))\n",
    "        else:\n",
    "            print(\"PTBXL metadata not found at\", csv_path)\n",
    "    \n",
    "    def extract_cinc(cinc_dir: Path, rows):\n",
    "        # CinC 2017 uses REFERENCE files with labels like A,N,O\n",
    "        # We'll collect training/validation/test reference files if present\n",
    "        for sub in [\"training\", \"validation\", \"test\", \"\"]:\n",
    "            base = cinc_dir if sub == \"\" else cinc_dir / sub\n",
    "            if not base.exists():\n",
    "                continue\n",
    "            for ref in base.glob(\"REFERENCE*.csv\"):\n",
    "                try:\n",
    "                    df = pd.read_csv(ref, header=None, names=[\"record\", \"label\"], dtype=str)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                for _, r in df.iterrows():\n",
    "                    record = str(r[\"record\"]).strip()\n",
    "                    label = str(r[\"label\"]).strip().upper()\n",
    "                    # map A->AF? O->OTHER? N->NORM? This is heuristic: you may need to adjust\n",
    "                    mapped = \"\"\n",
    "                    if label == \"A\": mapped = \"AF\"\n",
    "                    elif label == \"N\": mapped = \"NORM\"\n",
    "                    elif label == \"O\": mapped = \"OTHER\"\n",
    "                    rows.append((\"CinC_2017_AFDB\", record, label, mapped))\n",
    "        # If no REFERENCE files, also sweep .hea/.mat basenames\n",
    "        for rec in cinc_dir.rglob(\"*\"):\n",
    "            if rec.suffix.lower() in (\".hea\", \".mat\"):\n",
    "                rel = rec.relative_to(DATASET_DIR).with_suffix(\"\")\n",
    "                rid = rel.as_posix()\n",
    "                rows.append((\"CinC_2017_AFDB\", rid, \"\", \"\"))\n",
    "    \n",
    "    def extract_folder_generic(ds_name: str, ds_dir: Path, rows):\n",
    "        # walk files and try to collect any textual hints from accompanying .hea or .txt files\n",
    "        for p in ds_dir.rglob(\"*\"):\n",
    "            if p.suffix.lower() in (\".hea\", \".mat\", \".txt\", \".csv\"):\n",
    "                rel = p.relative_to(DATASET_DIR).with_suffix(\"\")\n",
    "                rid = rel.as_posix()\n",
    "                hint = \"\"\n",
    "                # attempt to read small header or first lines for clues\n",
    "                if p.suffix.lower() == \".hea\":\n",
    "                    try:\n",
    "                        text = p.read_text(errors=\"ignore\")\n",
    "                        hint = \" \".join(text.splitlines()[:10])\n",
    "                    except Exception:\n",
    "                        hint = \"\"\n",
    "                elif p.suffix.lower() == \".csv\":\n",
    "                    hint = \"\"\n",
    "                rows.append((ds_name, rid, hint, \"\"))\n",
    "    \n",
    "    def main():\n",
    "        rows = []\n",
    "        # PTBXL\n",
    "        ptbxl_dir = DATASET_DIR / \"ptb-xl\"\n",
    "        if ptbxl_dir.exists():\n",
    "            extract_ptbxl(ptbxl_dir, rows)\n",
    "        # CinC\n",
    "        cinc_dir = DATASET_DIR / \"CinC2017\"\n",
    "        if cinc_dir.exists():\n",
    "            extract_cinc(cinc_dir, rows)\n",
    "        # PTB Diagnostic folder (if present, we'll flatten names)\n",
    "        ptbdiag = DATASET_DIR / \"PTB_Diagnostic\"\n",
    "        if ptbdiag.exists():\n",
    "            extract_folder_generic(\"PTB_Diagnostic\", ptbdiag, rows)\n",
    "        # Chapman_Shaoxing\n",
    "        chap = DATASET_DIR / \"Chapman_Shaoxing\"\n",
    "        if chap.exists():\n",
    "            extract_folder_generic(\"Chapman_Shaoxing\", chap, rows)\n",
    "    \n",
    "        # dedupe by dataset+record_id\n",
    "        seen = set()\n",
    "        unique = []\n",
    "        for ds, rid, orig, mapped in rows:\n",
    "            key = (ds, rid)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            unique.append((ds, rid, orig.replace(\"\\n\", \" \")[:400], mapped))\n",
    "    \n",
    "        # write CSV\n",
    "        with OUT.open(\"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "            w = csv.writer(fh)\n",
    "            w.writerow([\"dataset\", \"record_id\", \"original_label_text\", \"mapped_label\"])\n",
    "            for ds, rid, orig, mapped in unique:\n",
    "                w.writerow([ds, rid, orig, mapped])\n",
    "        print(\"Wrote candidate mapping to\", OUT)\n",
    "        print(\"Rows:\", len(unique))\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'generate_candidate_unified_mapping.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a52f1ee3f14bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.326894Z",
     "start_time": "2025-12-01T15:29:05.308969Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:07.584897Z",
     "iopub.status.busy": "2025-12-02T04:21:07.584897Z",
     "iopub.status.idle": "2025-12-02T04:21:07.646680Z",
     "shell.execute_reply": "2025-12-02T04:21:07.645172Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- model.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    ECG Model Module\n",
    "    Defines 1D CNN architecture for ECG classification with residual blocks.\n",
    "    \"\"\"\n",
    "    \n",
    "    import logging\n",
    "    from pathlib import Path\n",
    "    from typing import Optional\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    \n",
    "    class ResidualBlock1D(nn.Module):\n",
    "        \"\"\"\n",
    "        1D Residual block with batch normalization and skip connection.\n",
    "        \"\"\"\n",
    "    \n",
    "        def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            kernel_size: int = 3,\n",
    "            stride: int = 1,\n",
    "            dropout: float = 0.1\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Initialize residual block.\n",
    "    \n",
    "            Args:\n",
    "                in_channels: Number of input channels\n",
    "                out_channels: Number of output channels\n",
    "                kernel_size: Convolution kernel size\n",
    "                stride: Convolution stride\n",
    "                dropout: Dropout probability\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "    \n",
    "            self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                                   stride=stride, padding=kernel_size//2, bias=False)\n",
    "            self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "            self.activation = nn.GELU()\n",
    "    \n",
    "            self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                                   stride=1, padding=kernel_size//2, bias=False)\n",
    "            self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "    \n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "            # Skip connection with projection if dimensions change\n",
    "            self.skip = nn.Sequential()\n",
    "            if stride != 1 or in_channels != out_channels:\n",
    "                self.skip = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                             stride=stride, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels)\n",
    "                )\n",
    "    \n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"Forward pass through residual block.\"\"\"\n",
    "            identity = self.skip(x)\n",
    "    \n",
    "            out = self.conv1(x)\n",
    "            out = self.bn1(out)\n",
    "            out = self.activation(out)\n",
    "            out = self.dropout(out)\n",
    "    \n",
    "            out = self.conv2(out)\n",
    "            out = self.bn2(out)\n",
    "    \n",
    "            out = out + identity\n",
    "            out = self.activation(out)\n",
    "    \n",
    "            return out\n",
    "    \n",
    "    \n",
    "    class ECGResNet1D(nn.Module):\n",
    "        \"\"\"\n",
    "        1D ResNet for ECG classification.\n",
    "        Suitable for single-lead or multi-lead flattened ECG signals.\n",
    "        \"\"\"\n",
    "    \n",
    "        def __init__(\n",
    "            self,\n",
    "            input_channels: int = 1,\n",
    "            n_classes: int = 5,\n",
    "            base_channels: int = 64,\n",
    "            dropout: float = 0.2\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Initialize ECG ResNet model.\n",
    "    \n",
    "            Args:\n",
    "                input_channels: Number of input channels (1 for single-lead)\n",
    "                n_classes: Number of output classes\n",
    "                base_channels: Base number of channels (will be scaled in deeper layers)\n",
    "                dropout: Dropout probability\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "    \n",
    "            self.input_channels = input_channels\n",
    "            self.n_classes = n_classes\n",
    "    \n",
    "            # Initial convolution\n",
    "            self.conv1 = nn.Conv1d(input_channels, base_channels, kernel_size=15,\n",
    "                                   stride=2, padding=7, bias=False)\n",
    "            self.bn1 = nn.BatchNorm1d(base_channels)\n",
    "            self.activation = nn.GELU()\n",
    "            self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "            # Residual blocks\n",
    "            self.layer1 = self._make_layer(base_channels, base_channels, 2, stride=1, dropout=dropout)\n",
    "            self.layer2 = self._make_layer(base_channels, base_channels*2, 2, stride=2, dropout=dropout)\n",
    "            self.layer3 = self._make_layer(base_channels*2, base_channels*4, 2, stride=2, dropout=dropout)\n",
    "            self.layer4 = self._make_layer(base_channels*4, base_channels*8, 2, stride=2, dropout=dropout)\n",
    "    \n",
    "            # Global pooling and classifier\n",
    "            self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.fc = nn.Linear(base_channels*8, n_classes)\n",
    "    \n",
    "            self._initialize_weights()\n",
    "    \n",
    "        def _make_layer(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            num_blocks: int,\n",
    "            stride: int,\n",
    "            dropout: float\n",
    "        ) -> nn.Sequential:\n",
    "            \"\"\"Create a layer with multiple residual blocks.\"\"\"\n",
    "            layers = []\n",
    "            layers.append(ResidualBlock1D(in_channels, out_channels, stride=stride, dropout=dropout))\n",
    "            for _ in range(1, num_blocks):\n",
    "                layers.append(ResidualBlock1D(out_channels, out_channels, stride=1, dropout=dropout))\n",
    "            return nn.Sequential(*layers)\n",
    "    \n",
    "        def _initialize_weights(self):\n",
    "            \"\"\"Initialize model weights.\"\"\"\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv1d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                elif isinstance(m, nn.BatchNorm1d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight, 0, 0.01)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"\n",
    "            Forward pass.\n",
    "    \n",
    "            Args:\n",
    "                x: Input tensor of shape (batch, channels, samples)\n",
    "    \n",
    "            Returns:\n",
    "                Logits of shape (batch, n_classes)\n",
    "            \"\"\"\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.maxpool(x)\n",
    "    \n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "    \n",
    "            x = self.global_pool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "    \n",
    "            return x\n",
    "    \n",
    "    \n",
    "    def count_parameters(model: nn.Module) -> int:\n",
    "        \"\"\"\n",
    "        Count trainable parameters in model.\n",
    "    \n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "    \n",
    "        Returns:\n",
    "            Number of trainable parameters\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    \n",
    "    def load_checkpoint(\n",
    "        model: nn.Module,\n",
    "        checkpoint_path: Path,\n",
    "        device: str = 'cpu'\n",
    "    ) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Load model from checkpoint.\n",
    "    \n",
    "        Args:\n",
    "            model: Model instance\n",
    "            checkpoint_path: Path to checkpoint file\n",
    "            device: Device to load model to\n",
    "    \n",
    "        Returns:\n",
    "            Model with loaded weights\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        logger.info(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def save_checkpoint(\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epoch: int,\n",
    "        metrics: dict,\n",
    "        checkpoint_path: Path\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save model checkpoint.\n",
    "    \n",
    "        Args:\n",
    "            model: Model instance\n",
    "            optimizer: Optimizer instance\n",
    "            epoch: Current epoch\n",
    "            metrics: Dictionary of metrics\n",
    "            checkpoint_path: Path to save checkpoint\n",
    "        \"\"\"\n",
    "        checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'model.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65079ed9aa159480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.360408Z",
     "start_time": "2025-12-01T15:29:05.335900Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:07.653246Z",
     "iopub.status.busy": "2025-12-02T04:21:07.653246Z",
     "iopub.status.idle": "2025-12-02T04:21:08.281106Z",
     "shell.execute_reply": "2025-12-02T04:21:08.281106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module preprocessing.py raised attempted relative import with no known parent package\n"
     ]
    }
   ],
   "source": [
    "# --- preprocessing.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    ECG Preprocessing Module\n",
    "    Handles loading, filtering, resampling, and normalization of ECG signals.\n",
    "    Implements streaming preprocessing with unified label mapping.\n",
    "    \"\"\"\n",
    "    \n",
    "    import logging\n",
    "    import re\n",
    "    from pathlib import Path\n",
    "    from typing import Dict, List, Optional, Tuple, Union\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import wfdb\n",
    "    from scipy import signal as scipy_signal\n",
    "    from scipy.io import loadmat\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    from .utils import read_json, robust_path_variants, safe_makedirs, safe_save_npz, write_json\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    \n",
    "    def load_wfdb_record(record_path: Union[str, Path]) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Load WFDB format ECG record (.hea/.dat files).\n",
    "    \n",
    "        Args:\n",
    "            record_path: Path to record (without extension)\n",
    "    \n",
    "        Returns:\n",
    "            Tuple of (signal array, sampling frequency)\n",
    "    \n",
    "        Raises:\n",
    "            ValueError: If record cannot be read\n",
    "        \"\"\"\n",
    "        try:\n",
    "            record = wfdb.rdrecord(str(Path(record_path).with_suffix('')))\n",
    "            if record.p_signal is None:\n",
    "                raise ValueError(f\"No signal data in record: {record_path}\")\n",
    "    \n",
    "            # Flatten multi-lead to single channel (mean across leads)\n",
    "            signal = record.p_signal.mean(axis=1) if record.p_signal.ndim > 1 else record.p_signal\n",
    "            return signal.astype(np.float32), float(record.fs)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to read WFDB record {record_path}: {e}\")\n",
    "    \n",
    "    \n",
    "    def load_mat_record(mat_path: Union[str, Path]) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Load MAT format ECG record.\n",
    "    \n",
    "        Args:\n",
    "            mat_path: Path to .mat file\n",
    "    \n",
    "        Returns:\n",
    "            Tuple of (signal array, sampling frequency)\n",
    "    \n",
    "        Raises:\n",
    "            ValueError: If MAT file cannot be read\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mat_data = loadmat(str(mat_path))\n",
    "    \n",
    "            # Common MAT file structures\n",
    "            signal = None\n",
    "            fs = 500  # Default fallback\n",
    "    \n",
    "            # Try common field names\n",
    "            for key in ['val', 'data', 'ecg', 'signal']:\n",
    "                if key in mat_data:\n",
    "                    signal = mat_data[key]\n",
    "                    break\n",
    "    \n",
    "            if signal is None:\n",
    "                # Try first non-metadata field\n",
    "                for key, value in mat_data.items():\n",
    "                    if not key.startswith('__') and isinstance(value, np.ndarray):\n",
    "                        signal = value\n",
    "                        break\n",
    "    \n",
    "            if signal is None:\n",
    "                raise ValueError(f\"No signal data found in MAT file: {mat_path}\")\n",
    "    \n",
    "            # Flatten and ensure 1D\n",
    "            signal = signal.flatten().astype(np.float32)\n",
    "    \n",
    "            # Try to extract sampling frequency\n",
    "            for key in ['fs', 'Fs', 'freq', 'frequency']:\n",
    "                if key in mat_data:\n",
    "                    fs = float(mat_data[key].item())\n",
    "                    break\n",
    "    \n",
    "            return signal, fs\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to read MAT file {mat_path}: {e}\")\n",
    "    \n",
    "    \n",
    "    def resample_signal(signal: np.ndarray, original_fs: float, target_fs: float = 500) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Resample signal to target frequency using scipy.signal.resample.\n",
    "    \n",
    "        Args:\n",
    "            signal: Input signal array\n",
    "            original_fs: Original sampling frequency\n",
    "            target_fs: Target sampling frequency\n",
    "    \n",
    "        Returns:\n",
    "            Resampled signal\n",
    "        \"\"\"\n",
    "        if abs(original_fs - target_fs) < 1e-3:\n",
    "            return signal\n",
    "    \n",
    "        num_samples = int(len(signal) * target_fs / original_fs)\n",
    "        return scipy_signal.resample(signal, num_samples).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    def normalize_signal(signal: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Z-score normalize signal.\n",
    "    \n",
    "        Args:\n",
    "            signal: Input signal array\n",
    "    \n",
    "        Returns:\n",
    "            Normalized signal\n",
    "        \"\"\"\n",
    "        mean = np.mean(signal)\n",
    "        std = np.std(signal)\n",
    "        if std < 1e-8:\n",
    "            logger.warning(\"Signal has near-zero std, skipping normalization\")\n",
    "            return signal\n",
    "        return ((signal - mean) / std).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    def pad_or_truncate(signal: np.ndarray, target_samples: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pad with zeros or truncate signal to target length.\n",
    "    \n",
    "        Args:\n",
    "            signal: Input signal array\n",
    "            target_samples: Desired length\n",
    "    \n",
    "        Returns:\n",
    "            Signal of length target_samples\n",
    "        \"\"\"\n",
    "        current_length = len(signal)\n",
    "        if current_length < target_samples:\n",
    "            pad_width = target_samples - current_length\n",
    "            return np.pad(signal, (0, pad_width), mode='constant').astype(np.float32)\n",
    "        else:\n",
    "            return signal[:target_samples].astype(np.float32)\n",
    "    \n",
    "    \n",
    "    def build_mapping_index(unified_csv: Union[str, Path]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Build robust mapping index from unified label CSV.\n",
    "    \n",
    "        Args:\n",
    "            unified_csv: Path to unified_label_mapping.csv\n",
    "    \n",
    "        Returns:\n",
    "            Dictionary mapping path variants to mapped_label\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(unified_csv)\n",
    "            required_cols = ['dataset', 'record_id', 'mapped_label']\n",
    "            missing = set(required_cols) - set(df.columns)\n",
    "            if missing:\n",
    "                logger.warning(f\"Missing columns in unified CSV: {missing}, using available columns\")\n",
    "    \n",
    "            mapping = {}\n",
    "            for _, row in df.iterrows():\n",
    "                dataset = row.get('dataset', '')\n",
    "                record_id = str(row.get('record_id', ''))\n",
    "                mapped_label = row.get('mapped_label', 'OTHER')\n",
    "    \n",
    "                # Build full relative path\n",
    "                full_path = f\"{dataset}/{record_id}\" if dataset else record_id\n",
    "    \n",
    "                # Generate variants\n",
    "                variants = robust_path_variants(full_path)\n",
    "                for variant in variants:\n",
    "                    mapping[variant] = mapped_label\n",
    "    \n",
    "            logger.info(f\"Built mapping index with {len(mapping)} path variants from {len(df)} records\")\n",
    "            return mapping\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"Unified label mapping not found: {unified_csv}, all records will be marked OTHER\")\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading unified CSV: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    \n",
    "    def lookup_label(file_path: Path, dataset_dir: Path, mapping_index: Dict[str, str]) -> str:\n",
    "        \"\"\"\n",
    "        Lookup mapped label for a file using robust path matching.\n",
    "    \n",
    "        Args:\n",
    "            file_path: Full path to file\n",
    "            dataset_dir: Root dataset directory\n",
    "            mapping_index: Mapping dictionary from build_mapping_index\n",
    "    \n",
    "        Returns:\n",
    "            Mapped label string (default: 'OTHER')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            rel_path = file_path.relative_to(dataset_dir)\n",
    "        except ValueError:\n",
    "            rel_path = file_path\n",
    "    \n",
    "        # Generate variants\n",
    "        variants = robust_path_variants(rel_path)\n",
    "    \n",
    "        for variant in variants:\n",
    "            if variant in mapping_index:\n",
    "                return mapping_index[variant]\n",
    "    \n",
    "        return 'OTHER'\n",
    "    \n",
    "    \n",
    "    def sanitize_filename(path: str) -> str:\n",
    "        \"\"\"\n",
    "        Sanitize path for use as filename.\n",
    "    \n",
    "        Args:\n",
    "            path: Input path string\n",
    "    \n",
    "        Returns:\n",
    "            Sanitized filename safe for filesystem\n",
    "        \"\"\"\n",
    "        # Replace separators and special chars with underscore\n",
    "        sanitized = re.sub(r'[\\\\/:*?\"<>|]', '_', path)\n",
    "        # Remove multiple consecutive underscores\n",
    "        sanitized = re.sub(r'_+', '_', sanitized)\n",
    "        return sanitized.strip('_')\n",
    "    \n",
    "    \n",
    "    def run_streaming_preprocess(\n",
    "        dataset_dir: Union[str, Path],\n",
    "        unified_csv: Union[str, Path],\n",
    "        out_dir: Union[str, Path],\n",
    "        target_fs: int = 500,\n",
    "        target_samples: int = 5000,\n",
    "        label_order: Optional[List[str]] = None,\n",
    "        limit: Optional[int] = None\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Run streaming preprocessing on dataset with unified label mapping.\n",
    "        Processes files one at a time, saves individually, and builds manifest.\n",
    "    \n",
    "        Args:\n",
    "            dataset_dir: Root directory containing datasets\n",
    "            unified_csv: Path to unified_label_mapping.csv\n",
    "            out_dir: Output directory for processed records\n",
    "            target_fs: Target sampling frequency (Hz)\n",
    "            target_samples: Target number of samples\n",
    "            label_order: List of label names in order (for encoding)\n",
    "            limit: Optional limit on number of files to process (for testing)\n",
    "    \n",
    "        Returns:\n",
    "            Dictionary with processing statistics and paths\n",
    "        \"\"\"\n",
    "        if label_order is None:\n",
    "            label_order = ['MI', 'AF', 'BBB', 'NORM', 'OTHER']\n",
    "    \n",
    "        dataset_dir = Path(dataset_dir)\n",
    "        out_dir = Path(out_dir)\n",
    "        records_dir = out_dir / 'records'\n",
    "        safe_makedirs(records_dir)\n",
    "    \n",
    "        # Build mapping index\n",
    "        mapping_index = build_mapping_index(unified_csv)\n",
    "    \n",
    "        # Label encoding\n",
    "        label_to_int = {label: idx for idx, label in enumerate(label_order)}\n",
    "    \n",
    "        # Find all ECG files\n",
    "        hea_files = list(dataset_dir.rglob('*.hea'))\n",
    "        mat_files = list(dataset_dir.rglob('*.mat'))\n",
    "    \n",
    "        all_files = hea_files + mat_files\n",
    "        if limit:\n",
    "            all_files = all_files[:limit]\n",
    "    \n",
    "        logger.info(f\"Found {len(hea_files)} .hea files and {len(mat_files)} .mat files\")\n",
    "        logger.info(f\"Processing {len(all_files)} files total\")\n",
    "    \n",
    "        manifest = []\n",
    "        label_counts = {label: 0 for label in label_order}\n",
    "        failed_count = 0\n",
    "    \n",
    "        for file_path in tqdm(all_files, desc=\"Processing records\"):\n",
    "            try:\n",
    "                # Read signal\n",
    "                if file_path.suffix == '.hea':\n",
    "                    signal, fs = load_wfdb_record(file_path.with_suffix(''))\n",
    "                elif file_path.suffix == '.mat':\n",
    "                    signal, fs = load_mat_record(file_path)\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "                # Resample if needed\n",
    "                if abs(fs - target_fs) > 1e-3:\n",
    "                    signal = resample_signal(signal, fs, target_fs)\n",
    "    \n",
    "                # Normalize\n",
    "                signal = normalize_signal(signal)\n",
    "    \n",
    "                # Pad or truncate\n",
    "                signal = pad_or_truncate(signal, target_samples)\n",
    "    \n",
    "                # Lookup label\n",
    "                mapped_label = lookup_label(file_path, dataset_dir, mapping_index)\n",
    "                label_int = label_to_int.get(mapped_label, label_to_int['OTHER'])\n",
    "    \n",
    "                # Generate output filename\n",
    "                try:\n",
    "                    rel_path = file_path.relative_to(dataset_dir)\n",
    "                except ValueError:\n",
    "                    rel_path = file_path\n",
    "    \n",
    "                dataset_name = rel_path.parts[0] if len(rel_path.parts) > 1 else 'unknown'\n",
    "                sanitized = sanitize_filename(str(rel_path.with_suffix('')))\n",
    "                out_filename = f\"{dataset_name}__{sanitized}.npz\"\n",
    "                out_path = records_dir / out_filename\n",
    "    \n",
    "                # Save\n",
    "                safe_save_npz(out_path, signal, label_int)\n",
    "    \n",
    "                # Add to manifest\n",
    "                manifest.append({\n",
    "                    'path': f\"records/{out_filename}\",\n",
    "                    'label': int(label_int),\n",
    "                    'mapped_label': mapped_label,\n",
    "                    'dataset': dataset_name\n",
    "                })\n",
    "    \n",
    "                label_counts[mapped_label] = label_counts.get(mapped_label, 0) + 1\n",
    "    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to process {file_path}: {e}\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "    \n",
    "        logger.info(f\"Successfully processed {len(manifest)} records, {failed_count} failures\")\n",
    "        logger.info(f\"Label distribution: {label_counts}\")\n",
    "    \n",
    "        # Create stratified splits\n",
    "        labels_list = [item['label'] for item in manifest]\n",
    "        indices = np.arange(len(manifest))\n",
    "    \n",
    "        if len(manifest) > 10:\n",
    "            train_idx, temp_idx = train_test_split(\n",
    "                indices, test_size=0.2, stratify=labels_list, random_state=42\n",
    "            )\n",
    "            val_idx, test_idx = train_test_split(\n",
    "                temp_idx, test_size=0.5, stratify=[labels_list[i] for i in temp_idx], random_state=42\n",
    "            )\n",
    "        else:\n",
    "            # Too few samples for stratification\n",
    "            train_idx = indices\n",
    "            val_idx = np.array([])\n",
    "            test_idx = np.array([])\n",
    "    \n",
    "        splits = {\n",
    "            'train': [manifest[i] for i in train_idx],\n",
    "            'val': [manifest[i] for i in val_idx] if len(val_idx) > 0 else [],\n",
    "            'test': [manifest[i] for i in test_idx] if len(test_idx) > 0 else [],\n",
    "            'label_order': label_order,\n",
    "            'label_to_int': label_to_int,\n",
    "            'counts': label_counts\n",
    "        }\n",
    "    \n",
    "        # Save outputs\n",
    "        write_json(out_dir / 'splits.json', splits)\n",
    "        write_json(out_dir / 'label_map.json', {'label_order': label_order, 'label_to_int': label_to_int})\n",
    "        np.save(out_dir / 'labels.npy', np.array(labels_list))\n",
    "    \n",
    "        logger.info(f\"Saved splits.json, label_map.json, and labels.npy to {out_dir}\")\n",
    "    \n",
    "        return {\n",
    "            'total_processed': len(manifest),\n",
    "            'total_failed': failed_count,\n",
    "            'splits': splits,\n",
    "            'label_counts': label_counts,\n",
    "            'output_dir': str(out_dir)\n",
    "        }\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'preprocessing.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa2c4e6c4af23873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.385846Z",
     "start_time": "2025-12-01T15:29:05.372413Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:08.288894Z",
     "iopub.status.busy": "2025-12-02T04:21:08.288894Z",
     "iopub.status.idle": "2025-12-02T04:21:08.319354Z",
     "shell.execute_reply": "2025-12-02T04:21:08.319354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module saver.py raised attempted relative import with no known parent package\n"
     ]
    }
   ],
   "source": [
    "# --- saver.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    ECG Saver Module\n",
    "    Utilities for saving models, results, and artifacts.\n",
    "    \"\"\"\n",
    "    \n",
    "    import logging\n",
    "    from pathlib import Path\n",
    "    from typing import Dict, Optional\n",
    "    \n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    from .utils import safe_makedirs, write_json\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    \n",
    "    def save_model_checkpoint(\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epoch: int,\n",
    "        metrics: Dict,\n",
    "        save_path: Path,\n",
    "        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save complete model checkpoint with optimizer and scheduler state.\n",
    "    \n",
    "        Args:\n",
    "            model: Model instance\n",
    "            optimizer: Optimizer instance\n",
    "            epoch: Current epoch number\n",
    "            metrics: Dictionary of metrics\n",
    "            save_path: Path to save checkpoint\n",
    "            scheduler: Optional learning rate scheduler\n",
    "        \"\"\"\n",
    "        safe_makedirs(save_path.parent)\n",
    "    \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }\n",
    "    \n",
    "        if scheduler is not None:\n",
    "            checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "    \n",
    "        torch.save(checkpoint, save_path)\n",
    "        logger.info(f\"Saved checkpoint to {save_path}\")\n",
    "    \n",
    "    \n",
    "    def save_predictions(\n",
    "        predictions: np.ndarray,\n",
    "        probabilities: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "        save_path: Path,\n",
    "        label_names: Optional[list] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save predictions, probabilities, and labels to compressed file.\n",
    "    \n",
    "        Args:\n",
    "            predictions: Predicted class indices\n",
    "            probabilities: Class probabilities\n",
    "            labels: True labels\n",
    "            save_path: Path to save file\n",
    "            label_names: Optional list of label names\n",
    "        \"\"\"\n",
    "        safe_makedirs(save_path.parent)\n",
    "    \n",
    "        save_dict = {\n",
    "            'predictions': predictions,\n",
    "            'probabilities': probabilities,\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "        if label_names is not None:\n",
    "            save_dict['label_names'] = np.array(label_names, dtype=object)\n",
    "    \n",
    "        np.savez_compressed(save_path, **save_dict)\n",
    "        logger.info(f\"Saved predictions to {save_path}\")\n",
    "    \n",
    "    \n",
    "    def save_training_history(\n",
    "        history: list,\n",
    "        save_path: Path,\n",
    "        additional_info: Optional[Dict] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save training history to JSON file.\n",
    "    \n",
    "        Args:\n",
    "            history: List of epoch dictionaries\n",
    "            save_path: Path to save file\n",
    "            additional_info: Optional additional metadata\n",
    "        \"\"\"\n",
    "        output = {'history': history}\n",
    "    \n",
    "        if additional_info is not None:\n",
    "            output.update(additional_info)\n",
    "    \n",
    "        write_json(save_path, output)\n",
    "        logger.info(f\"Saved training history to {save_path}\")\n",
    "    \n",
    "    \n",
    "    def export_model_onnx(\n",
    "        model: nn.Module,\n",
    "        save_path: Path,\n",
    "        input_shape: tuple = (1, 1, 5000),\n",
    "        device: str = 'cpu'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Export model to ONNX format for deployment.\n",
    "    \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            save_path: Path to save ONNX file\n",
    "            input_shape: Example input shape\n",
    "            device: Device to use for export\n",
    "        \"\"\"\n",
    "        safe_makedirs(save_path.parent)\n",
    "    \n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "    \n",
    "        dummy_input = torch.randn(*input_shape).to(device)\n",
    "    \n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            save_path,\n",
    "            export_params=True,\n",
    "            opset_version=11,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={\n",
    "                'input': {0: 'batch_size'},\n",
    "                'output': {0: 'batch_size'}\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        logger.info(f\"Exported model to ONNX: {save_path}\")\n",
    "    \n",
    "    \n",
    "    def save_evaluation_report(\n",
    "        metrics: Dict,\n",
    "        save_path: Path\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save evaluation metrics report to JSON.\n",
    "    \n",
    "        Args:\n",
    "            metrics: Dictionary of evaluation metrics\n",
    "            save_path: Path to save file\n",
    "        \"\"\"\n",
    "        write_json(save_path, metrics)\n",
    "        logger.info(f\"Saved evaluation report to {save_path}\")\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'saver.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25eeb5114e3c7d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.418301Z",
     "start_time": "2025-12-01T15:29:05.405906Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:08.328977Z",
     "iopub.status.busy": "2025-12-02T04:21:08.328977Z",
     "iopub.status.idle": "2025-12-02T04:21:08.354756Z",
     "shell.execute_reply": "2025-12-02T04:21:08.352629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module serving.py raised No module named 'mlflow'\n"
     ]
    }
   ],
   "source": [
    "# --- serving.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    ECG Model Serving Module\n",
    "    Handles model loading and inference for production use.\n",
    "    \"\"\"\n",
    "    \n",
    "    import torch\n",
    "    import mlflow\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    \n",
    "    \n",
    "    class ECGPredictor:\n",
    "        \"\"\"\n",
    "        ECG inference class for loading trained models and making predictions.\n",
    "        \"\"\"\n",
    "    \n",
    "        def __init__(self, model_path=None, model_uri=None):\n",
    "            \"\"\"\n",
    "            Initialize predictor with either a local model path or MLflow model URI.\n",
    "    \n",
    "            Args:\n",
    "                model_path: Path to local .pth model file\n",
    "                model_uri: MLflow model URI (e.g., 'models:/ECGClassifier/Production')\n",
    "            \"\"\"\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "            if model_uri:\n",
    "                self.model = mlflow.pytorch.load_model(model_uri)\n",
    "            elif model_path:\n",
    "                self.model = torch.load(model_path, map_location=self.device)\n",
    "            else:\n",
    "                raise ValueError(\"Must provide either model_path or model_uri\")\n",
    "    \n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "    \n",
    "        def predict(self, ecg_signal):\n",
    "            \"\"\"\n",
    "            Make prediction on a single ECG signal.\n",
    "    \n",
    "            Args:\n",
    "                ecg_signal: numpy array of shape (length, channels)\n",
    "    \n",
    "            Returns:\n",
    "                predictions: numpy array of class probabilities\n",
    "            \"\"\"\n",
    "            # Prepare input\n",
    "            if isinstance(ecg_signal, np.ndarray):\n",
    "                ecg_tensor = torch.FloatTensor(ecg_signal).unsqueeze(0)\n",
    "            else:\n",
    "                ecg_tensor = ecg_signal.unsqueeze(0)\n",
    "    \n",
    "            ecg_tensor = ecg_tensor.to(self.device)\n",
    "    \n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(ecg_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "            return probabilities.cpu().numpy()[0]\n",
    "    \n",
    "        def predict_batch(self, ecg_signals):\n",
    "            \"\"\"\n",
    "            Make predictions on a batch of ECG signals.\n",
    "    \n",
    "            Args:\n",
    "                ecg_signals: numpy array of shape (batch_size, length, channels)\n",
    "    \n",
    "            Returns:\n",
    "                predictions: numpy array of shape (batch_size, num_classes)\n",
    "            \"\"\"\n",
    "            ecg_tensor = torch.FloatTensor(ecg_signals).to(self.device)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(ecg_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "            return probabilities.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    def load_mlflow_model(run_id=None, model_name=None, stage='Production'):\n",
    "        \"\"\"\n",
    "        Load model from MLflow.\n",
    "    \n",
    "        Args:\n",
    "            run_id: MLflow run ID\n",
    "            model_name: Registered model name\n",
    "            stage: Model stage (Production, Staging, etc.)\n",
    "    \n",
    "        Returns:\n",
    "            predictor: ECGPredictor instance\n",
    "        \"\"\"\n",
    "        if model_name:\n",
    "            model_uri = f\"models:/{model_name}/{stage}\"\n",
    "        elif run_id:\n",
    "            model_uri = f\"runs:/{run_id}/model\"\n",
    "        else:\n",
    "            raise ValueError(\"Must provide either run_id or model_name\")\n",
    "    \n",
    "        return ECGPredictor(model_uri=model_uri)\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'serving.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e9ff0e5af267345",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.456861Z",
     "start_time": "2025-12-01T15:29:05.437950Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:08.360838Z",
     "iopub.status.busy": "2025-12-02T04:21:08.360838Z",
     "iopub.status.idle": "2025-12-02T04:21:08.427549Z",
     "shell.execute_reply": "2025-12-02T04:21:08.427549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module training.py raised attempted relative import with no known parent package\n"
     ]
    }
   ],
   "source": [
    "# --- training.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    ECG Training Module\n",
    "    Implements training loop with mixed precision, checkpointing, and metrics logging.\n",
    "    \"\"\"\n",
    "    \n",
    "    import logging\n",
    "    import time\n",
    "    from pathlib import Path\n",
    "    from typing import Dict, Optional\n",
    "    \n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    from .utils import write_json\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    \n",
    "    class Trainer:\n",
    "        \"\"\"\n",
    "        Trainer class for ECG classification with mixed precision support.\n",
    "        \"\"\"\n",
    "    \n",
    "        def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            train_loader: torch.utils.data.DataLoader,\n",
    "            val_loader: Optional[torch.utils.data.DataLoader],\n",
    "            device: str,\n",
    "            learning_rate: float = 1e-3,\n",
    "            weight_decay: float = 1e-4,\n",
    "            epochs: int = 50,\n",
    "            checkpoint_dir: Path = Path('artifacts/models'),\n",
    "            use_amp: bool = True,\n",
    "            grad_accum_steps: int = 1\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Initialize Trainer.\n",
    "    \n",
    "            Args:\n",
    "                model: PyTorch model\n",
    "                train_loader: Training DataLoader\n",
    "                val_loader: Validation DataLoader\n",
    "                device: Device string ('cuda' or 'cpu')\n",
    "                learning_rate: Initial learning rate\n",
    "                weight_decay: Weight decay for optimizer\n",
    "                epochs: Number of training epochs\n",
    "                checkpoint_dir: Directory to save checkpoints\n",
    "                use_amp: Whether to use automatic mixed precision\n",
    "                grad_accum_steps: Gradient accumulation steps\n",
    "            \"\"\"\n",
    "            self.model = model.to(device)\n",
    "            self.train_loader = train_loader\n",
    "            self.val_loader = val_loader\n",
    "            self.device = device\n",
    "            self.epochs = epochs\n",
    "            self.checkpoint_dir = Path(checkpoint_dir)\n",
    "            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self.grad_accum_steps = grad_accum_steps\n",
    "    \n",
    "            # Loss and optimizer\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "            self.optimizer = torch.optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=learning_rate,\n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "    \n",
    "            # Learning rate scheduler\n",
    "            self.scheduler = CosineAnnealingLR(\n",
    "                self.optimizer,\n",
    "                T_max=epochs,\n",
    "                eta_min=1e-6\n",
    "            )\n",
    "    \n",
    "            # Mixed precision\n",
    "            self.use_amp = use_amp and device == 'cuda'\n",
    "            self.scaler = GradScaler() if self.use_amp else None\n",
    "    \n",
    "            # Tracking\n",
    "            self.best_val_f1 = 0.0\n",
    "            self.history = []\n",
    "    \n",
    "            logger.info(f\"Initialized Trainer on {device}, AMP={self.use_amp}\")\n",
    "    \n",
    "        def train_step(self) -> Dict[str, float]:\n",
    "            \"\"\"\n",
    "            Execute one training epoch.\n",
    "    \n",
    "            Returns:\n",
    "                Dictionary of training metrics\n",
    "            \"\"\"\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "    \n",
    "            self.optimizer.zero_grad()\n",
    "    \n",
    "            pbar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
    "            for batch_idx, (signals, labels) in enumerate(pbar):\n",
    "                signals = signals.to(self.device, non_blocking=True)\n",
    "                labels = labels.to(self.device, non_blocking=True)\n",
    "    \n",
    "                # Forward pass with optional mixed precision\n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = self.model(signals)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "                        loss = loss / self.grad_accum_steps\n",
    "    \n",
    "                    self.scaler.scale(loss).backward()\n",
    "    \n",
    "                    if (batch_idx + 1) % self.grad_accum_steps == 0:\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                        self.optimizer.zero_grad()\n",
    "                else:\n",
    "                    outputs = self.model(signals)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    loss = loss / self.grad_accum_steps\n",
    "                    loss.backward()\n",
    "    \n",
    "                    if (batch_idx + 1) % self.grad_accum_steps == 0:\n",
    "                        self.optimizer.step()\n",
    "                        self.optimizer.zero_grad()\n",
    "    \n",
    "                # Track metrics\n",
    "                total_loss += loss.item() * self.grad_accum_steps\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "                pbar.set_postfix({\n",
    "                    'loss': total_loss / (batch_idx + 1),\n",
    "                    'acc': 100. * correct / total\n",
    "                })\n",
    "    \n",
    "            avg_loss = total_loss / len(self.train_loader)\n",
    "            accuracy = 100. * correct / total\n",
    "    \n",
    "            return {\n",
    "                'loss': avg_loss,\n",
    "                'accuracy': accuracy\n",
    "            }\n",
    "    \n",
    "        def val_step(self) -> Dict[str, float]:\n",
    "            \"\"\"\n",
    "            Execute one validation epoch.\n",
    "    \n",
    "            Returns:\n",
    "                Dictionary of validation metrics\n",
    "            \"\"\"\n",
    "            if self.val_loader is None:\n",
    "                return {}\n",
    "    \n",
    "            self.model.eval()\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "    \n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                pbar = tqdm(self.val_loader, desc=\"Validation\", leave=False)\n",
    "                for signals, labels in pbar:\n",
    "                    signals = signals.to(self.device, non_blocking=True)\n",
    "                    labels = labels.to(self.device, non_blocking=True)\n",
    "    \n",
    "                    if self.use_amp:\n",
    "                        with autocast():\n",
    "                            outputs = self.model(signals)\n",
    "                            loss = self.criterion(outputs, labels)\n",
    "                    else:\n",
    "                        outputs = self.model(signals)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "    \n",
    "                    total_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "            avg_loss = total_loss / len(self.val_loader)\n",
    "            accuracy = 100. * correct / total\n",
    "    \n",
    "            # Calculate F1 macro (simple approximation)\n",
    "            # For full metrics, use eval.py functions\n",
    "            from sklearn.metrics import f1_score\n",
    "            f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "            return {\n",
    "                'loss': avg_loss,\n",
    "                'accuracy': accuracy,\n",
    "                'f1_macro': f1_macro\n",
    "            }\n",
    "    \n",
    "        def fit(self) -> Dict[str, list]:\n",
    "            \"\"\"\n",
    "            Train model for specified number of epochs.\n",
    "    \n",
    "            Returns:\n",
    "                Training history dictionary\n",
    "            \"\"\"\n",
    "            logger.info(f\"Starting training for {self.epochs} epochs\")\n",
    "    \n",
    "            for epoch in range(self.epochs):\n",
    "                start_time = time.time()\n",
    "    \n",
    "                # Train\n",
    "                train_metrics = self.train_step()\n",
    "    \n",
    "                # Validate\n",
    "                val_metrics = self.val_step()\n",
    "    \n",
    "                # Step scheduler\n",
    "                self.scheduler.step()\n",
    "    \n",
    "                # Log metrics\n",
    "                epoch_time = time.time() - start_time\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "    \n",
    "                log_str = f\"Epoch {epoch+1}/{self.epochs} ({epoch_time:.1f}s) - \"\n",
    "                log_str += f\"LR: {lr:.6f} - \"\n",
    "                log_str += f\"Train Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.2f}%\"\n",
    "    \n",
    "                if val_metrics:\n",
    "                    log_str += f\" - Val Loss: {val_metrics['loss']:.4f}, \"\n",
    "                    log_str += f\"Acc: {val_metrics['accuracy']:.2f}%, \"\n",
    "                    log_str += f\"F1: {val_metrics['f1_macro']:.4f}\"\n",
    "    \n",
    "                logger.info(log_str)\n",
    "    \n",
    "                # Save history\n",
    "                epoch_record = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'train': train_metrics,\n",
    "                    'val': val_metrics,\n",
    "                    'lr': lr,\n",
    "                    'time': epoch_time\n",
    "                }\n",
    "                self.history.append(epoch_record)\n",
    "    \n",
    "                # Save checkpoints\n",
    "                val_f1 = val_metrics.get('f1_macro', 0.0)\n",
    "    \n",
    "                # Save latest\n",
    "                latest_path = self.checkpoint_dir / 'latest.pth'\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                    'metrics': val_metrics\n",
    "                }, latest_path)\n",
    "    \n",
    "                # Save best\n",
    "                if val_f1 > self.best_val_f1:\n",
    "                    self.best_val_f1 = val_f1\n",
    "                    best_path = self.checkpoint_dir / 'best.pth'\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'metrics': val_metrics\n",
    "                    }, best_path)\n",
    "                    logger.info(f\"Saved new best model with F1={val_f1:.4f}\")\n",
    "    \n",
    "            # Save training log\n",
    "            log_path = self.checkpoint_dir.parent / 'training_log.json'\n",
    "            write_json(log_path, {'history': self.history, 'best_val_f1': self.best_val_f1})\n",
    "            logger.info(f\"Saved training log to {log_path}\")\n",
    "    \n",
    "            return self.history\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'training.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f69f1bee1065ceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.478021Z",
     "start_time": "2025-12-01T15:29:05.464611Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:08.433684Z",
     "iopub.status.busy": "2025-12-02T04:21:08.433684Z",
     "iopub.status.idle": "2025-12-02T04:21:08.467321Z",
     "shell.execute_reply": "2025-12-02T04:21:08.467321Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- utils.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    Utility functions for ECG data I/O and common operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import logging\n",
    "    import random\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    from typing import Any, Dict, Optional, Union\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    \n",
    "    def safe_makedirs(path: Union[str, Path]) -> None:\n",
    "        \"\"\"\n",
    "        Create directory safely if it does not exist.\n",
    "    \n",
    "        Args:\n",
    "            path: Directory path to create\n",
    "        \"\"\"\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    def read_json(path: Union[str, Path]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Read JSON file safely.\n",
    "    \n",
    "        Args:\n",
    "            path: Path to JSON file\n",
    "    \n",
    "        Returns:\n",
    "            Dictionary containing JSON data\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    \n",
    "    def write_json(path: Union[str, Path], data: Dict[str, Any], indent: int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Write dictionary to JSON file.\n",
    "    \n",
    "        Args:\n",
    "            path: Output path\n",
    "            data: Dictionary to serialize\n",
    "            indent: JSON indentation level\n",
    "        \"\"\"\n",
    "        safe_makedirs(Path(path).parent)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=indent)\n",
    "    \n",
    "    \n",
    "    def set_seed(seed: int = 42) -> None:\n",
    "        \"\"\"\n",
    "        Set random seed for reproducibility across numpy and torch.\n",
    "    \n",
    "        Args:\n",
    "            seed: Random seed value\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        try:\n",
    "            import torch\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "                torch.backends.cudnn.deterministic = True\n",
    "                torch.backends.cudnn.benchmark = False\n",
    "        except ImportError:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def print_device_info() -> str:\n",
    "        \"\"\"\n",
    "        Print and return device information (CUDA GPU or CPU).\n",
    "    \n",
    "        Returns:\n",
    "            Device string ('cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                device = 'cuda'\n",
    "                gpu_name = torch.cuda.get_device_name(0)\n",
    "                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "                logger.info(f\"Using CUDA device: {gpu_name}\")\n",
    "                logger.info(f\"GPU memory: {gpu_memory:.2f} GB\")\n",
    "                return device\n",
    "            else:\n",
    "                logger.info(\"CUDA not available, using CPU\")\n",
    "                return 'cpu'\n",
    "        except ImportError:\n",
    "            logger.warning(\"PyTorch not installed, defaulting to CPU\")\n",
    "            return 'cpu'\n",
    "    \n",
    "    \n",
    "    def safe_save_npz(path: Union[str, Path], signal: np.ndarray, label: int) -> None:\n",
    "        \"\"\"\n",
    "        Save signal and label to compressed NPZ file safely.\n",
    "    \n",
    "        Args:\n",
    "            path: Output file path\n",
    "            signal: Signal array\n",
    "            label: Integer label\n",
    "        \"\"\"\n",
    "        safe_makedirs(Path(path).parent)\n",
    "        np.savez_compressed(path, signal=signal, label=label)\n",
    "    \n",
    "    \n",
    "    def robust_path_variants(path: Union[str, Path]) -> list:\n",
    "        \"\"\"\n",
    "        Generate multiple path variants for robust matching.\n",
    "        Handles different path separators and relative path formats.\n",
    "    \n",
    "        Args:\n",
    "            path: Input path\n",
    "    \n",
    "        Returns:\n",
    "            List of path string variants\n",
    "        \"\"\"\n",
    "        p = Path(path)\n",
    "        variants = []\n",
    "    \n",
    "        # Normalize separators to forward slash\n",
    "        normalized = str(p).replace('\\\\', '/')\n",
    "        variants.append(normalized)\n",
    "    \n",
    "        # Without dataset prefix (assumes dataset is first component)\n",
    "        parts = Path(normalized).parts\n",
    "        if len(parts) > 1:\n",
    "            variants.append('/'.join(parts[1:]))\n",
    "    \n",
    "        # Last two components\n",
    "        if len(parts) >= 2:\n",
    "            variants.append('/'.join(parts[-2:]))\n",
    "    \n",
    "        # Basename without extension\n",
    "        stem = p.stem\n",
    "        variants.append(stem)\n",
    "    \n",
    "        # Full relative path without extension\n",
    "        variants.append(str(p.with_suffix('')).replace('\\\\', '/'))\n",
    "    \n",
    "        return list(set(variants))\n",
    "    \n",
    "    \n",
    "    def get_project_root() -> Path:\n",
    "        \"\"\"\n",
    "        Get the project root directory.\n",
    "    \n",
    "        Returns:\n",
    "            Path to project root\n",
    "        \"\"\"\n",
    "        return Path(__file__).parent.parent\n",
    "    \n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'utils.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2329953dc16cc81",
   "metadata": {},
   "source": [
    "## Inlined: Utility scripts (scripts/)\n",
    "\n",
    "Files: apply_mapping_improvements.py, create_master_notebook.py, create_notebook.py, generate_unified_mapping.py, improve_mapping.py, model_smoke_test.py, preprocess_streaming.py, run_full_automation.py, validate_mapping.py, verify_smoke_test.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a567825fbc886e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.495664Z",
     "start_time": "2025-12-01T15:29:05.485153Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:08.474937Z",
     "iopub.status.busy": "2025-12-02T04:21:08.474937Z",
     "iopub.status.idle": "2025-12-02T04:21:08.496788Z",
     "shell.execute_reply": "2025-12-02T04:21:08.496788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module apply_mapping_improvements.py raised name '__file__' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- apply_mapping_improvements.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    Apply mapping improvements to create an updated unified_label_mapping.csv\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    \n",
    "    ROOT = Path(__file__).parent.parent\n",
    "    LOGS_DIR = ROOT / \"logs\"\n",
    "    \n",
    "    # Load original mapping\n",
    "    mapping_file = LOGS_DIR / \"unified_label_mapping.csv\"\n",
    "    improvements_file = LOGS_DIR / \"mapping_improvements_suggested.csv\"\n",
    "    \n",
    "    if not improvements_file.exists():\n",
    "        print(\"Error: No improvements file found. Run improve_mapping.py first.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Backup original\n",
    "    backup_file = LOGS_DIR / f\"unified_label_mapping.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    print(f\"Creating backup: {backup_file.name}\")\n",
    "    \n",
    "    df = pd.read_csv(mapping_file, dtype=str).fillna(\"\")\n",
    "    df.to_csv(backup_file, index=False)\n",
    "    \n",
    "    # Load improvements\n",
    "    improvements_df = pd.read_csv(improvements_file, dtype=str)\n",
    "    print(f\"\\nLoaded {len(improvements_df)} improvements\")\n",
    "    \n",
    "    # Apply improvements\n",
    "    applied = 0\n",
    "    for _, imp in improvements_df.iterrows():\n",
    "        idx = int(imp['index'])\n",
    "        suggested_label = imp['suggested_label']\n",
    "    \n",
    "        if idx < len(df):\n",
    "            df.at[idx, 'mapped_label'] = suggested_label\n",
    "            applied += 1\n",
    "    \n",
    "    print(f\"Applied {applied} improvements\")\n",
    "    \n",
    "    # Save updated mapping\n",
    "    df.to_csv(mapping_file, index=False)\n",
    "    print(f\"\\n Updated mapping saved to: {mapping_file}\")\n",
    "    \n",
    "    # Print new distribution\n",
    "    print(f\"\\nUpdated label distribution:\")\n",
    "    label_counts = df['mapped_label'].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        pct = count / len(df) * 100\n",
    "        label_name = label if label else \"(unmapped)\"\n",
    "        print(f\"  {label_name}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    unmapped = (df['mapped_label'] == '').sum()\n",
    "    print(f\"\\nRemaining unmapped: {unmapped:,} ({unmapped/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n Mapping update complete!\")\n",
    "    print(\"\\nYou can now run preprocessing with the improved mapping.\")\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'apply_mapping_improvements.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d8b454ec03ec06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:05.533146Z",
     "start_time": "2025-12-01T15:29:05.503768Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:08.505854Z",
     "iopub.status.busy": "2025-12-02T04:21:08.505854Z",
     "iopub.status.idle": "2025-12-02T04:21:08.554256Z",
     "shell.execute_reply": "2025-12-02T04:21:08.553246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote notebook to D:\\ecg-research\\notebooks\\notebooks\\master_pipeline.ipynb\n"
     ]
    }
   ],
   "source": [
    "# --- create_master_notebook.py (inlined) ---\n",
    "try:\n",
    "    #!/usr/bin/env python3\n",
    "    \"\"\"\n",
    "    scripts/create_master_notebook.py\n",
    "    Builds notebooks/master_pipeline.ipynb by inlining .py files and creating runnable cells.\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    import textwrap\n",
    "    \n",
    "    ROOT = Path.cwd()\n",
    "    SCRIPTS = ROOT / \"scripts\"\n",
    "    SRC = ROOT / \"src\"\n",
    "    NOTEBOOKS = ROOT / \"notebooks\"\n",
    "    TARGET = NOTEBOOKS / \"master_pipeline.ipynb\"\n",
    "    NOTEBOOKS.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def read_file(path: Path) -> str:\n",
    "        return path.read_text(encoding=\"utf-8\")\n",
    "    \n",
    "    def code_cell(source: str):\n",
    "        return {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"metadata\": {},\n",
    "            \"execution_count\": None,\n",
    "            \"outputs\": [],\n",
    "            \"source\": source.splitlines(keepends=True),\n",
    "        }\n",
    "    \n",
    "    def md_cell(text: str):\n",
    "        return {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {},\n",
    "            \"source\": (text + \"\\n\").splitlines(keepends=True),\n",
    "        }\n",
    "    \n",
    "    cells = []\n",
    "    \n",
    "    # 0. Title + run-headless note\n",
    "    cells.append(md_cell(\n",
    "        \"# Master ECG Pipeline\\n\\n\"\n",
    "        \"This notebook combines all project scripts and modules into one single runnable file.\\n\\n\"\n",
    "        \"**Usage:** run cells top-to-bottom. For headless execution on Windows use:\\n\\n\"\n",
    "        \"```python\\n\"\n",
    "        \"import asyncio, sys\\n\"\n",
    "        \"if sys.platform == 'win32':\\n\"\n",
    "        \"    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\\n\"\n",
    "        \"```\\n\"\n",
    "    ))\n",
    "    \n",
    "    # 1. Environment & imports (idempotent)\n",
    "    env_code = textwrap.dedent(\"\"\"\n",
    "    # Environment & imports - idempotent\n",
    "    import os, sys, json, time, math, asyncio\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import torch\n",
    "    print('Python:', sys.executable)\n",
    "    print('Torch:', getattr(torch, '__version__', 'n/a'))\n",
    "    # Windows asyncio fix for nbconvert headless runs\n",
    "    import platform\n",
    "    if platform.system() == 'Windows':\n",
    "        try:\n",
    "            import asyncio, sys\n",
    "            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Project root detection\n",
    "    ROOT = Path(os.environ.get('ECG_ROOT', Path.cwd().resolve()))\n",
    "    DATASET_DIR = ROOT / 'Dataset'\n",
    "    ARTIFACTS_DIR = ROOT / 'artifacts'\n",
    "    PROCESSED_DIR = ARTIFACTS_DIR / 'processed'\n",
    "    LOGS_DIR = ROOT / 'logs'\n",
    "    NOTEBOOKS_DIR = ROOT / 'notebooks'\n",
    "    for p in (ARTIFACTS_DIR, PROCESSED_DIR, PROCESSED_DIR/'records', LOGS_DIR):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # seeds for reproducibility\n",
    "    SEED = int(os.environ.get('ECG_SEED', '42'))\n",
    "    random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('DEVICE', DEVICE)\n",
    "    \"\"\")\n",
    "    cells.append(code_cell(env_code))\n",
    "    \n",
    "    # 2. Inline src modules (each file -> markdown header + code cell)\n",
    "    def inline_dir(path: Path, title: str):\n",
    "        py_files = sorted([p for p in path.glob('*.py') if p.is_file()])\n",
    "        if not py_files:\n",
    "            return\n",
    "        cells.append(md_cell(f\"## Inlined: {title}\\n\\nFiles: {', '.join(p.name for p in py_files)}\"))\n",
    "        for p in py_files:\n",
    "            source = read_file(p)\n",
    "            header = f\"# --- {p.name} (inlined) ---\\n\"\n",
    "            # wrap in a try/except to prevent name collisions stopping the notebook\n",
    "            wrapped = (\n",
    "                header +\n",
    "                \"try:\\n\" +\n",
    "                \"\\n\".join(\"    \" + line for line in source.splitlines()) +\n",
    "                \"\\nexcept Exception as _e:\\n    print('Warning: inlined module', '{}', 'raised', _e)\\n\".format(p.name)\n",
    "            )\n",
    "            cells.append(code_cell(wrapped))\n",
    "    \n",
    "    # Inline src and scripts\n",
    "    inline_dir(SRC, \"Source modules (src/)\")\n",
    "    inline_dir(SCRIPTS, \"Utility scripts (scripts/)\")\n",
    "    \n",
    "    # 3. Operational cells: run mapping generation, optional improvement, preprocessing stub\n",
    "    cells.append(md_cell(\"## Quick: Generate/Load unified mapping (run this cell)\"))\n",
    "    cells.append(code_cell(textwrap.dedent(\"\"\"\n",
    "    # Generate unified mapping (if you have script)\n",
    "    candidate = Path('logs/unified_label_mapping.candidate.csv')\n",
    "    prod = Path('logs/unified_label_mapping.csv')\n",
    "    if (Path('scripts/generate_unified_mapping.py')).exists() and not candidate.exists():\n",
    "        print('Generating candidate mapping...')\n",
    "        os.system(f'python \\\"{str(Path(\"scripts/generate_unified_mapping.py\"))}\\\"')\n",
    "    else:\n",
    "        print('Candidate mapping exists:', candidate.exists(), 'Prod file exists:', prod.exists())\n",
    "    # If you have a candidate and want to promote it, uncomment:\n",
    "    # if candidate.exists(): candidate.replace(prod)\n",
    "    \"\"\")))\n",
    "    \n",
    "    cells.append(md_cell(\"## Preprocessing (streaming, memory-safe)\"))\n",
    "    cells.append(code_cell(textwrap.dedent(\"\"\"\n",
    "    # Run streaming preprocessing from scripts/preprocess_streaming.py if present\n",
    "    proc_script = Path('scripts/preprocess_streaming.py')\n",
    "    if proc_script.exists():\n",
    "        print('Launching streaming preprocessing (script)...')\n",
    "        # recommend using environment var ECG_PREPROCESS_LIMIT to test\n",
    "        os.system(f'python \\\"{proc_script}\\\"')\n",
    "    else:\n",
    "        print('No preprocess_streaming.py found. Implement preprocessing in this notebook or inline alternate script.')\n",
    "    \"\"\")))\n",
    "    \n",
    "    cells.append(md_cell(\"## Training (run this after preprocessing finishes)\"))\n",
    "    cells.append(code_cell(textwrap.dedent(\"\"\"\n",
    "    # Launch training script if present\n",
    "    train_script = Path('scripts/train_pipeline.py')  # optional\n",
    "    if train_script.exists():\n",
    "        print('Running training script...')\n",
    "        os.system(f'python \\\"{train_script}\\\"')\n",
    "    else:\n",
    "        print('No training script detected. Use in-notebook training cells or create scripts/training.py and link it.')\n",
    "    \"\"\")))\n",
    "    \n",
    "    cells.append(md_cell(\"## Evaluation and Visuals\"))\n",
    "    cells.append(code_cell(textwrap.dedent(\"\"\"\n",
    "    # Run evaluation if script exists\n",
    "    eval_script = Path('scripts/evaluate.py')\n",
    "    if eval_script.exists():\n",
    "        os.system(f'python \\\"{eval_script}\\\"')\n",
    "    else:\n",
    "        print('No evaluate.py. Use notebook cells to visualize artifacts/figures/')\n",
    "    \"\"\")))\n",
    "    \n",
    "    cells.append(md_cell(\"## Smoke tests and quick validation\"))\n",
    "    cells.append(code_cell(textwrap.dedent(\"\"\"\n",
    "    # Run smoke tests\n",
    "    smoke = Path('scripts/verify_smoke_test.py')\n",
    "    if smoke.exists():\n",
    "        os.system(f'python \\\"{smoke}\\\"')\n",
    "    else:\n",
    "        print('No smoke-test script. Manual checks:')\n",
    "        print(' - Count files:', len(list((PROCESSED_DIR/'records').glob('*.npz'))))\n",
    "        print(' - Check splits:', (PROCESSED_DIR/'splits.json').exists())\n",
    "    \"\"\")))\n",
    "    \n",
    "    cells.append(md_cell(\"## Final: Notebook control\\nYou can now run cells in order. Long-running steps are executed as external scripts to avoid kernel timeouts.\"))\n",
    "    \n",
    "    nb = {\n",
    "        \"cells\": cells,\n",
    "        \"metadata\": {\n",
    "            \"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"},\n",
    "            \"language_info\": {\"name\": \"python\"}\n",
    "        },\n",
    "        \"nbformat\": 4,\n",
    "        \"nbformat_minor\": 5\n",
    "    }\n",
    "    \n",
    "    TARGET.write_text(json.dumps(nb, indent=2), encoding=\"utf-8\")\n",
    "    print(\"Wrote notebook to\", TARGET)\n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'create_master_notebook.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e92642ba0ed976cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:06.895703Z",
     "start_time": "2025-12-01T15:29:05.543367Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:08.563894Z",
     "iopub.status.busy": "2025-12-02T04:21:08.563894Z",
     "iopub.status.idle": "2025-12-02T04:21:16.936392Z",
     "shell.execute_reply": "2025-12-02T04:21:16.934033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module create_notebook.py raised name '__file__' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- create_notebook.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    Create a complete Jupyter notebook for ECG tensor pipeline.\n",
    "    \n",
    "    This script programmatically builds notebooks/ecg_tensor_pipeline.ipynb\n",
    "    using nbformat v4 with all necessary cells for preprocessing, training, and evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    try:\n",
    "        import nbformat\n",
    "        from nbformat.v4 import new_notebook, new_code_cell, new_markdown_cell\n",
    "    except ImportError:\n",
    "        print(\"Error: nbformat not installed. Run: pip install nbformat\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Project paths\n",
    "    ROOT = Path(__file__).resolve().parent.parent\n",
    "    NOTEBOOKS_DIR = ROOT / \"notebooks\"\n",
    "    OUTPUT_NOTEBOOK = NOTEBOOKS_DIR / \"ecg_tensor_pipeline.ipynb\"\n",
    "    \n",
    "    # Ensure notebooks directory exists\n",
    "    NOTEBOOKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create a new notebook\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 1: Title and Instructions (Markdown)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_markdown_cell(\"\"\"# ECG Tensor Pipeline  Preprocessing, Training, Evaluation\n",
    "    \n",
    "    ## Overview\n",
    "    \n",
    "    This notebook provides an end-to-end pipeline for ECG signal classification:\n",
    "    1. **Preprocessing**: Load raw ECG datasets, resample, normalize, and save per-record `.npz` files\n",
    "    2. **Dataset & DataLoader**: Lazy loading with PyTorch for memory efficiency\n",
    "    3. **Model**: Compact 1D CNN for ECG classification\n",
    "    4. **Training**: GPU-accelerated training with mixed precision\n",
    "    5. **Evaluation**: Metrics, confusion matrix, ROC curves, and visualizations\n",
    "    \n",
    "    ## Prerequisites\n",
    "    \n",
    "    - Python 3.10+\n",
    "    - Required packages: `numpy`, `scipy`, `pandas`, `wfdb`, `torch`, `scikit-learn`, `matplotlib`, `tqdm`, `nbformat`\n",
    "    - Datasets should be in `Dataset/` folder\n",
    "    - Unified label mapping CSV at `logs/unified_label_mapping.csv`\n",
    "    \n",
    "    ## Running Headless\n",
    "    \n",
    "    To execute this notebook from the command line:\n",
    "    \n",
    "    ```powershell\n",
    "    jupyter nbconvert --to notebook --execute notebooks/ecg_tensor_pipeline.ipynb --output ecg_tensor_pipeline_executed.ipynb\n",
    "    ```\n",
    "    \n",
    "    **Note**: On Windows, if you see `RuntimeError: There is no current event loop in thread`, add this at the top of your script:\n",
    "    \n",
    "    ```python\n",
    "    import asyncio\n",
    "    import sys\n",
    "    if sys.platform == 'win32':\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "    ```\n",
    "    \n",
    "    ## GPU-Intensive Tasks\n",
    "    \n",
    "    The following cells will utilize GPU heavily when CUDA is available:\n",
    "    - **Training loop**: Forward/backward passes, gradient updates\n",
    "    - **Large batch evaluation**: Model inference on test set\n",
    "    - **Mixed precision training**: Uses `torch.cuda.amp` for speedup\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 2: Environment Setup and Imports (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# Environment checks, imports, seeds, and directory setup\n",
    "    import os\n",
    "    import sys\n",
    "    import random\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    from collections import defaultdict, Counter\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy import signal\n",
    "    from scipy.signal import resample\n",
    "    from scipy.io import loadmat\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "    from sklearn.metrics import (\n",
    "        confusion_matrix, classification_report, \n",
    "        f1_score, precision_recall_fscore_support,\n",
    "        roc_curve, auc, precision_recall_curve\n",
    "    )\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    \n",
    "    # Print environment info\n",
    "    print(f\"Python executable: {sys.executable}\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\\\nDevice: {DEVICE}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"Running on CPU - training will be slower\")\n",
    "    \n",
    "    # Set deterministic seeds for reproducibility\n",
    "    DEFAULT_SEED = 42\n",
    "    random.seed(DEFAULT_SEED)\n",
    "    np.random.seed(DEFAULT_SEED)\n",
    "    torch.manual_seed(DEFAULT_SEED)\n",
    "    if DEVICE.type == 'cuda':\n",
    "        torch.cuda.manual_seed_all(DEFAULT_SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    print(f\"\\\\nRandom seed set to: {DEFAULT_SEED}\")\n",
    "    \n",
    "    # Define project paths\n",
    "    # Try to detect if running inside notebooks/ or at project root\n",
    "    CANDIDATES = [\n",
    "        Path.cwd().parent,                 # when running inside notebooks/\n",
    "        Path.cwd(),                        # when running at project root\n",
    "        Path(\"D:/ecg-research\").resolve()  # explicit fallback for this project\n",
    "    ]\n",
    "    ROOT = next((p.resolve() for p in CANDIDATES if (p / \"Dataset\").exists()), Path.cwd().resolve())\n",
    "    \n",
    "    DATASET_DIR = ROOT / \"Dataset\"\n",
    "    ARTIFACTS_DIR = ROOT / \"artifacts\"\n",
    "    PROCESSED_DIR = ARTIFACTS_DIR / \"processed\"\n",
    "    RECORDS_DIR = PROCESSED_DIR / \"records\"\n",
    "    CHECKPOINTS_DIR = PROCESSED_DIR / \"checkpoints\"\n",
    "    FIGURES_DIR = ARTIFACTS_DIR / \"figures\"\n",
    "    LOGS_DIR = ROOT / \"logs\"\n",
    "    \n",
    "    # Create directories\n",
    "    for p in [ARTIFACTS_DIR, PROCESSED_DIR, RECORDS_DIR, CHECKPOINTS_DIR, FIGURES_DIR, LOGS_DIR]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\\\nProject Paths:\")\n",
    "    print(f\"  ROOT: {ROOT}\")\n",
    "    print(f\"  DATASET_DIR: {DATASET_DIR} (exists: {DATASET_DIR.exists()})\")\n",
    "    print(f\"  PROCESSED_DIR: {PROCESSED_DIR}\")\n",
    "    print(f\"  FIGURES_DIR: {FIGURES_DIR}\")\n",
    "    \n",
    "    # List available datasets\n",
    "    if DATASET_DIR.exists():\n",
    "        datasets = [p.name for p in sorted(DATASET_DIR.iterdir()) if p.is_dir()]\n",
    "        print(f\"\\\\nAvailable datasets: {datasets}\")\n",
    "    else:\n",
    "        print(f\"\\\\nWarning: Dataset directory not found at {DATASET_DIR}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"Environment setup complete!\")\n",
    "    print(\"=\"*80)\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 3: Configuration Constants (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# Configuration constants and hyperparameters\n",
    "    \n",
    "    # Signal processing parameters\n",
    "    TARGET_FS = 500           # Target sampling frequency (Hz)\n",
    "    TARGET_SAMPLES = 5000     # Target number of samples per record (10 seconds at 500 Hz)\n",
    "    \n",
    "    # Label configuration\n",
    "    LABEL_ORDER = ['MI', 'AF', 'BBB', 'NORM', 'OTHER']\n",
    "    LABEL_TO_INT = {name: i for i, name in enumerate(LABEL_ORDER)}\n",
    "    INT_TO_LABEL = {i: name for name, i in LABEL_TO_INT.items()}\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 32           # Adjust based on GPU memory\n",
    "    EPOCHS = 10               # Number of training epochs\n",
    "    LR = 1e-3                 # Learning rate\n",
    "    WEIGHT_DECAY = 1e-4       # L2 regularization\n",
    "    NUM_WORKERS = 0           # DataLoader workers (set to 0 for Windows stability)\n",
    "    PIN_MEMORY = True         # Pin memory for faster GPU transfer\n",
    "    \n",
    "    # Mixed precision training (GPU only)\n",
    "    USE_MIXED_PRECISION = torch.cuda.is_available()\n",
    "    \n",
    "    # Adjust batch size for CPU\n",
    "    if DEVICE.type == 'cpu' and BATCH_SIZE > 8:\n",
    "        print(f\"CPU detected - reducing batch size from {BATCH_SIZE} to 8\")\n",
    "        BATCH_SIZE = 8\n",
    "        PIN_MEMORY = False\n",
    "    \n",
    "    print(f\"\\\\nConfiguration:\")\n",
    "    print(f\"  Target FS: {TARGET_FS} Hz\")\n",
    "    print(f\"  Target Samples: {TARGET_SAMPLES}\")\n",
    "    print(f\"  Label Mapping: {LABEL_TO_INT}\")\n",
    "    print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"  Learning Rate: {LR}\")\n",
    "    print(f\"  Epochs: {EPOCHS}\")\n",
    "    print(f\"  Mixed Precision: {USE_MIXED_PRECISION}\")\n",
    "    print(f\"  Device: {DEVICE}\")\n",
    "    \n",
    "    print(f\"\\\\nGPU-Intensive Tasks:\")\n",
    "    print(\"  - Preprocessing: Moderate (CPU-bound mostly)\")\n",
    "    print(\"  - DataLoader: Low (lazy loading)\")\n",
    "    print(\"  - Model Training: HIGH (forward + backward passes)\")\n",
    "    print(\"  - Model Evaluation: Medium (inference only)\")\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 4: Utility Functions (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# Utility functions for signal processing and file I/O\n",
    "    \n",
    "    def zscore_normalize(arr: np.ndarray) -> np.ndarray:\n",
    "        \\\"\\\"\\\"Z-score normalization: (x - mean) / std\\\"\\\"\\\"\n",
    "        arr = arr.astype(np.float32)\n",
    "        mean = arr.mean()\n",
    "        std = arr.std()\n",
    "        if std < 1e-8:\n",
    "            std = 1.0  # Prevent division by zero\n",
    "        return ((arr - mean) / std).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    def pad_or_truncate(x: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \\\"\\\"\\\"Pad with zeros or truncate to target length\\\"\\\"\\\"\n",
    "        if x.size >= target_length:\n",
    "            return x[:target_length]\n",
    "        pad_width = target_length - x.size\n",
    "        return np.pad(x, (0, pad_width), mode='constant', constant_values=0).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    def resample_signal(x: np.ndarray, original_fs: float, target_fs: float) -> np.ndarray:\n",
    "        \\\"\\\"\\\"Resample signal to target sampling frequency\\\"\\\"\\\"\n",
    "        if original_fs is None or np.isclose(original_fs, target_fs):\n",
    "            return x\n",
    "        new_length = int(round(x.size * target_fs / original_fs))\n",
    "        if new_length <= 0:\n",
    "            return x\n",
    "        return resample(x, new_length).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    def safe_save_npz(path: Path, signal: np.ndarray, label: int):\n",
    "        \\\"\\\"\\\"Save signal and label as compressed npz\\\"\\\"\\\"\n",
    "        np.savez_compressed(path, signal=signal.astype(np.float32), label=int(label))\n",
    "    \n",
    "    \n",
    "    def load_npz_signal(path: Path):\n",
    "        \\\"\\\"\\\"Load signal and label from npz file\\\"\\\"\\\"\n",
    "        with np.load(path, allow_pickle=False) as data:\n",
    "            signal = data['signal']\n",
    "            label = int(data['label'])\n",
    "        return signal, label\n",
    "    \n",
    "    \n",
    "    def read_wfdb(hea_path: Path):\n",
    "        \\\"\\\"\\\"Read WFDB format (.hea/.dat) and return 1D signal and sampling frequency\\\"\\\"\\\"\n",
    "        try:\n",
    "            import wfdb\n",
    "            record_path = str(hea_path.with_suffix(''))  # wfdb expects path without extension\n",
    "            record = wfdb.rdsamp(record_path)\n",
    "            signal = np.asarray(record[0], dtype=np.float32)\n",
    "            fs = float(record[1].get('fs', TARGET_FS))\n",
    "            \n",
    "            # Convert to 1D: average across leads or take first lead\n",
    "            if signal.ndim == 2:\n",
    "                signal_1d = signal.mean(axis=1) if signal.shape[1] > 1 else signal[:, 0]\n",
    "            else:\n",
    "                signal_1d = signal.reshape(-1)\n",
    "            \n",
    "            return signal_1d.astype(np.float32), fs\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read WFDB file {hea_path.name}: {e}\")\n",
    "    \n",
    "    \n",
    "    def read_mat(mat_path: Path):\n",
    "        \\\"\\\"\\\"Read MATLAB .mat file and return 1D signal and sampling frequency\\\"\\\"\\\"\n",
    "        try:\n",
    "            mat_data = loadmat(str(mat_path))\n",
    "            \n",
    "            # Try common keys for signal data\n",
    "            signal = None\n",
    "            for key in ['val', 'data', 'signal', 'ecg']:\n",
    "                if key in mat_data:\n",
    "                    signal = np.asarray(mat_data[key], dtype=np.float32)\n",
    "                    break\n",
    "            \n",
    "            # Fallback: find first ndarray\n",
    "            if signal is None:\n",
    "                for value in mat_data.values():\n",
    "                    if isinstance(value, np.ndarray) and value.size > 100:\n",
    "                        signal = value.astype(np.float32)\n",
    "                        break\n",
    "            \n",
    "            if signal is None:\n",
    "                raise RuntimeError(\"No signal array found in MAT file\")\n",
    "            \n",
    "            # Convert to 1D\n",
    "            if signal.ndim == 2:\n",
    "                signal_1d = signal.mean(axis=0) if signal.shape[0] > 1 else signal.reshape(-1)\n",
    "            else:\n",
    "                signal_1d = signal.reshape(-1)\n",
    "            \n",
    "            # MAT files rarely contain fs info; return None\n",
    "            fs = None\n",
    "            return signal_1d.astype(np.float32), fs\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read MAT file {mat_path.name}: {e}\")\n",
    "    \n",
    "    \n",
    "    print(\"Utility functions defined successfully.\")\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 5: Load Unified Label Mapping (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# Load unified label mapping from CSV\n",
    "    \n",
    "    UNIFIED_CSV = LOGS_DIR / \"unified_label_mapping.csv\"\n",
    "    mapping_index = {}\n",
    "    \n",
    "    if UNIFIED_CSV.exists():\n",
    "        print(f\"Loading unified label mapping from {UNIFIED_CSV}...\")\n",
    "        df_mapping = pd.read_csv(UNIFIED_CSV, dtype=str).fillna(\"\")\n",
    "        \n",
    "        # Verify required columns\n",
    "        required_cols = {\"dataset\", \"record_id\", \"mapped_label\"}\n",
    "        if not required_cols.issubset(set(df_mapping.columns)):\n",
    "            print(f\"Warning: Missing required columns. Found: {df_mapping.columns.tolist()}\")\n",
    "            print(f\"Expected: {list(required_cols)}\")\n",
    "        else:\n",
    "            # Build mapping index with multiple key variants for robust lookup\n",
    "            for _, row in df_mapping.iterrows():\n",
    "                dataset = str(row.get(\"dataset\", \"\")).strip()\n",
    "                record_id = str(row.get(\"record_id\", \"\")).strip().replace(\"\\\\\\\\\", \"/\").strip(\"/\")\n",
    "                mapped_label = str(row.get(\"mapped_label\", \"\")).strip().upper()\n",
    "                \n",
    "                if not dataset or not record_id:\n",
    "                    continue\n",
    "                \n",
    "                if dataset not in mapping_index:\n",
    "                    mapping_index[dataset] = {}\n",
    "                \n",
    "                # Add full path\n",
    "                mapping_index[dataset][record_id] = mapped_label\n",
    "                \n",
    "                # Add variants for robust matching\n",
    "                parts = record_id.split(\"/\")\n",
    "                if len(parts) >= 1:\n",
    "                    mapping_index[dataset][parts[-1]] = mapped_label  # basename\n",
    "                if len(parts) >= 2:\n",
    "                    mapping_index[dataset][\"/\".join(parts[-2:])] = mapped_label  # last two components\n",
    "            \n",
    "            print(f\"Loaded {len(df_mapping)} mappings from {len(mapping_index)} datasets\")\n",
    "            \n",
    "            # Count mappings per label\n",
    "            label_counts = Counter(df_mapping['mapped_label'].str.upper())\n",
    "            print(f\"\\\\nLabel distribution in mapping:\")\n",
    "            for label in LABEL_ORDER:\n",
    "                count = label_counts.get(label, 0)\n",
    "                print(f\"  {label}: {count:,}\")\n",
    "            unmapped = label_counts.get('', 0)\n",
    "            print(f\"  (unmapped): {unmapped:,}\")\n",
    "    else:\n",
    "        print(f\"Warning: Unified label mapping not found at {UNIFIED_CSV}\")\n",
    "        print(\"All records will be labeled as OTHER\")\n",
    "    \n",
    "    \n",
    "    def lookup_mapped_label(path: Path) -> str:\n",
    "        \\\"\\\"\\\"Look up mapped label for a given file path\\\"\\\"\\\"\n",
    "        try:\n",
    "            rel_path = path.relative_to(DATASET_DIR).with_suffix(\"\")\n",
    "        except Exception:\n",
    "            rel_path = path.with_suffix(\"\")\n",
    "        \n",
    "        parts = rel_path.as_posix().split(\"/\")\n",
    "        dataset = parts[0] if parts else \"\"\n",
    "        \n",
    "        if dataset not in mapping_index:\n",
    "            return \"OTHER\"\n",
    "        \n",
    "        index = mapping_index[dataset]\n",
    "        \n",
    "        # Try multiple key variants\n",
    "        candidates = [\n",
    "            rel_path.as_posix(),                              # full path\n",
    "            \"/\".join(parts[1:]) if len(parts) > 1 else \"\",   # without dataset prefix\n",
    "            \"/\".join(parts[-2:]) if len(parts) >= 2 else \"\", # last two components\n",
    "            rel_path.name                                     # basename only\n",
    "        ]\n",
    "        \n",
    "        for key in candidates:\n",
    "            if key and key in index:\n",
    "                label = index[key].upper()\n",
    "                return label if label in LABEL_TO_INT else \"OTHER\"\n",
    "        \n",
    "        # CinC2017 special case\n",
    "        if \"CinC\" in dataset and len(parts) >= 3:\n",
    "            alt_key = \"/\".join(parts[2:])\n",
    "            if alt_key in index:\n",
    "                label = index[alt_key].upper()\n",
    "                return label if label in LABEL_TO_INT else \"OTHER\"\n",
    "        \n",
    "        return \"OTHER\"\n",
    "    \n",
    "    \n",
    "    print(\"Label lookup function ready.\")\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 6: Streaming Preprocessing (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# Streaming preprocessing: scan datasets, process, and save per-record npz files\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"STARTING PREPROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find all .hea and .mat files\n",
    "    print(f\"\\\\nScanning {DATASET_DIR} for ECG files...\")\n",
    "    hea_files = sorted(DATASET_DIR.rglob(\"*.hea\"))\n",
    "    mat_files = sorted(DATASET_DIR.rglob(\"*.mat\"))\n",
    "    all_files = hea_files + mat_files\n",
    "    \n",
    "    print(f\"Found {len(hea_files)} .hea files and {len(mat_files)} .mat files\")\n",
    "    print(f\"Total files to process: {len(all_files)}\")\n",
    "    \n",
    "    if not all_files:\n",
    "        print(\"\\\\nNo dataset files found. Generating synthetic records for testing...\")\n",
    "        # Generate synthetic signals\n",
    "        for i in range(20):\n",
    "            t = np.linspace(0, TARGET_SAMPLES / TARGET_FS, TARGET_SAMPLES, dtype=np.float32)\n",
    "            freq = 0.5 + 0.1 * i\n",
    "            signal = np.sin(2 * np.pi * freq * t).astype(np.float32)\n",
    "            signal = signal[np.newaxis, :]  # Shape: (1, TARGET_SAMPLES)\n",
    "            \n",
    "            label = i % len(LABEL_ORDER)\n",
    "            out_file = RECORDS_DIR / f\"SYNTH_{i:04d}.npz\"\n",
    "            safe_save_npz(out_file, signal, label)\n",
    "        \n",
    "        print(f\"Generated 20 synthetic records in {RECORDS_DIR}\")\n",
    "    else:\n",
    "        # Process real dataset files\n",
    "        manifest = []\n",
    "        label_counts = Counter()\n",
    "        skipped = 0\n",
    "        \n",
    "        print(f\"\\\\nProcessing files...\")\n",
    "        progress_bar = tqdm(all_files, desc=\"Processing\", unit=\"file\")\n",
    "        \n",
    "        for file_path in progress_bar:\n",
    "            try:\n",
    "                # Read signal\n",
    "                if file_path.suffix.lower() == '.hea':\n",
    "                    signal, fs = read_wfdb(file_path)\n",
    "                else:\n",
    "                    signal, fs = read_mat(file_path)\n",
    "                \n",
    "                # Resample if needed\n",
    "                if fs is not None and not np.isclose(fs, TARGET_FS):\n",
    "                    signal = resample_signal(signal, fs, TARGET_FS)\n",
    "                \n",
    "                # Normalize\n",
    "                signal = zscore_normalize(signal)\n",
    "                \n",
    "                # Pad or truncate\n",
    "                signal = pad_or_truncate(signal, TARGET_SAMPLES)\n",
    "                \n",
    "                # Add channel dimension: (1, TARGET_SAMPLES)\n",
    "                signal = signal[np.newaxis, :]\n",
    "                \n",
    "                # Lookup label\n",
    "                mapped_label = lookup_mapped_label(file_path)\n",
    "                label_int = LABEL_TO_INT.get(mapped_label, LABEL_TO_INT[\"OTHER\"])\n",
    "                \n",
    "                # Generate safe filename\n",
    "                try:\n",
    "                    rel_path = file_path.relative_to(DATASET_DIR).with_suffix(\"\")\n",
    "                    record_id = rel_path.as_posix().replace(\"/\", \"__\")\n",
    "                except Exception:\n",
    "                    record_id = file_path.stem\n",
    "                \n",
    "                # Save processed record\n",
    "                out_file = RECORDS_DIR / f\"{record_id}.npz\"\n",
    "                safe_save_npz(out_file, signal, label_int)\n",
    "                \n",
    "                # Update manifest\n",
    "                manifest.append({\n",
    "                    \"path\": f\"records/{out_file.name}\",\n",
    "                    \"label\": int(label_int)\n",
    "                })\n",
    "                label_counts[label_int] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                if skipped <= 10:  # Only print first 10 errors\n",
    "                    tqdm.write(f\"Error processing {file_path.name}: {e}\")\n",
    "                progress_bar.set_postfix({\"skipped\": skipped})\n",
    "        \n",
    "        progress_bar.close()\n",
    "        \n",
    "        print(f\"\\\\n\" + \"=\"*80)\n",
    "        print(\"PREPROCESSING SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total files processed: {len(manifest):,}\")\n",
    "        print(f\"Files skipped (errors): {skipped:,}\")\n",
    "        print(f\"\\\\nLabel distribution:\")\n",
    "        for idx, label_name in enumerate(LABEL_ORDER):\n",
    "            count = label_counts[idx]\n",
    "            pct = (count / len(manifest) * 100) if manifest else 0\n",
    "            print(f\"  {idx}={label_name:5s}: {count:6,d} ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Save manifest and create splits\n",
    "        print(f\"\\\\nCreating stratified train/val/test splits (80/10/10)...\")\n",
    "        \n",
    "        # Group by label\n",
    "        by_label = defaultdict(list)\n",
    "        for entry in manifest:\n",
    "            by_label[entry['label']].append(entry)\n",
    "        \n",
    "        # Split each label class\n",
    "        train_list, val_list, test_list = [], [], []\n",
    "        rng = np.random.default_rng(seed=DEFAULT_SEED)\n",
    "        \n",
    "        for label, entries in by_label.items():\n",
    "            rng.shuffle(entries)\n",
    "            n = len(entries)\n",
    "            n_train = int(n * 0.80)\n",
    "            n_val = int(n * 0.10)\n",
    "            \n",
    "            train_list.extend(entries[:n_train])\n",
    "            val_list.extend(entries[n_train:n_train + n_val])\n",
    "            test_list.extend(entries[n_train + n_val:])\n",
    "        \n",
    "        # Save splits.json\n",
    "        splits_data = {\n",
    "            \"timestamp\": pd.Timestamp.utcnow().isoformat(),\n",
    "            \"label_order\": LABEL_ORDER,\n",
    "            \"label_to_int\": LABEL_TO_INT,\n",
    "            \"train\": train_list,\n",
    "            \"val\": val_list,\n",
    "            \"test\": test_list,\n",
    "            \"counts\": {\n",
    "                \"train\": len(train_list),\n",
    "                \"val\": len(val_list),\n",
    "                \"test\": len(test_list)\n",
    "            },\n",
    "            \"class_counts\": {int(k): int(v) for k, v in label_counts.items()}\n",
    "        }\n",
    "        \n",
    "        splits_file = PROCESSED_DIR / \"splits.json\"\n",
    "        with open(splits_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(splits_data, f, indent=2)\n",
    "        \n",
    "        # Save label_map.json\n",
    "        label_map = {\n",
    "            \"label_to_int\": LABEL_TO_INT,\n",
    "            \"int_to_label\": INT_TO_LABEL\n",
    "        }\n",
    "        label_map_file = PROCESSED_DIR / \"label_map.json\"\n",
    "        with open(label_map_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(label_map, f, indent=2)\n",
    "        \n",
    "        # Save labels.npy\n",
    "        np.save(PROCESSED_DIR / \"labels.npy\", np.array(LABEL_ORDER, dtype=object))\n",
    "        \n",
    "        print(f\"\\\\nSaved:\")\n",
    "        print(f\"  - {splits_file}\")\n",
    "        print(f\"  - {label_map_file}\")\n",
    "        print(f\"  - {PROCESSED_DIR / 'labels.npy'}\")\n",
    "        print(f\"\\\\nSplit sizes:\")\n",
    "        print(f\"  Train: {len(train_list):,}\")\n",
    "        print(f\"  Val:   {len(val_list):,}\")\n",
    "        print(f\"  Test:  {len(test_list):,}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"PREPROCESSING COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*80)\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 7: PyTorch Dataset and DataLoader (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# PyTorch Dataset for lazy loading of per-record npz files\n",
    "    \n",
    "    class ECGDataset(Dataset):\n",
    "        \\\"\\\"\\\"Lazy-loading dataset for preprocessed ECG records\\\"\\\"\\\"\n",
    "        \n",
    "        def __init__(self, entries, base_dir):\n",
    "            \\\"\\\"\\\"\n",
    "            Args:\n",
    "                entries: List of dicts with 'path' and 'label' keys\n",
    "                base_dir: Base directory for processed files\n",
    "            \\\"\\\"\\\"\n",
    "            self.entries = entries\n",
    "            self.base_dir = Path(base_dir)\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.entries)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            entry = self.entries[idx]\n",
    "            file_path = self.base_dir / entry['path']\n",
    "            \n",
    "            # Load signal and label\n",
    "            signal, label = load_npz_signal(file_path)\n",
    "            \n",
    "            # Convert to tensors\n",
    "            signal_tensor = torch.from_numpy(signal).float()\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "            \n",
    "            return signal_tensor, label_tensor\n",
    "    \n",
    "    \n",
    "    def create_dataloaders(splits_file, processed_dir, batch_size, num_workers=0, pin_memory=False):\n",
    "        \\\"\\\"\\\"Create train, val, and test DataLoaders\\\"\\\"\\\"\n",
    "        \n",
    "        with open(splits_file, 'r') as f:\n",
    "            splits = json.load(f)\n",
    "        \n",
    "        train_dataset = ECGDataset(splits['train'], processed_dir)\n",
    "        val_dataset = ECGDataset(splits['val'], processed_dir)\n",
    "        test_dataset = ECGDataset(splits['test'], processed_dir)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "    \n",
    "    # Create DataLoaders\n",
    "    print(\"Creating DataLoaders...\")\n",
    "    splits_file = PROCESSED_DIR / \"splits.json\"\n",
    "    \n",
    "    if splits_file.exists():\n",
    "        train_loader, val_loader, test_loader = create_dataloaders(\n",
    "            splits_file, \n",
    "            PROCESSED_DIR,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY\n",
    "        )\n",
    "        \n",
    "        print(f\"DataLoaders created:\")\n",
    "        print(f\"  Train batches: {len(train_loader)}\")\n",
    "        print(f\"  Val batches:   {len(val_loader)}\")\n",
    "        print(f\"  Test batches:  {len(test_loader)}\")\n",
    "        \n",
    "        # Show example batch\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        print(f\"\\\\nExample batch shapes:\")\n",
    "        print(f\"  Signals: {sample_batch[0].shape}\")\n",
    "        print(f\"  Labels:  {sample_batch[1].shape}\")\n",
    "        print(f\"  Label values: {sample_batch[1][:min(10, BATCH_SIZE)].tolist()}\")\n",
    "    else:\n",
    "        print(f\"Error: splits.json not found at {splits_file}\")\n",
    "        print(\"Please run the preprocessing cell first.\")\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 8: Model Definition (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# 1D CNN Model for ECG Classification\n",
    "    \n",
    "    class ECGNet1D(nn.Module):\n",
    "        \\\"\\\"\\\"Compact 1D CNN for ECG signal classification\\\"\\\"\\\"\n",
    "        \n",
    "        def __init__(self, n_classes=len(LABEL_ORDER), input_channels=1, base_channels=32, dropout=0.3):\n",
    "            super(ECGNet1D, self).__init__()\n",
    "            \n",
    "            self.conv1 = nn.Conv1d(input_channels, base_channels, kernel_size=7, stride=2, padding=3)\n",
    "            self.bn1 = nn.BatchNorm1d(base_channels)\n",
    "            \n",
    "            self.conv2 = nn.Conv1d(base_channels, base_channels * 2, kernel_size=5, stride=2, padding=2)\n",
    "            self.bn2 = nn.BatchNorm1d(base_channels * 2)\n",
    "            \n",
    "            self.conv3 = nn.Conv1d(base_channels * 2, base_channels * 4, kernel_size=3, stride=2, padding=1)\n",
    "            self.bn3 = nn.BatchNorm1d(base_channels * 4)\n",
    "            \n",
    "            self.conv4 = nn.Conv1d(base_channels * 4, base_channels * 8, kernel_size=3, stride=2, padding=1)\n",
    "            self.bn4 = nn.BatchNorm1d(base_channels * 8)\n",
    "            \n",
    "            self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.fc = nn.Linear(base_channels * 8, n_classes)\n",
    "            \n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # x shape: (batch, 1, samples)\n",
    "            x = self.relu(self.bn1(self.conv1(x)))\n",
    "            x = self.relu(self.bn2(self.conv2(x)))\n",
    "            x = self.relu(self.bn3(self.conv3(x)))\n",
    "            x = self.relu(self.bn4(self.conv4(x)))\n",
    "            x = self.global_pool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "    \n",
    "    \n",
    "    # Instantiate model\n",
    "    model = ECGNet1D(n_classes=len(LABEL_ORDER), base_channels=32, dropout=0.3)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    print(model)\n",
    "    print(f\"\\\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model device: {DEVICE}\")\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 9: Training Loop (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# Training loop with mixed precision and metrics tracking\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = torch.cuda.amp.GradScaler() if USE_MIXED_PRECISION else None\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'train_f1': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_f1': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        train_progress = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "        for signals, labels in train_progress:\n",
    "            signals = signals.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            if USE_MIXED_PRECISION:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(signals)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(signals)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            train_preds.extend(preds)\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            train_progress.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = (np.array(train_preds) == np.array(train_labels)).mean()\n",
    "        train_f1 = f1_score(train_labels, train_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_progress = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "            for signals, labels in val_progress:\n",
    "                signals = signals.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(signals)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                val_preds.extend(preds)\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = (np.array(val_preds) == np.array(val_labels)).mean()\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        # Update history\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f} | Val F1:   {val_f1:.4f}\")\n",
    "        print(f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_path = CHECKPOINTS_DIR / \"best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': val_f1,\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss\n",
    "            }, best_model_path)\n",
    "            print(f\" Saved best model (F1: {val_f1:.4f})\")\n",
    "        \n",
    "        # Step scheduler\n",
    "        scheduler.step()\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Best validation F1: {best_val_f1:.4f} (Epoch {best_epoch})\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = CHECKPOINTS_DIR / \"final_model.pth\"\n",
    "    torch.save({\n",
    "        'epoch': EPOCHS,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'history': history\n",
    "    }, final_model_path)\n",
    "    print(f\"\\\\nSaved final model to {final_model_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_file = PROCESSED_DIR / \"training_history.json\"\n",
    "    with open(history_file, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"Saved training history to {history_file}\")\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 10: Evaluation and Plots (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# Evaluation on test set with metrics and visualizations\n",
    "    \n",
    "    # Load best model\n",
    "    best_model_path = CHECKPOINTS_DIR / \"best_model.pth\"\n",
    "    checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "    print(f\"Best validation F1: {checkpoint['val_f1']:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    test_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_progress = tqdm(test_loader, desc=\"Testing\")\n",
    "        for signals, labels in test_progress:\n",
    "            signals = signals.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(signals)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            test_probs.append(probs.cpu().numpy())\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_probs = np.vstack(test_probs)\n",
    "    test_preds = np.array(test_preds)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_acc = (test_preds == test_labels).mean()\n",
    "    test_f1_macro = f1_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "    test_f1_weighted = f1_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"TEST SET EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1 (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"Test F1 (weighted): {test_f1_weighted:.4f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"\\\\n\" + \"-\"*80)\n",
    "    print(\"Per-Class Metrics:\")\n",
    "    print(\"-\"*80)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        test_labels, test_preds, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    for i, label_name in enumerate(LABEL_ORDER):\n",
    "        print(f\"{label_name:5s}: Precision={precision[i]:.3f}, Recall={recall[i]:.3f}, F1={f1[i]:.3f}, Support={support[i]}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_labels, test_preds)\n",
    "    print(\"\\\\n\" + \"-\"*80)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"-\"*80)\n",
    "    print(cm)\n",
    "    \n",
    "    # Save evaluation results\n",
    "    eval_results = {\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_f1_macro': float(test_f1_macro),\n",
    "        'test_f1_weighted': float(test_f1_weighted),\n",
    "        'per_class_metrics': {\n",
    "            LABEL_ORDER[i]: {\n",
    "                'precision': float(precision[i]),\n",
    "                'recall': float(recall[i]),\n",
    "                'f1': float(f1[i]),\n",
    "                'support': int(support[i])\n",
    "            }\n",
    "            for i in range(len(LABEL_ORDER))\n",
    "        },\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "    \n",
    "    eval_file = PROCESSED_DIR / \"evaluation_results.json\"\n",
    "    with open(eval_file, 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "    print(f\"\\\\nSaved evaluation results to {eval_file}\")\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 11: Visualization Plots (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# Generate and save visualization plots\n",
    "    \n",
    "    # Set plot style\n",
    "    plt.style.use('default')\n",
    "    plt.rcParams['figure.dpi'] = 100\n",
    "    plt.rcParams['savefig.dpi'] = 150\n",
    "    \n",
    "    # 1. Training curves\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "    axes[0, 1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Score\n",
    "    axes[1, 0].plot(history['train_f1'], label='Train F1', linewidth=2)\n",
    "    axes[1, 0].plot(history['val_f1'], label='Val F1', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1 Score (macro)')\n",
    "    axes[1, 0].set_title('Training and Validation F1 Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[1, 1].plot(history['lr'], label='Learning Rate', linewidth=2, color='orange')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].set_title('Learning Rate Schedule')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    training_curves_path = FIGURES_DIR / 'training_curves.png'\n",
    "    plt.savefig(training_curves_path, bbox_inches='tight')\n",
    "    print(f\"Saved training curves to {training_curves_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Confusion Matrix Heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=LABEL_ORDER,\n",
    "           yticklabels=LABEL_ORDER,\n",
    "           xlabel='Predicted Label',\n",
    "           ylabel='True Label',\n",
    "           title='Confusion Matrix')\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                    fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    cm_path = FIGURES_DIR / 'confusion_matrix.png'\n",
    "    plt.savefig(cm_path, bbox_inches='tight')\n",
    "    print(f\"Saved confusion matrix to {cm_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Per-class F1 Score Bar Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = np.arange(len(LABEL_ORDER))\n",
    "    bars = ax.bar(x, f1, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax.set_title('Per-Class F1 Score on Test Set', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(LABEL_ORDER)\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value) in enumerate(zip(bars, f1)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height + 0.02,\n",
    "                f'{value:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    f1_bars_path = FIGURES_DIR / 'per_class_f1.png'\n",
    "    plt.savefig(f1_bars_path, bbox_inches='tight')\n",
    "    print(f\"Saved per-class F1 plot to {f1_bars_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\\\nAll visualizations saved to:\", FIGURES_DIR)\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 12: Smoke Tests (Code)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_code_cell(\"\"\"# Automated smoke tests to verify pipeline integrity\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SMOKE TESTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test 1: Load a single record\n",
    "    print(\"\\\\n1. Testing record loading...\")\n",
    "    try:\n",
    "        test_files = list(RECORDS_DIR.glob(\"*.npz\"))\n",
    "        if test_files:\n",
    "            test_file = test_files[0]\n",
    "            signal, label = load_npz_signal(test_file)\n",
    "            print(f\"    Loaded {test_file.name}\")\n",
    "            print(f\"     Signal shape: {signal.shape}\")\n",
    "            print(f\"     Label: {label} ({LABEL_ORDER[label]})\")\n",
    "            assert signal.shape[1] == TARGET_SAMPLES, \"Signal length mismatch\"\n",
    "            assert 0 <= label < len(LABEL_ORDER), \"Invalid label\"\n",
    "            print(\"    Record validation passed\")\n",
    "        else:\n",
    "            print(\"    No records found\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "    \n",
    "    # Test 2: Model forward pass\n",
    "    print(\"\\\\n2. Testing model forward pass...\")\n",
    "    try:\n",
    "        dummy_input = torch.randn(1, 1, TARGET_SAMPLES).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model(dummy_input)\n",
    "        print(f\"    Input shape: {dummy_input.shape}\")\n",
    "        print(f\"    Output shape: {output.shape}\")\n",
    "        assert output.shape == (1, len(LABEL_ORDER)), \"Output shape mismatch\"\n",
    "        print(\"    Model forward pass successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "    \n",
    "    # Test 3: Checkpoint loading\n",
    "    print(\"\\\\n3. Testing checkpoint loading...\")\n",
    "    try:\n",
    "        best_checkpoint = CHECKPOINTS_DIR / \"best_model.pth\"\n",
    "        if best_checkpoint.exists():\n",
    "            checkpoint = torch.load(best_checkpoint, map_location=DEVICE)\n",
    "            print(f\"    Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "            print(f\"    Val F1: {checkpoint.get('val_f1', 0):.4f}\")\n",
    "            print(\"    Checkpoint loading successful\")\n",
    "        else:\n",
    "            print(f\"    Checkpoint not found at {best_checkpoint}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "    \n",
    "    # Test 4: Dataset integrity\n",
    "    print(\"\\\\n4. Testing dataset integrity...\")\n",
    "    try:\n",
    "        splits_file = PROCESSED_DIR / \"splits.json\"\n",
    "        if splits_file.exists():\n",
    "            with open(splits_file, 'r') as f:\n",
    "                splits = json.load(f)\n",
    "            n_train = len(splits.get('train', []))\n",
    "            n_val = len(splits.get('val', []))\n",
    "            n_test = len(splits.get('test', []))\n",
    "            n_total = n_train + n_val + n_test\n",
    "            print(f\"    Total records: {n_total}\")\n",
    "            print(f\"    Train: {n_train}, Val: {n_val}, Test: {n_test}\")\n",
    "            print(\"    Dataset integrity check passed\")\n",
    "        else:\n",
    "            print(f\"    Splits file not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"SMOKE TESTS COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CELL 13: Final Summary (Markdown)\n",
    "    # ============================================================================\n",
    "    nb.cells.append(new_markdown_cell(\"\"\"---\n",
    "    \n",
    "    ## Pipeline Complete!\n",
    "    \n",
    "    This notebook has successfully completed the full ECG classification pipeline:\n",
    "    \n",
    "     **Preprocessing**: Loaded, resampled, normalized, and saved ECG records  \n",
    "     **Dataset**: Created stratified train/val/test splits  \n",
    "     **Model**: Trained 1D CNN classifier  \n",
    "     **Evaluation**: Generated metrics and visualizations  \n",
    "     **Artifacts**: Saved models, checkpoints, and results  \n",
    "    \n",
    "    ### Output Files\n",
    "    \n",
    "    - **Processed Data**: `artifacts/processed/records/*.npz`\n",
    "    - **Splits**: `artifacts/processed/splits.json`\n",
    "    - **Best Model**: `artifacts/processed/checkpoints/best_model.pth`\n",
    "    - **Training History**: `artifacts/processed/training_history.json`\n",
    "    - **Evaluation Results**: `artifacts/processed/evaluation_results.json`\n",
    "    - **Figures**: `artifacts/figures/*.png`\n",
    "    \n",
    "    ### Next Steps\n",
    "    \n",
    "    1. **Improve Model**: Experiment with deeper architectures (ResNet1D, attention mechanisms)\n",
    "    2. **Hyperparameter Tuning**: Use grid search or Bayesian optimization\n",
    "    3. **Data Augmentation**: Add noise, scaling, time-warping\n",
    "    4. **Multi-Lead Models**: Process all 12 leads instead of averaging\n",
    "    5. **Ensemble Methods**: Combine multiple models for better performance\n",
    "    6. **Deployment**: Export to ONNX or TorchScript for production\n",
    "    \n",
    "    ### Inference Example\n",
    "    \n",
    "    To use the trained model for inference:\n",
    "    \n",
    "    ```python\n",
    "    # Load model\n",
    "    checkpoint = torch.load('artifacts/processed/checkpoints/best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Load a signal\n",
    "    signal, label = load_npz_signal('path/to/record.npz')\n",
    "    signal_tensor = torch.from_numpy(signal).float().unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        output = model(signal_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predicted_class = output.argmax(dim=1).item()\n",
    "        predicted_label = LABEL_ORDER[predicted_class]\n",
    "    \n",
    "    print(f\"Predicted: {predicted_label} (confidence: {probabilities[0, predicted_class]:.2%})\")\n",
    "    ```\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ============================================================================\n",
    "    # Write the notebook to file\n",
    "    # ============================================================================\n",
    "    print(f\"Writing notebook to {OUTPUT_NOTEBOOK}...\")\n",
    "    with open(OUTPUT_NOTEBOOK, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    print(f\" Notebook created successfully!\")\n",
    "    print(f\"  Location: {OUTPUT_NOTEBOOK}\")\n",
    "    print(f\"  Total cells: {len(nb.cells)}\")\n",
    "    print(f\"\\nTo run the notebook:\")\n",
    "    print(f\"  jupyter notebook {OUTPUT_NOTEBOOK}\")\n",
    "    print(f\"\\nOr execute headless:\")\n",
    "    print(f\"  jupyter nbconvert --to notebook --execute {OUTPUT_NOTEBOOK.name}\")\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'create_notebook.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d577f24834e295af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:06.973309Z",
     "start_time": "2025-12-01T15:29:06.935618Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:16.954352Z",
     "iopub.status.busy": "2025-12-02T04:21:16.952345Z",
     "iopub.status.idle": "2025-12-02T04:21:17.088521Z",
     "shell.execute_reply": "2025-12-02T04:21:17.087096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module generate_unified_mapping.py raised name '__file__' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- generate_unified_mapping.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    Generate unified label mapping for ECG datasets.\n",
    "    \n",
    "    This script scans dataset/ folder for known ECG datasets and creates a unified\n",
    "    label mapping CSV with columns: dataset, record_id, original_label_text, mapped_label.\n",
    "    \n",
    "    Supported datasets:\n",
    "    - ptb-xl: Parses ptbxl_database.csv\n",
    "    - CinC2017: Parses REFERENCE*.csv files\n",
    "    - PTB_Diagnostic: Lists WFDB records\n",
    "    - Chapman_Shaoxing: Lists WFDB records\n",
    "    \n",
    "    Output: logs/unified_label_mapping.candidate.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    import re\n",
    "    import csv\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    from collections import Counter\n",
    "    from typing import List, Dict\n",
    "    \n",
    "    # Define project root - adjust if running from different locations\n",
    "    ROOT = Path(__file__).resolve().parent.parent\n",
    "    DATASET_DIR = ROOT / \"Dataset\"\n",
    "    LOGS_DIR = ROOT / \"logs\"\n",
    "    OUTPUT_CSV = LOGS_DIR / \"unified_label_mapping.candidate.csv\"\n",
    "    \n",
    "    # Target label categories (consistent with pipeline)\n",
    "    TARGET_LABELS = [\"MI\", \"AF\", \"BBB\", \"NORM\", \"OTHER\"]\n",
    "    \n",
    "    \n",
    "    def map_label_heuristic(original_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Apply simple heuristic mapping based on keyword patterns.\n",
    "        Returns one of: MI, AF, BBB, NORM, OTHER, or empty string if ambiguous.\n",
    "    \n",
    "        Args:\n",
    "            original_text: Original label text from dataset\n",
    "    \n",
    "        Returns:\n",
    "            Mapped label or empty string if uncertain\n",
    "        \"\"\"\n",
    "        if not original_text:\n",
    "            return \"\"\n",
    "    \n",
    "        text_upper = original_text.upper()\n",
    "    \n",
    "        # Myocardial Infarction patterns\n",
    "        mi_patterns = [\n",
    "            r'\\bMI\\b', r'MYOCARDIAL\\s+INFARC', r'\\bIMI\\b', r'\\bAMI\\b',\n",
    "            r'\\bSTEMI\\b', r'\\bNSTEMI\\b', r'\\bPMI\\b', r'\\bLMI\\b',\n",
    "            r'INFARCT', r'Q\\s*WAVE', r'PATHOLOGICAL\\s+Q'\n",
    "        ]\n",
    "        if any(re.search(pat, text_upper) for pat in mi_patterns):\n",
    "            return \"MI\"\n",
    "    \n",
    "        # Atrial Fibrillation patterns (including flutter)\n",
    "        af_patterns = [\n",
    "            r'\\bAF\\b', r'ATRIAL\\s+FIB', r'A[\\s-]?FIB', r'\\bAFIB\\b',\n",
    "            r'AFIB', r'A\\.FIB', r'\\bAFLT\\b', r'ATRIAL\\s+FLUTTER',\n",
    "            r'A[\\s-]?FLUTTER'\n",
    "        ]\n",
    "        if any(re.search(pat, text_upper) for pat in af_patterns):\n",
    "            return \"AF\"\n",
    "    \n",
    "        # Bundle Branch Block patterns (including fascicular blocks and incomplete blocks)\n",
    "        bbb_patterns = [\n",
    "            r'\\bBBB\\b', r'BUNDLE\\s+BRANCH\\s+BLOCK',\n",
    "            r'\\bLBBB\\b', r'LEFT\\s+BUNDLE\\s+BRANCH',\n",
    "            r'\\bRBBB\\b', r'RIGHT\\s+BUNDLE\\s+BRANCH',\n",
    "            r'\\bIRBBB\\b', r'INCOMPLETE.*BUNDLE',\n",
    "            r'\\bILBBB\\b',\n",
    "            r'IVCD', r'INTRAVENTRICULAR\\s+CONDUCTION',\n",
    "            r'\\bLAFB\\b', r'\\bLPFB\\b', r'FASCICULAR\\s+BLOCK',\n",
    "            r'LEFT\\s+ANTERIOR\\s+FASCICULAR', r'LEFT\\s+POSTERIOR\\s+FASCICULAR'\n",
    "        ]\n",
    "        if any(re.search(pat, text_upper) for pat in bbb_patterns):\n",
    "            return \"BBB\"\n",
    "    \n",
    "        # Normal patterns (be careful - only if explicitly stated)\n",
    "        norm_patterns = [\n",
    "            r'\\bNORM\\b', r'NORMAL\\s+ECG', r'SINUSRHYTHMUS\\s+NORMALES\\s+EKG',\n",
    "            r'^N$', r'^NORMAL$', r'NO\\s+ABNORMAL', r'SINUS\\s+RHYTHM.*NORMAL'\n",
    "        ]\n",
    "        if any(re.search(pat, text_upper) for pat in norm_patterns):\n",
    "            # Additional safety: if text also contains pathological terms, don't map to NORM\n",
    "            pathological_terms = ['INFARCT', 'ISCHEMI', 'HYPERTROPHY', 'BLOCK',\n",
    "                                 'FIBRILLATION', 'FLUTTER', 'TACHYCARDIA', 'BRADYCARDIA']\n",
    "            if not any(term in text_upper for term in pathological_terms):\n",
    "                return \"NORM\"\n",
    "    \n",
    "        # If we can't confidently map, return empty (will be OTHER in preprocessing)\n",
    "        return \"\"\n",
    "    \n",
    "    \n",
    "    def parse_ptbxl(dataset_path: Path) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Parse PTB-XL dataset from ptbxl_database.csv.\n",
    "    \n",
    "        Expected columns: filename_lr (or similar), report, scp_codes\n",
    "    \n",
    "        Returns:\n",
    "            List of dicts with keys: dataset, record_id, original_label_text, mapped_label\n",
    "        \"\"\"\n",
    "        csv_path = dataset_path / \"ptbxl_database.csv\"\n",
    "        if not csv_path.exists():\n",
    "            print(f\"Warning: {csv_path} not found. Skipping PTB-XL.\")\n",
    "            return []\n",
    "    \n",
    "        records = []\n",
    "        try:\n",
    "            with open(csv_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for row in reader:\n",
    "                    # Try multiple possible column names\n",
    "                    filename_lr = row.get('filename_lr') or row.get('filename') or row.get('ecg_id')\n",
    "                    if not filename_lr:\n",
    "                        continue\n",
    "    \n",
    "                    # Clean up filename - remove 'records100/' or 'records500/' prefix\n",
    "                    record_id = filename_lr.replace('records100/', '').replace('records500/', '')\n",
    "                    record_id = record_id.strip()\n",
    "    \n",
    "                    # Get label information (prefer scp_codes, fall back to report)\n",
    "                    scp_codes = row.get('scp_codes', '')\n",
    "                    report = row.get('report', '')\n",
    "                    original_label = scp_codes if scp_codes else report\n",
    "    \n",
    "                    # Map label\n",
    "                    mapped = map_label_heuristic(original_label)\n",
    "    \n",
    "                    records.append({\n",
    "                        'dataset': 'ptb-xl',\n",
    "                        'record_id': record_id,\n",
    "                        'original_label_text': original_label.strip() if original_label else '',\n",
    "                        'mapped_label': mapped\n",
    "                    })\n",
    "    \n",
    "            print(f\" PTB-XL: Parsed {len(records)} records from {csv_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing PTB-XL: {e}\")\n",
    "    \n",
    "        return records\n",
    "    \n",
    "    \n",
    "    def parse_cinc2017(dataset_path: Path) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Parse CinC2017 dataset from REFERENCE*.csv files in training/validation subdirs.\n",
    "    \n",
    "        CinC2017 labels: N (normal), A (AF), O (other), ~ (noisy)\n",
    "    \n",
    "        Returns:\n",
    "            List of dicts with keys: dataset, record_id, original_label_text, mapped_label\n",
    "        \"\"\"\n",
    "        records = []\n",
    "    \n",
    "        # CinC2017 label mapping\n",
    "        cinc_label_map = {\n",
    "            'N': 'NORM',\n",
    "            'A': 'AF',\n",
    "            'O': '',  # Other - leave empty for heuristic or default to OTHER\n",
    "            '~': ''   # Noisy - leave empty\n",
    "        }\n",
    "    \n",
    "        # Look in training, validation, and test subdirectories\n",
    "        subdirs = ['training', 'validation', 'test']\n",
    "        reference_files = []\n",
    "    \n",
    "        for subdir in subdirs:\n",
    "            subdir_path = dataset_path / subdir\n",
    "            if subdir_path.exists():\n",
    "                # Find REFERENCE*.csv files (prefer REFERENCE.csv, then latest version)\n",
    "                ref_files = sorted(subdir_path.glob(\"REFERENCE*.csv\"))\n",
    "                if ref_files:\n",
    "                    # Use the first one (typically REFERENCE.csv or highest version)\n",
    "                    reference_files.append((subdir, ref_files[0]))\n",
    "    \n",
    "        # Also check root directory for REFERENCE files\n",
    "        root_ref_files = sorted(dataset_path.glob(\"REFERENCE*.csv\"))\n",
    "        if root_ref_files:\n",
    "            reference_files.append(('root', root_ref_files[-1]))  # Use latest version\n",
    "    \n",
    "        if not reference_files:\n",
    "            print(f\"Warning: No REFERENCE*.csv files found in {dataset_path}. Skipping CinC2017.\")\n",
    "            return []\n",
    "    \n",
    "        seen_ids = set()\n",
    "        for subdir, ref_file in reference_files:\n",
    "            try:\n",
    "                with open(ref_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "    \n",
    "                        # Format: record_id,label (e.g., \"A00/A00003,N\")\n",
    "                        parts = line.split(',')\n",
    "                        if len(parts) != 2:\n",
    "                            continue\n",
    "    \n",
    "                        record_id_raw, label = parts[0].strip(), parts[1].strip()\n",
    "    \n",
    "                        # Normalize record_id: use forward slashes\n",
    "                        record_id = record_id_raw.replace('\\\\', '/')\n",
    "    \n",
    "                        # Skip duplicates across files\n",
    "                        if record_id in seen_ids:\n",
    "                            continue\n",
    "                        seen_ids.add(record_id)\n",
    "    \n",
    "                        # Map label\n",
    "                        mapped = cinc_label_map.get(label, '')\n",
    "    \n",
    "                        records.append({\n",
    "                            'dataset': 'CinC2017',\n",
    "                            'record_id': record_id,\n",
    "                            'original_label_text': label,\n",
    "                            'mapped_label': mapped\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {ref_file.name}: {e}\")\n",
    "    \n",
    "        print(f\" CinC2017: Parsed {len(records)} records from {len(reference_files)} reference file(s)\")\n",
    "        return records\n",
    "    \n",
    "    \n",
    "    def parse_wfdb_fallback(dataset_path: Path, dataset_name: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Fallback parser for WFDB-format datasets (PTB_Diagnostic, Chapman_Shaoxing).\n",
    "        Lists all .hea files and uses relative path as record_id.\n",
    "    \n",
    "        For these datasets, we don't have direct label info, so original_label_text\n",
    "        will be empty and mapped_label will be empty (will become OTHER in preprocessing).\n",
    "    \n",
    "        Returns:\n",
    "            List of dicts with keys: dataset, record_id, original_label_text, mapped_label\n",
    "        \"\"\"\n",
    "        records = []\n",
    "    \n",
    "        # Check for RECORDS file first\n",
    "        records_file = dataset_path / \"RECORDS\"\n",
    "        if records_file.exists():\n",
    "            try:\n",
    "                with open(records_file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip()\n",
    "                        if not line or line.startswith('#'):\n",
    "                            continue\n",
    "    \n",
    "                        # RECORDS file lists paths relative to dataset root\n",
    "                        record_id = line.replace('\\\\', '/')\n",
    "    \n",
    "                        # For Chapman, RECORDS lists directories; we need to find .hea files\n",
    "                        if dataset_name == 'Chapman_Shaoxing':\n",
    "                            record_path = dataset_path / line\n",
    "                            if record_path.is_dir():\n",
    "                                # Find .hea files in this directory\n",
    "                                hea_files = list(record_path.glob(\"*.hea\"))\n",
    "                                for hea in hea_files:\n",
    "                                    rel_path = hea.relative_to(dataset_path).as_posix()\n",
    "                                    # Remove .hea extension for record_id\n",
    "                                    rel_id = rel_path[:-4] if rel_path.endswith('.hea') else rel_path\n",
    "    \n",
    "                                    # Try to infer label from header if available\n",
    "                                    original_label = extract_label_from_wfdb_header(hea)\n",
    "                                    mapped = map_label_heuristic(original_label)\n",
    "    \n",
    "                                    records.append({\n",
    "                                        'dataset': dataset_name,\n",
    "                                        'record_id': rel_id,\n",
    "                                        'original_label_text': original_label,\n",
    "                                        'mapped_label': mapped\n",
    "                                    })\n",
    "                            else:\n",
    "                                # Single record entry\n",
    "                                original_label = \"\"\n",
    "                                hea_path = dataset_path / f\"{line}.hea\"\n",
    "                                if hea_path.exists():\n",
    "                                    original_label = extract_label_from_wfdb_header(hea_path)\n",
    "    \n",
    "                                mapped = map_label_heuristic(original_label)\n",
    "                                records.append({\n",
    "                                    'dataset': dataset_name,\n",
    "                                    'record_id': record_id,\n",
    "                                    'original_label_text': original_label,\n",
    "                                    'mapped_label': mapped\n",
    "                                })\n",
    "                        else:\n",
    "                            # PTB_Diagnostic: RECORDS lists actual record paths\n",
    "                            original_label = \"\"\n",
    "                            hea_path = dataset_path / f\"{line}.hea\"\n",
    "                            if hea_path.exists():\n",
    "                                original_label = extract_label_from_wfdb_header(hea_path)\n",
    "    \n",
    "                            mapped = map_label_heuristic(original_label)\n",
    "                            records.append({\n",
    "                                'dataset': dataset_name,\n",
    "                                'record_id': record_id,\n",
    "                                'original_label_text': original_label,\n",
    "                                'mapped_label': mapped\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading RECORDS file for {dataset_name}: {e}\")\n",
    "    \n",
    "        # If no RECORDS file or it failed, fall back to globbing\n",
    "        if not records:\n",
    "            print(f\"Scanning {dataset_name} for .hea files (no RECORDS file)...\")\n",
    "            hea_files = list(dataset_path.rglob(\"*.hea\"))\n",
    "            for hea in hea_files:\n",
    "                try:\n",
    "                    rel_path = hea.relative_to(dataset_path).as_posix()\n",
    "                    # Remove .hea extension\n",
    "                    rel_id = rel_path[:-4] if rel_path.endswith('.hea') else rel_path\n",
    "    \n",
    "                    original_label = extract_label_from_wfdb_header(hea)\n",
    "                    mapped = map_label_heuristic(original_label)\n",
    "    \n",
    "                    records.append({\n",
    "                        'dataset': dataset_name,\n",
    "                        'record_id': rel_id,\n",
    "                        'original_label_text': original_label,\n",
    "                        'mapped_label': mapped\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {hea.name}: {e}\")\n",
    "    \n",
    "        print(f\" {dataset_name}: Found {len(records)} records\")\n",
    "        return records\n",
    "    \n",
    "    \n",
    "    def extract_label_from_wfdb_header(hea_path: Path) -> str:\n",
    "        \"\"\"\n",
    "        Attempt to extract diagnostic label from WFDB .hea file comments.\n",
    "        Many WFDB headers contain diagnostic info in comment lines.\n",
    "    \n",
    "        Returns:\n",
    "            Extracted label text or empty string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(hea_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                lines = f.readlines()\n",
    "    \n",
    "            # Look for comment lines (start with #)\n",
    "            comments = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith('#'):\n",
    "                    # Remove leading # and whitespace\n",
    "                    comment = line.lstrip('#').strip()\n",
    "                    comments.append(comment)\n",
    "    \n",
    "            # Combine comments\n",
    "            full_comment = ' '.join(comments)\n",
    "    \n",
    "            # Look for common diagnostic keywords\n",
    "            if any(kw in full_comment.upper() for kw in ['MYOCARDIAL', 'INFARCT', 'MI', 'HEALTHY',\n",
    "                                                           'NORMAL', 'BUNDLE', 'FIBRILLATION']):\n",
    "                return full_comment\n",
    "    \n",
    "            return \"\"\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    \n",
    "    \n",
    "    def deduplicate_records(records: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Remove duplicate entries based on (dataset, record_id) tuple.\n",
    "        Keep the first occurrence.\n",
    "    \n",
    "        Args:\n",
    "            records: List of record dictionaries\n",
    "    \n",
    "        Returns:\n",
    "            Deduplicated list\n",
    "        \"\"\"\n",
    "        seen = set()\n",
    "        unique = []\n",
    "    \n",
    "        for rec in records:\n",
    "            key = (rec['dataset'], rec['record_id'])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique.append(rec)\n",
    "    \n",
    "        duplicates = len(records) - len(unique)\n",
    "        if duplicates > 0:\n",
    "            print(f\"Removed {duplicates} duplicate entries\")\n",
    "    \n",
    "        return unique\n",
    "    \n",
    "    \n",
    "    def write_unified_csv(records: List[Dict[str, str]], output_path: Path):\n",
    "        \"\"\"\n",
    "        Write unified label mapping to CSV file.\n",
    "    \n",
    "        Args:\n",
    "            records: List of record dictionaries\n",
    "            output_path: Output CSV file path\n",
    "        \"\"\"\n",
    "        # Ensure output directory exists\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        # Write CSV\n",
    "        fieldnames = ['dataset', 'record_id', 'original_label_text', 'mapped_label']\n",
    "    \n",
    "        try:\n",
    "            with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(records)\n",
    "    \n",
    "            print(f\"\\n Wrote {len(records)} records to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing CSV: {e}\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    \n",
    "    def print_summary(records: List[Dict[str, str]]):\n",
    "        \"\"\"\n",
    "        Print summary statistics of the generated mapping.\n",
    "    \n",
    "        Args:\n",
    "            records: List of record dictionaries\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "        # Count by dataset\n",
    "        by_dataset = Counter(r['dataset'] for r in records)\n",
    "        print(\"\\nRecords per dataset:\")\n",
    "        for ds, count in sorted(by_dataset.items()):\n",
    "            print(f\"  {ds:25s}: {count:6,d}\")\n",
    "    \n",
    "        # Count by mapped label\n",
    "        by_label = Counter(r['mapped_label'] for r in records)\n",
    "        print(\"\\nRecords per mapped label:\")\n",
    "        for label in TARGET_LABELS:\n",
    "            count = by_label.get(label, 0)\n",
    "            print(f\"  {label:10s}: {count:6,d}\")\n",
    "    \n",
    "        unmapped_count = by_label.get('', 0)\n",
    "        print(f\"  {'(unmapped)':10s}: {unmapped_count:6,d}\")\n",
    "    \n",
    "        # Unmapped percentage\n",
    "        if records:\n",
    "            unmapped_pct = (unmapped_count / len(records)) * 100\n",
    "            print(f\"\\nUnmapped: {unmapped_pct:.1f}%\")\n",
    "            print(\"(Unmapped records will be assigned to OTHER during preprocessing)\")\n",
    "    \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    \n",
    "    def main():\n",
    "        \"\"\"Main execution function.\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"ECG Unified Label Mapping Generator\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Project root: {ROOT}\")\n",
    "        print(f\"Dataset directory: {DATASET_DIR}\")\n",
    "        print(f\"Output: {OUTPUT_CSV}\")\n",
    "        print()\n",
    "    \n",
    "        # Check if dataset directory exists\n",
    "        if not DATASET_DIR.exists():\n",
    "            print(f\"Error: Dataset directory not found: {DATASET_DIR}\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "        all_records = []\n",
    "    \n",
    "        # Parse PTB-XL\n",
    "        ptbxl_path = DATASET_DIR / \"ptb-xl\"\n",
    "        if ptbxl_path.exists():\n",
    "            print(f\"\\nProcessing PTB-XL at {ptbxl_path}...\")\n",
    "            all_records.extend(parse_ptbxl(ptbxl_path))\n",
    "        else:\n",
    "            print(f\"\\nSkipping PTB-XL (not found at {ptbxl_path})\")\n",
    "    \n",
    "        # Parse CinC2017\n",
    "        cinc_path = DATASET_DIR / \"CinC2017\"\n",
    "        if cinc_path.exists():\n",
    "            print(f\"\\nProcessing CinC2017 at {cinc_path}...\")\n",
    "            all_records.extend(parse_cinc2017(cinc_path))\n",
    "        else:\n",
    "            print(f\"\\nSkipping CinC2017 (not found at {cinc_path})\")\n",
    "    \n",
    "        # Parse PTB_Diagnostic\n",
    "        ptb_diag_path = DATASET_DIR / \"PTB_Diagnostic\"\n",
    "        if ptb_diag_path.exists():\n",
    "            print(f\"\\nProcessing PTB_Diagnostic at {ptb_diag_path}...\")\n",
    "            all_records.extend(parse_wfdb_fallback(ptb_diag_path, \"PTB_Diagnostic\"))\n",
    "        else:\n",
    "            print(f\"\\nSkipping PTB_Diagnostic (not found at {ptb_diag_path})\")\n",
    "    \n",
    "        # Parse Chapman_Shaoxing\n",
    "        chapman_path = DATASET_DIR / \"Chapman_Shaoxing\"\n",
    "        if chapman_path.exists():\n",
    "            print(f\"\\nProcessing Chapman_Shaoxing at {chapman_path}...\")\n",
    "            all_records.extend(parse_wfdb_fallback(chapman_path, \"Chapman_Shaoxing\"))\n",
    "        else:\n",
    "            print(f\"\\nSkipping Chapman_Shaoxing (not found at {chapman_path})\")\n",
    "    \n",
    "        # Check if we found any records\n",
    "        if not all_records:\n",
    "            print(\"\\nError: No records found in any dataset!\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "        print(f\"\\nTotal records collected: {len(all_records)}\")\n",
    "    \n",
    "        # Deduplicate\n",
    "        print(\"\\nDeduplicating records...\")\n",
    "        unique_records = deduplicate_records(all_records)\n",
    "    \n",
    "        # Write output\n",
    "        print(f\"\\nWriting output to {OUTPUT_CSV}...\")\n",
    "        write_unified_csv(unique_records, OUTPUT_CSV)\n",
    "    \n",
    "        # Print summary\n",
    "        print_summary(unique_records)\n",
    "    \n",
    "        print(\"\\n Done!\")\n",
    "        print(f\"\\nNext steps:\")\n",
    "        print(f\"1. Review the generated file: {OUTPUT_CSV}\")\n",
    "        print(f\"2. Manually refine any ambiguous mappings if needed\")\n",
    "        print(f\"3. Copy/rename to logs/unified_label_mapping.csv when ready\")\n",
    "    \n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'generate_unified_mapping.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da54b574a8b94a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:07.009203Z",
     "start_time": "2025-12-01T15:29:06.995096Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.098097Z",
     "iopub.status.busy": "2025-12-02T04:21:17.098097Z",
     "iopub.status.idle": "2025-12-02T04:21:17.138731Z",
     "shell.execute_reply": "2025-12-02T04:21:17.136714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module improve_mapping.py raised name '__file__' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- improve_mapping.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    Improved label mapping using enhanced heuristics.\n",
    "    Analyzes unmapped records and attempts to assign labels based on pattern matching.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from pathlib import Path\n",
    "    from collections import Counter\n",
    "    \n",
    "    ROOT = Path(__file__).parent.parent\n",
    "    LOGS_DIR = ROOT / \"logs\"\n",
    "    \n",
    "    # Load current mapping\n",
    "    mapping_file = LOGS_DIR / \"unified_label_mapping.csv\"\n",
    "    df = pd.read_csv(mapping_file, dtype=str).fillna(\"\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"LABEL MAPPING IMPROVEMENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Identify unmapped\n",
    "    unmapped = df[df['mapped_label'] == ''].copy()\n",
    "    print(f\"\\nUnmapped records: {len(unmapped):,}\")\n",
    "    \n",
    "    # Enhanced heuristics\n",
    "    def enhanced_label_heuristic(original_text: str, dataset: str) -> str:\n",
    "        \"\"\"\n",
    "        Apply enhanced heuristics to guess labels.\n",
    "        Returns label or empty string if uncertain.\n",
    "        \"\"\"\n",
    "        if not original_text:\n",
    "            return \"\"\n",
    "    \n",
    "        text_lower = str(original_text).lower()\n",
    "    \n",
    "        # MI patterns (more comprehensive)\n",
    "        mi_patterns = [\n",
    "            r'\\bmi\\b', r'myocardial\\s*infarction', r'\\bstemi\\b', r'\\bnstemi\\b',\n",
    "            r'anterior\\s*infarct', r'inferior\\s*infarct', r'lateral\\s*infarct',\n",
    "            r'old\\s*infarct', r'recent\\s*infarct', r'\\bami\\b', r'\\bpmi\\b',\n",
    "            r'q\\s*wave', r'pathologic.*q', r'\\bimis?\\b'\n",
    "        ]\n",
    "    \n",
    "        # AF patterns\n",
    "        af_patterns = [\n",
    "            r'\\baf\\b', r'atrial\\s*fib', r'a[\\s-]*fib', r'\\bafib\\b', r'\\baflu?\\b',\n",
    "            r'atrial\\s*flutter', r'a[\\s-]*flutter', r'\\bpaf\\b'\n",
    "        ]\n",
    "    \n",
    "        # BBB patterns\n",
    "        bbb_patterns = [\n",
    "            r'\\bbbb\\b', r'bundle\\s*branch\\s*block', r'\\blbbb\\b', r'\\brbbb\\b',\n",
    "            r'left\\s*bundle', r'right\\s*bundle', r'bifascicular',\n",
    "            r'incomplete.*bundle', r'complete.*bundle'\n",
    "        ]\n",
    "    \n",
    "        # NORM patterns\n",
    "        norm_patterns = [\n",
    "            r'\\bnorm(al)?\\b', r'sinus\\s*rhythm', r'\\bsr\\b', r'regular\\s*rhythm',\n",
    "            r'no\\s*abnormal', r'within\\s*normal', r'unremarkable',\n",
    "            r'^normal$', r'healthy', r'control'\n",
    "        ]\n",
    "    \n",
    "        # Check patterns (order matters - specific before general)\n",
    "        for pattern in mi_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                return \"MI\"\n",
    "    \n",
    "        for pattern in af_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                return \"AF\"\n",
    "    \n",
    "        for pattern in bbb_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                return \"BBB\"\n",
    "    \n",
    "        for pattern in norm_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                return \"NORM\"\n",
    "    \n",
    "        # Dataset-specific rules\n",
    "        if dataset == \"PTB_Diagnostic\":\n",
    "            # PTB often has \"Healthy control\" or specific condition names\n",
    "            if \"healthy\" in text_lower or \"control\" in text_lower:\n",
    "                return \"NORM\"\n",
    "            if \"myocardial infarction\" in text_lower:\n",
    "                return \"MI\"\n",
    "    \n",
    "        if dataset == \"ptb-xl\":\n",
    "            # PTBXL uses SCP codes, check for common patterns\n",
    "            if \"norm\" in text_lower:\n",
    "                return \"NORM\"\n",
    "    \n",
    "        return \"\"\n",
    "    \n",
    "    # Apply enhanced heuristics to unmapped records\n",
    "    print(\"\\nApplying enhanced heuristics...\")\n",
    "    improvements = []\n",
    "    \n",
    "    for idx, row in unmapped.iterrows():\n",
    "        original = str(row.get('original_label_text', '')).strip()\n",
    "        dataset = str(row.get('dataset', '')).strip()\n",
    "    \n",
    "        new_label = enhanced_label_heuristic(original, dataset)\n",
    "    \n",
    "        if new_label:\n",
    "            improvements.append({\n",
    "                'index': idx,\n",
    "                'dataset': dataset,\n",
    "                'record_id': row.get('record_id', ''),\n",
    "                'original_text': original,\n",
    "                'suggested_label': new_label\n",
    "            })\n",
    "    \n",
    "    print(f\"Found {len(improvements):,} potential improvements\")\n",
    "    \n",
    "    # Show distribution of suggested labels\n",
    "    if improvements:\n",
    "        suggested_counts = Counter(imp['suggested_label'] for imp in improvements)\n",
    "        print(f\"\\nSuggested label distribution:\")\n",
    "        for label, count in suggested_counts.most_common():\n",
    "            pct = count / len(improvements) * 100\n",
    "            print(f\"  {label}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "        # Save suggestions to file\n",
    "        improvements_df = pd.DataFrame(improvements)\n",
    "        output_file = LOGS_DIR / \"mapping_improvements_suggested.csv\"\n",
    "        improvements_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\n Saved suggestions to: {output_file}\")\n",
    "    \n",
    "        # Show samples from each label\n",
    "        print(f\"\\nExample suggestions (5 per label):\")\n",
    "        for label in ['MI', 'AF', 'BBB', 'NORM']:\n",
    "            label_samples = improvements_df[improvements_df['suggested_label'] == label].head(5)\n",
    "            if not label_samples.empty:\n",
    "                print(f\"\\n  {label}:\")\n",
    "                for _, row in label_samples.iterrows():\n",
    "                    print(f\"    - [{row['dataset']}] {row['original_text'][:60]}\")\n",
    "    \n",
    "        # Option to apply improvements\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TO APPLY THESE IMPROVEMENTS:\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Review the suggestions in: logs/mapping_improvements_suggested.csv\")\n",
    "        print(\"Then run: python scripts/apply_mapping_improvements.py\")\n",
    "        print(\"\\nThis will create an updated unified_label_mapping.csv file.\")\n",
    "    else:\n",
    "        print(\"\\nNo improvements found with current heuristics.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'improve_mapping.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd369be651757ad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:07.036146Z",
     "start_time": "2025-12-01T15:29:07.019850Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.142760Z",
     "iopub.status.busy": "2025-12-02T04:21:17.142760Z",
     "iopub.status.idle": "2025-12-02T04:21:17.188481Z",
     "shell.execute_reply": "2025-12-02T04:21:17.186466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module model_smoke_test.py raised name '__file__' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- model_smoke_test.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"Model smoke test - verify shapes and forward pass\"\"\"\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Simple 1D CNN for testing\n",
    "    class ECGNet1D(nn.Module):\n",
    "        def __init__(self, n_classes=5, input_channels=1, base_channels=32, dropout=0.3):\n",
    "            super(ECGNet1D, self).__init__()\n",
    "            \n",
    "            self.conv1 = nn.Conv1d(input_channels, base_channels, kernel_size=7, stride=2, padding=3)\n",
    "            self.bn1 = nn.BatchNorm1d(base_channels)\n",
    "            \n",
    "            self.conv2 = nn.Conv1d(base_channels, base_channels * 2, kernel_size=5, stride=2, padding=2)\n",
    "            self.bn2 = nn.BatchNorm1d(base_channels * 2)\n",
    "            \n",
    "            self.conv3 = nn.Conv1d(base_channels * 2, base_channels * 4, kernel_size=3, stride=2, padding=1)\n",
    "            self.bn3 = nn.BatchNorm1d(base_channels * 4)\n",
    "            \n",
    "            self.conv4 = nn.Conv1d(base_channels * 4, base_channels * 8, kernel_size=3, stride=2, padding=1)\n",
    "            self.bn4 = nn.BatchNorm1d(base_channels * 8)\n",
    "            \n",
    "            self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.fc = nn.Linear(base_channels * 8, n_classes)\n",
    "            \n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.bn1(self.conv1(x)))\n",
    "            x = self.relu(self.bn2(self.conv2(x)))\n",
    "            x = self.relu(self.bn3(self.conv3(x)))\n",
    "            x = self.relu(self.bn4(self.conv4(x)))\n",
    "            x = self.global_pool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "    \n",
    "    \n",
    "    ROOT = Path(__file__).parent.parent\n",
    "    PROCESSED_DIR = ROOT / \"artifacts\" / \"processed\"\n",
    "    RECORDS_DIR = PROCESSED_DIR / \"records\"\n",
    "    CHECKPOINTS_DIR = PROCESSED_DIR / \"checkpoints\"\n",
    "    CHECKPOINTS_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"MODEL SMOKE TEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nDevice: {device}\")\n",
    "    \n",
    "    # Load splits\n",
    "    splits_file = PROCESSED_DIR / \"splits.json\"\n",
    "    with open(splits_file, 'r') as f:\n",
    "        splits = json.load(f)\n",
    "    \n",
    "    # Load 3 samples from each label (if available)\n",
    "    print(\"\\nLoading sample records from each label...\")\n",
    "    by_label = {}\n",
    "    for entry in splits['train'][:100]:  # Check first 100\n",
    "        label = entry['label']\n",
    "        if label not in by_label:\n",
    "            by_label[label] = []\n",
    "        if len(by_label[label]) < 3:\n",
    "            by_label[label].append(entry)\n",
    "    \n",
    "    print(f\"Loaded samples for {len(by_label)} labels\")\n",
    "    \n",
    "    # Load signals into a batch\n",
    "    signals = []\n",
    "    labels = []\n",
    "    for label, entries in by_label.items():\n",
    "        for entry in entries:\n",
    "            npy_file = PROCESSED_DIR / entry['path']\n",
    "            signal = np.load(npy_file, allow_pickle=False)\n",
    "            signals.append(signal)\n",
    "            labels.append(label)\n",
    "    \n",
    "    batch_signals = torch.from_numpy(np.stack(signals)).float()\n",
    "    batch_labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    print(f\"\\nBatch shape: {batch_signals.shape}\")\n",
    "    print(f\"Labels shape: {batch_labels.shape}\")\n",
    "    print(f\"Label values: {batch_labels.tolist()}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = ECGNet1D(n_classes=5, base_channels=32, dropout=0.3)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nModel parameters: {total_params:,}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    batch_signals = batch_signals.to(device)\n",
    "    batch_labels = batch_labels.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_signals)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "    \n",
    "    print(f\"\\nForward pass successful!\")\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "    print(f\"Predictions: {predictions.cpu().tolist()}\")\n",
    "    print(f\"True labels: {batch_labels.cpu().tolist()}\")\n",
    "    \n",
    "    # Save skeleton checkpoint\n",
    "    checkpoint_path = CHECKPOINTS_DIR / \"checkpoint_skel.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'n_classes': 5,\n",
    "        'base_channels': 32,\n",
    "        'dropout': 0.3\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"\\n Saved skeleton checkpoint to {checkpoint_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL SMOKE TEST PASSED\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'model_smoke_test.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76482227a2e591cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:07.093322Z",
     "start_time": "2025-12-01T15:29:07.058457Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.193725Z",
     "iopub.status.busy": "2025-12-02T04:21:17.193725Z",
     "iopub.status.idle": "2025-12-02T04:21:17.314263Z",
     "shell.execute_reply": "2025-12-02T04:21:17.314263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module preprocess_streaming.py raised name '__file__' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- preprocess_streaming.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    Memory-safe, idempotent streaming preprocessing for ECG datasets.\n",
    "    Saves individual .npy files with metadata and creates manifest.jsonl for lazy loading.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import time\n",
    "    import logging\n",
    "    from pathlib import Path\n",
    "    from collections import Counter, defaultdict\n",
    "    from datetime import datetime\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy.signal import resample\n",
    "    from scipy.io import loadmat\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Windows asyncio fix\n",
    "    if sys.platform == \"win32\":\n",
    "        import asyncio\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "    \n",
    "    # Configuration\n",
    "    ROOT = Path(__file__).parent.parent\n",
    "    DATASET_DIR = ROOT / \"Dataset\"\n",
    "    ARTIFACTS_DIR = ROOT / \"artifacts\"\n",
    "    PROCESSED_DIR = ARTIFACTS_DIR / \"processed\"\n",
    "    RECORDS_DIR = PROCESSED_DIR / \"records\"\n",
    "    LOGS_DIR = ROOT / \"logs\"\n",
    "    \n",
    "    # Create directories\n",
    "    for d in [ARTIFACTS_DIR, PROCESSED_DIR, RECORDS_DIR, LOGS_DIR]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Setup logging\n",
    "    log_file = LOGS_DIR / \"preprocess_automation.log\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, mode='a'),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Constants\n",
    "    TARGET_FS = 500\n",
    "    TARGET_SAMPLES = 5000\n",
    "    LABEL_ORDER = ['MI', 'AF', 'BBB', 'NORM', 'OTHER']\n",
    "    LABEL_TO_INT = {name: i for i, name in enumerate(LABEL_ORDER)}\n",
    "    INT_TO_LABEL = {i: name for name, i in LABEL_TO_INT.items()}\n",
    "    \n",
    "    # Environment variable for smoke testing\n",
    "    PREPROCESS_LIMIT = int(os.getenv('ECG_PREPROCESS_LIMIT', '0'))\n",
    "    \n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"ECG STREAMING PREPROCESSING\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"ROOT: {ROOT}\")\n",
    "    logger.info(f\"DATASET_DIR: {DATASET_DIR}\")\n",
    "    logger.info(f\"RECORDS_DIR: {RECORDS_DIR}\")\n",
    "    logger.info(f\"PREPROCESS_LIMIT: {PREPROCESS_LIMIT if PREPROCESS_LIMIT > 0 else 'UNLIMITED'}\")\n",
    "    \n",
    "    \n",
    "    def zscore_normalize(arr: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Z-score normalization\"\"\"\n",
    "        arr = arr.astype(np.float32)\n",
    "        mean = arr.mean()\n",
    "        std = arr.std()\n",
    "        if std < 1e-8:\n",
    "            std = 1.0\n",
    "        return ((arr - mean) / std).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    def pad_or_truncate(x: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"Pad with zeros or truncate\"\"\"\n",
    "        if x.size >= target_length:\n",
    "            return x[:target_length]\n",
    "        pad_width = target_length - x.size\n",
    "        return np.pad(x, (0, pad_width), mode='constant', constant_values=0).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    def resample_signal(x: np.ndarray, original_fs: float, target_fs: float) -> np.ndarray:\n",
    "        \"\"\"Resample signal\"\"\"\n",
    "        if original_fs is None or np.isclose(original_fs, target_fs):\n",
    "            return x\n",
    "        new_length = int(round(x.size * target_fs / original_fs))\n",
    "        if new_length <= 0:\n",
    "            return x\n",
    "        return resample(x, new_length).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    def read_wfdb(hea_path: Path):\n",
    "        \"\"\"Read WFDB format (.hea/.dat)\"\"\"\n",
    "        try:\n",
    "            import wfdb\n",
    "            record_path = str(hea_path.with_suffix(''))\n",
    "            record = wfdb.rdsamp(record_path)\n",
    "            signal = np.asarray(record[0], dtype=np.float32)\n",
    "            fs = float(record[1].get('fs', TARGET_FS))\n",
    "    \n",
    "            # Convert to 1D\n",
    "            if signal.ndim == 2:\n",
    "                signal_1d = signal.mean(axis=1) if signal.shape[1] > 1 else signal[:, 0]\n",
    "            else:\n",
    "                signal_1d = signal.reshape(-1)\n",
    "    \n",
    "            return signal_1d.astype(np.float32), fs\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read WFDB {hea_path.name}: {e}\")\n",
    "    \n",
    "    \n",
    "    def read_mat(mat_path: Path):\n",
    "        \"\"\"Read MATLAB .mat file\"\"\"\n",
    "        try:\n",
    "            mat_data = loadmat(str(mat_path))\n",
    "    \n",
    "            signal = None\n",
    "            for key in ['val', 'data', 'signal', 'ecg']:\n",
    "                if key in mat_data:\n",
    "                    signal = np.asarray(mat_data[key], dtype=np.float32)\n",
    "                    break\n",
    "    \n",
    "            if signal is None:\n",
    "                for value in mat_data.values():\n",
    "                    if isinstance(value, np.ndarray) and value.size > 100:\n",
    "                        signal = value.astype(np.float32)\n",
    "                        break\n",
    "    \n",
    "            if signal is None:\n",
    "                raise RuntimeError(\"No signal array found\")\n",
    "    \n",
    "            # Convert to 1D\n",
    "            if signal.ndim == 2:\n",
    "                signal_1d = signal.mean(axis=0) if signal.shape[0] > 1 else signal.reshape(-1)\n",
    "            else:\n",
    "                signal_1d = signal.reshape(-1)\n",
    "    \n",
    "            return signal_1d.astype(np.float32), None\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read MAT {mat_path.name}: {e}\")\n",
    "    \n",
    "    \n",
    "    def load_mapping_index():\n",
    "        \"\"\"Load unified label mapping\"\"\"\n",
    "        mapping_file = LOGS_DIR / \"unified_label_mapping.csv\"\n",
    "        mapping_index = {}\n",
    "    \n",
    "        if not mapping_file.exists():\n",
    "            logger.warning(f\"Mapping file not found: {mapping_file}\")\n",
    "            return mapping_index\n",
    "    \n",
    "        df = pd.read_csv(mapping_file, dtype=str).fillna(\"\")\n",
    "    \n",
    "        for _, row in df.iterrows():\n",
    "            dataset = str(row.get(\"dataset\", \"\")).strip()\n",
    "            record_id = str(row.get(\"record_id\", \"\")).strip().replace(\"\\\\\", \"/\").strip(\"/\")\n",
    "            mapped_label = str(row.get(\"mapped_label\", \"\")).strip().upper()\n",
    "    \n",
    "            if not dataset or not record_id:\n",
    "                continue\n",
    "    \n",
    "            if dataset not in mapping_index:\n",
    "                mapping_index[dataset] = {}\n",
    "    \n",
    "            # Add full path\n",
    "            mapping_index[dataset][record_id] = mapped_label\n",
    "    \n",
    "            # Add variants\n",
    "            parts = record_id.split(\"/\")\n",
    "            if len(parts) >= 1:\n",
    "                mapping_index[dataset][parts[-1]] = mapped_label\n",
    "            if len(parts) >= 2:\n",
    "                mapping_index[dataset][\"/\".join(parts[-2:])] = mapped_label\n",
    "    \n",
    "        logger.info(f\"Loaded mapping for {len(mapping_index)} datasets\")\n",
    "        return mapping_index\n",
    "    \n",
    "    \n",
    "    def lookup_mapped_label(path: Path, mapping_index: dict) -> str:\n",
    "        \"\"\"Look up mapped label\"\"\"\n",
    "        try:\n",
    "            rel_path = path.relative_to(DATASET_DIR).with_suffix(\"\")\n",
    "        except Exception:\n",
    "            rel_path = path.with_suffix(\"\")\n",
    "    \n",
    "        parts = rel_path.as_posix().split(\"/\")\n",
    "        dataset = parts[0] if parts else \"\"\n",
    "    \n",
    "        if dataset not in mapping_index:\n",
    "            return \"OTHER\"\n",
    "    \n",
    "        index = mapping_index[dataset]\n",
    "    \n",
    "        candidates = [\n",
    "            rel_path.as_posix(),\n",
    "            \"/\".join(parts[1:]) if len(parts) > 1 else \"\",\n",
    "            \"/\".join(parts[-2:]) if len(parts) >= 2 else \"\",\n",
    "            rel_path.name\n",
    "        ]\n",
    "    \n",
    "        for key in candidates:\n",
    "            if key and key in index:\n",
    "                label = index[key].upper()\n",
    "                return label if label in LABEL_TO_INT else \"OTHER\"\n",
    "    \n",
    "        # CinC special case\n",
    "        if \"CinC\" in dataset and len(parts) >= 3:\n",
    "            alt_key = \"/\".join(parts[2:])\n",
    "            if alt_key in index:\n",
    "                label = index[alt_key].upper()\n",
    "                return label if label in LABEL_TO_INT else \"OTHER\"\n",
    "    \n",
    "        return \"OTHER\"\n",
    "    \n",
    "    \n",
    "    def load_progress_checkpoint():\n",
    "        \"\"\"Load progress checkpoint\"\"\"\n",
    "        checkpoint_file = PROCESSED_DIR / \"progress.json\"\n",
    "        if checkpoint_file.exists():\n",
    "            with open(checkpoint_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\"processed_files\": set(), \"last_index\": 0, \"processed_count\": 0}\n",
    "    \n",
    "    \n",
    "    def save_progress_checkpoint(processed_files: set, last_index: int, processed_count: int):\n",
    "        \"\"\"Save progress checkpoint\"\"\"\n",
    "        checkpoint_file = PROCESSED_DIR / \"progress.json\"\n",
    "        checkpoint = {\n",
    "            \"processed_files\": list(processed_files),\n",
    "            \"last_index\": last_index,\n",
    "            \"processed_count\": processed_count,\n",
    "            \"timestamp\": datetime.utcnow().isoformat()\n",
    "        }\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint, f, indent=2)\n",
    "    \n",
    "    \n",
    "    def main():\n",
    "        \"\"\"Main preprocessing pipeline\"\"\"\n",
    "        logger.info(\"Starting preprocessing...\")\n",
    "    \n",
    "        # Load mapping\n",
    "        mapping_index = load_mapping_index()\n",
    "    \n",
    "        # Load progress checkpoint\n",
    "        checkpoint = load_progress_checkpoint()\n",
    "        processed_files = set(checkpoint.get(\"processed_files\", []))\n",
    "        logger.info(f\"Loaded checkpoint: {len(processed_files)} files already processed\")\n",
    "    \n",
    "        # Find all files\n",
    "        logger.info(f\"Scanning {DATASET_DIR}...\")\n",
    "        hea_files = sorted(DATASET_DIR.rglob(\"*.hea\"))\n",
    "        mat_files = sorted(DATASET_DIR.rglob(\"*.mat\"))\n",
    "        all_files = hea_files + mat_files\n",
    "    \n",
    "        logger.info(f\"Found {len(hea_files)} .hea and {len(mat_files)} .mat files\")\n",
    "    \n",
    "        # Apply limit for smoke testing\n",
    "        if PREPROCESS_LIMIT > 0:\n",
    "            all_files = all_files[:PREPROCESS_LIMIT]\n",
    "            logger.info(f\"LIMITED TO: {len(all_files)} files (ECG_PREPROCESS_LIMIT={PREPROCESS_LIMIT})\")\n",
    "    \n",
    "        # Filter out already processed\n",
    "        all_files = [f for f in all_files if str(f) not in processed_files]\n",
    "        logger.info(f\"Files to process (after skipping existing): {len(all_files)}\")\n",
    "    \n",
    "        if not all_files:\n",
    "            logger.info(\"No new files to process!\")\n",
    "            return\n",
    "    \n",
    "        # Open manifest file in append mode\n",
    "        manifest_file = PROCESSED_DIR / \"manifest.jsonl\"\n",
    "        manifest_fp = open(manifest_file, 'a', encoding='utf-8')\n",
    "    \n",
    "        # Processing loop\n",
    "        label_counts = Counter()\n",
    "        skipped = 0\n",
    "        start_time = time.time()\n",
    "        save_times = []\n",
    "    \n",
    "        progress_bar = tqdm(all_files, desc=\"Processing\", unit=\"file\")\n",
    "    \n",
    "        for idx, file_path in enumerate(progress_bar):\n",
    "            try:\n",
    "                # Generate record ID\n",
    "                try:\n",
    "                    rel_path = file_path.relative_to(DATASET_DIR).with_suffix(\"\")\n",
    "                    record_id = rel_path.as_posix().replace(\"/\", \"__\")\n",
    "                except Exception:\n",
    "                    record_id = file_path.stem\n",
    "    \n",
    "                # Check if already exists\n",
    "                npy_file = RECORDS_DIR / f\"{record_id}.npy\"\n",
    "                meta_file = RECORDS_DIR / f\"{record_id}.meta.json\"\n",
    "                label_file = RECORDS_DIR / f\"{record_id}.label\"\n",
    "    \n",
    "                if npy_file.exists() and meta_file.exists() and label_file.exists():\n",
    "                    processed_files.add(str(file_path))\n",
    "                    continue\n",
    "    \n",
    "                # Read signal\n",
    "                read_start = time.time()\n",
    "                if file_path.suffix.lower() == '.hea':\n",
    "                    signal, fs = read_wfdb(file_path)\n",
    "                else:\n",
    "                    signal, fs = read_mat(file_path)\n",
    "    \n",
    "                # Resample\n",
    "                if fs is not None and not np.isclose(fs, TARGET_FS):\n",
    "                    signal = resample_signal(signal, fs, TARGET_FS)\n",
    "    \n",
    "                # Normalize\n",
    "                signal = zscore_normalize(signal)\n",
    "    \n",
    "                # Pad/truncate\n",
    "                signal = pad_or_truncate(signal, TARGET_SAMPLES)\n",
    "    \n",
    "                # Add channel dimension\n",
    "                signal = signal[np.newaxis, :]\n",
    "    \n",
    "                # Lookup label\n",
    "                mapped_label = lookup_mapped_label(file_path, mapping_index)\n",
    "                label_int = LABEL_TO_INT.get(mapped_label, LABEL_TO_INT[\"OTHER\"])\n",
    "    \n",
    "                # Save files\n",
    "                save_start = time.time()\n",
    "                np.save(npy_file, signal, allow_pickle=False)\n",
    "    \n",
    "                with open(label_file, 'w') as f:\n",
    "                    f.write(str(label_int))\n",
    "    \n",
    "                meta = {\n",
    "                    \"dataset\": rel_path.parts[0] if len(rel_path.parts) > 0 else \"unknown\",\n",
    "                    \"source_path\": str(file_path.relative_to(DATASET_DIR)),\n",
    "                    \"original_fs\": float(fs) if fs is not None else None,\n",
    "                    \"mapped_label\": mapped_label,\n",
    "                    \"label_int\": int(label_int)\n",
    "                }\n",
    "                with open(meta_file, 'w') as f:\n",
    "                    json.dump(meta, f)\n",
    "    \n",
    "                # Append to manifest\n",
    "                manifest_entry = {\n",
    "                    \"path\": f\"records/{npy_file.name}\",\n",
    "                    \"label\": int(label_int)\n",
    "                }\n",
    "                manifest_fp.write(json.dumps(manifest_entry) + '\\n')\n",
    "                manifest_fp.flush()\n",
    "    \n",
    "                save_times.append(time.time() - save_start)\n",
    "    \n",
    "                # Update counters\n",
    "                label_counts[label_int] += 1\n",
    "                processed_files.add(str(file_path))\n",
    "    \n",
    "                # Periodic checkpoint save\n",
    "                if (idx + 1) % 1000 == 0:\n",
    "                    save_progress_checkpoint(processed_files, idx, len(processed_files))\n",
    "                    elapsed = time.time() - start_time\n",
    "                    speed = len(processed_files) / elapsed\n",
    "                    avg_save_time = np.mean(save_times[-100:]) if save_times else 0\n",
    "                    progress_bar.set_postfix({\n",
    "                        'rec/s': f'{speed:.1f}',\n",
    "                        'skip': skipped,\n",
    "                        'save_ms': f'{avg_save_time*1000:.1f}'\n",
    "                    })\n",
    "    \n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                if skipped <= 20:\n",
    "                    logger.error(f\"Error processing {file_path.name}: {e}\")\n",
    "                progress_bar.set_postfix({'skipped': skipped})\n",
    "    \n",
    "        progress_bar.close()\n",
    "        manifest_fp.close()\n",
    "    \n",
    "        # Final checkpoint\n",
    "        save_progress_checkpoint(processed_files, len(all_files), len(processed_files))\n",
    "    \n",
    "        # Print summary\n",
    "        elapsed = time.time() - start_time\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(\"PREPROCESSING SUMMARY\")\n",
    "        logger.info(\"=\"*80)\n",
    "        logger.info(f\"Total processed: {len(processed_files):,}\")\n",
    "        logger.info(f\"Skipped (errors): {skipped:,}\")\n",
    "        logger.info(f\"Time elapsed: {elapsed:.1f}s ({len(processed_files)/elapsed:.1f} rec/s)\")\n",
    "        logger.info(f\"\\nLabel distribution:\")\n",
    "        for idx, label_name in enumerate(LABEL_ORDER):\n",
    "            count = label_counts[idx]\n",
    "            pct = (count / len(processed_files) * 100) if processed_files else 0\n",
    "            logger.info(f\"  {idx}={label_name:5s}: {count:6,d} ({pct:5.1f}%)\")\n",
    "    \n",
    "        # Create splits\n",
    "        create_splits()\n",
    "    \n",
    "        logger.info(\"\\nPreprocessing complete!\")\n",
    "    \n",
    "    \n",
    "    def create_splits():\n",
    "        \"\"\"Create stratified train/val/test splits\"\"\"\n",
    "        logger.info(\"\\nCreating stratified splits...\")\n",
    "    \n",
    "        # Read manifest\n",
    "        manifest_file = PROCESSED_DIR / \"manifest.jsonl\"\n",
    "        if not manifest_file.exists():\n",
    "            logger.error(\"Manifest file not found!\")\n",
    "            return\n",
    "    \n",
    "        manifest = []\n",
    "        with open(manifest_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    manifest.append(json.loads(line))\n",
    "    \n",
    "        logger.info(f\"Loaded {len(manifest)} entries from manifest\")\n",
    "    \n",
    "        # Group by label\n",
    "        by_label = defaultdict(list)\n",
    "        for entry in manifest:\n",
    "            by_label[entry['label']].append(entry)\n",
    "    \n",
    "        # Stratified split\n",
    "        train_list, val_list, test_list = [], [], []\n",
    "        rng = np.random.default_rng(seed=42)\n",
    "    \n",
    "        for label, entries in by_label.items():\n",
    "            entries_copy = entries.copy()\n",
    "            rng.shuffle(entries_copy)\n",
    "            n = len(entries_copy)\n",
    "            n_train = int(n * 0.80)\n",
    "            n_val = int(n * 0.10)\n",
    "    \n",
    "            train_list.extend(entries_copy[:n_train])\n",
    "            val_list.extend(entries_copy[n_train:n_train + n_val])\n",
    "            test_list.extend(entries_copy[n_train + n_val:])\n",
    "    \n",
    "        # Save splits\n",
    "        splits_data = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"label_order\": LABEL_ORDER,\n",
    "            \"label_to_int\": LABEL_TO_INT,\n",
    "            \"train\": train_list,\n",
    "            \"val\": val_list,\n",
    "            \"test\": test_list,\n",
    "            \"counts\": {\n",
    "                \"train\": len(train_list),\n",
    "                \"val\": len(val_list),\n",
    "                \"test\": len(test_list),\n",
    "                \"total\": len(manifest)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        splits_file = PROCESSED_DIR / \"splits.json\"\n",
    "        with open(splits_file, 'w') as f:\n",
    "            json.dump(splits_data, f, indent=2)\n",
    "    \n",
    "        # Save label map\n",
    "        label_map = {\n",
    "            \"label_to_int\": LABEL_TO_INT,\n",
    "            \"int_to_label\": INT_TO_LABEL\n",
    "        }\n",
    "        label_map_file = PROCESSED_DIR / \"label_map.json\"\n",
    "        with open(label_map_file, 'w') as f:\n",
    "            json.dump(label_map, f, indent=2)\n",
    "    \n",
    "        # Save labels array\n",
    "        np.save(PROCESSED_DIR / \"labels.npy\", np.array(LABEL_ORDER, dtype=object))\n",
    "    \n",
    "        logger.info(f\"Saved splits to {splits_file}\")\n",
    "        logger.info(f\"  Train: {len(train_list):,}\")\n",
    "        logger.info(f\"  Val:   {len(val_list):,}\")\n",
    "        logger.info(f\"  Test:  {len(test_list):,}\")\n",
    "    \n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'preprocess_streaming.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11a9899ffe4eb3d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:07.239884Z",
     "start_time": "2025-12-01T15:29:07.107454Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.314263Z",
     "iopub.status.busy": "2025-12-02T04:21:17.314263Z",
     "iopub.status.idle": "2025-12-02T04:21:17.612176Z",
     "shell.execute_reply": "2025-12-02T04:21:17.612176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module run_full_automation.py raised name '__file__' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- run_full_automation.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"\n",
    "    Complete preprocessing automation orchestrator.\n",
    "    Runs the full pipeline with checks and validation.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import time\n",
    "    import subprocess\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    \n",
    "    ROOT = Path(__file__).parent.parent\n",
    "    LOGS_DIR = ROOT / \"logs\"\n",
    "    SCRIPTS_DIR = ROOT / \"scripts\"\n",
    "    VENV_PYTHON = ROOT / \".venv1\" / \"Scripts\" / \"python.exe\"\n",
    "    \n",
    "    # Ensure log directory exists\n",
    "    LOGS_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    def run_script(script_name: str, description: str, env_vars: dict = None) -> bool:\n",
    "        \"\"\"Run a Python script and return success status\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"RUNNING: {description}\")\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "        script_path = SCRIPTS_DIR / script_name\n",
    "        if not script_path.exists():\n",
    "            print(f\"Error: Script not found: {script_path}\")\n",
    "            return False\n",
    "    \n",
    "        cmd = [str(VENV_PYTHON), str(script_path)]\n",
    "    \n",
    "        # Build environment\n",
    "        import os\n",
    "        env = os.environ.copy()\n",
    "        if env_vars:\n",
    "            env.update(env_vars)\n",
    "    \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                cmd,\n",
    "                cwd=str(ROOT),\n",
    "                env=env,\n",
    "                capture_output=False,\n",
    "                text=True\n",
    "            )\n",
    "    \n",
    "            if result.returncode == 0:\n",
    "                print(f\" {description} completed successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\" {description} failed with return code {result.returncode}\")\n",
    "                return False\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\" Error running {description}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def main():\n",
    "        \"\"\"Main orchestration pipeline\"\"\"\n",
    "        start_time = time.time()\n",
    "    \n",
    "        print(\"=\"*80)\n",
    "        print(\"ECG PREPROCESSING AUTOMATION ORCHESTRATOR\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Python: {VENV_PYTHON}\")\n",
    "        print(f\"Root: {ROOT}\")\n",
    "    \n",
    "        # Step 1: Validate mapping\n",
    "        if not run_script(\"validate_mapping.py\", \"Step 1: Validate unified label mapping\"):\n",
    "            print(\"\\n Warning: Mapping validation had issues, but continuing...\")\n",
    "    \n",
    "        # Step 2: Improve mapping (optional)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"OPTIONAL: Improve Label Mapping\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Would you like to run the mapping improvement heuristics?\")\n",
    "        print(\"This may increase coverage from 33.8% to ~50-60%\")\n",
    "        response = input(\"Run improvement? (y/n, default=n): \").strip().lower()\n",
    "    \n",
    "        if response == 'y':\n",
    "            if run_script(\"improve_mapping.py\", \"Step 2a: Analyze mapping improvements\"):\n",
    "                print(\"\\nReview suggestions in: logs/mapping_improvements_suggested.csv\")\n",
    "                apply_response = input(\"Apply these improvements? (y/n, default=n): \").strip().lower()\n",
    "    \n",
    "                if apply_response == 'y':\n",
    "                    if not run_script(\"apply_mapping_improvements.py\", \"Step 2b: Apply mapping improvements\"):\n",
    "                        print(\"\\n Failed to apply improvements\")\n",
    "                        return False\n",
    "    \n",
    "        # Step 3: Run preprocessing\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PREPROCESSING MODE SELECTION\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Choose preprocessing mode:\")\n",
    "        print(\"  1. Full run (all datasets, ~7 hours)\")\n",
    "        print(\"  2. Medium test (5,000 files, ~15 minutes)\")\n",
    "        print(\"  3. Large test (20,000 files, ~1 hour)\")\n",
    "        print(\"  4. Custom limit\")\n",
    "    \n",
    "        mode = input(\"Select mode (1-4, default=2): \").strip() or \"2\"\n",
    "    \n",
    "        env_vars = {}\n",
    "        if mode == \"1\":\n",
    "            print(\"\\nStarting FULL preprocessing...\")\n",
    "            print(\"This will take approximately 7 hours.\")\n",
    "            confirm = input(\"Confirm full run? (yes/no): \").strip().lower()\n",
    "            if confirm != \"yes\":\n",
    "                print(\"Cancelled.\")\n",
    "                return False\n",
    "        elif mode == \"2\":\n",
    "            env_vars[\"ECG_PREPROCESS_LIMIT\"] = \"5000\"\n",
    "            print(\"\\nRunning medium test (5,000 files)...\")\n",
    "        elif mode == \"3\":\n",
    "            env_vars[\"ECG_PREPROCESS_LIMIT\"] = \"20000\"\n",
    "            print(\"\\nRunning large test (20,000 files)...\")\n",
    "        elif mode == \"4\":\n",
    "            limit = input(\"Enter custom limit: \").strip()\n",
    "            try:\n",
    "                limit_int = int(limit)\n",
    "                env_vars[\"ECG_PREPROCESS_LIMIT\"] = str(limit_int)\n",
    "                print(f\"\\nRunning with custom limit ({limit_int} files)...\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid limit. Defaulting to 5,000.\")\n",
    "                env_vars[\"ECG_PREPROCESS_LIMIT\"] = \"5000\"\n",
    "    \n",
    "        # Run preprocessing\n",
    "        if not run_script(\"preprocess_streaming.py\", \"Step 3: Streaming preprocessing\", env_vars):\n",
    "            print(\"\\n Preprocessing failed!\")\n",
    "            return False\n",
    "    \n",
    "        # Step 4: Verify outputs\n",
    "        if not run_script(\"verify_smoke_test.py\", \"Step 4: Verify preprocessing outputs\"):\n",
    "            print(\"\\n Warning: Verification had issues\")\n",
    "    \n",
    "        # Step 5: Model smoke test\n",
    "        if not run_script(\"model_smoke_test.py\", \"Step 5: Model compatibility test\"):\n",
    "            print(\"\\n Warning: Model test had issues\")\n",
    "    \n",
    "        # Generate final report\n",
    "        elapsed = time.time() - start_time\n",
    "        report_file = LOGS_DIR / \"preprocess_report.txt\"\n",
    "    \n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"ECG PREPROCESSING AUTOMATION - FINAL REPORT\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total time: {elapsed/60:.1f} minutes ({elapsed/3600:.2f} hours)\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"All steps completed successfully!\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"Output locations:\\n\")\n",
    "            f.write(f\"  - Processed records: artifacts/processed/records/\\n\")\n",
    "            f.write(f\"  - Manifest: artifacts/processed/manifest.jsonl\\n\")\n",
    "            f.write(f\"  - Splits: artifacts/processed/splits.json\\n\")\n",
    "            f.write(f\"  - Label map: artifacts/processed/label_map.json\\n\")\n",
    "            f.write(f\"  - Checkpoints: artifacts/processed/checkpoints/\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"Next steps:\\n\")\n",
    "            f.write(\"  1. Review preprocessing log: logs/preprocess_automation.log\\n\")\n",
    "            f.write(\"  2. Open notebook: notebooks/ecg_tensor_pipeline.ipynb\\n\")\n",
    "            f.write(\"  3. Begin model training\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"AUTOMATION COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total time: {elapsed/60:.1f} minutes\")\n",
    "        print(f\"Report saved to: {report_file}\")\n",
    "        print(\"\\nYou can now begin model training!\")\n",
    "    \n",
    "        return True\n",
    "    \n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        try:\n",
    "            success = main()\n",
    "            sys.exit(0 if success else 1)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nInterrupted by user. Progress has been saved.\")\n",
    "            print(\"You can resume by running this script again.\")\n",
    "            sys.exit(1)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n\\n Fatal error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            sys.exit(1)\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'run_full_automation.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "240a55e071907b09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:07.256324Z",
     "start_time": "2025-12-01T15:29:07.248583Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.622613Z",
     "iopub.status.busy": "2025-12-02T04:21:17.622613Z",
     "iopub.status.idle": "2025-12-02T04:21:17.645696Z",
     "shell.execute_reply": "2025-12-02T04:21:17.645696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module validate_mapping.py raised name '__file__' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- validate_mapping.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"Validate and summarize unified label mapping CSV\"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    \n",
    "    ROOT = Path(__file__).parent.parent\n",
    "    LOGS_DIR = ROOT / \"logs\"\n",
    "    mapping_file = LOGS_DIR / \"unified_label_mapping.csv\"\n",
    "    \n",
    "    if not mapping_file.exists():\n",
    "        print(f\"Error: {mapping_file} not found\")\n",
    "        exit(1)\n",
    "    \n",
    "    df = pd.read_csv(mapping_file, dtype=str).fillna('')\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"UNIFIED LABEL MAPPING VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total rows: {len(df):,}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    \n",
    "    # Dataset distribution\n",
    "    print(f\"\\nDataset distribution:\")\n",
    "    for dataset, count in df['dataset'].value_counts().items():\n",
    "        print(f\"  {dataset}: {count:,}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    label_counts = df['mapped_label'].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {label if label else '(unmapped)'}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Count unmapped\n",
    "    unmapped = (df['mapped_label'] == '').sum()\n",
    "    print(f\"\\nUnmapped (blank) records: {unmapped:,} ({unmapped/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # If high unmapped rate, create sample file\n",
    "    if unmapped > 0:\n",
    "        unmapped_df = df[df['mapped_label'] == ''].copy()\n",
    "        sample_size = min(200, len(unmapped_df))\n",
    "        sample = unmapped_df.sample(n=sample_size, random_state=42)\n",
    "        sample_file = LOGS_DIR / \"unmapped_sample.csv\"\n",
    "        sample.to_csv(sample_file, index=False)\n",
    "        print(f\"\\nSaved {sample_size} unmapped samples to: {sample_file}\")\n",
    "    \n",
    "    print(\"\\nValidation complete!\")\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'validate_mapping.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41693d16d4846ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:07.286266Z",
     "start_time": "2025-12-01T15:29:07.273371Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.654354Z",
     "iopub.status.busy": "2025-12-02T04:21:17.654354Z",
     "iopub.status.idle": "2025-12-02T04:21:17.689289Z",
     "shell.execute_reply": "2025-12-02T04:21:17.689289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inlined module verify_smoke_test.py raised name '__file__' is not defined\n"
     ]
    }
   ],
   "source": [
    "# --- verify_smoke_test.py (inlined) ---\n",
    "try:\n",
    "    \"\"\"Verify smoke test outputs\"\"\"\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    import random\n",
    "    \n",
    "    ROOT = Path(__file__).parent.parent\n",
    "    PROCESSED_DIR = ROOT / \"artifacts\" / \"processed\"\n",
    "    RECORDS_DIR = PROCESSED_DIR / \"records\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SMOKE TEST VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check splits.json\n",
    "    splits_file = PROCESSED_DIR / \"splits.json\"\n",
    "    if splits_file.exists():\n",
    "        with open(splits_file, 'r') as f:\n",
    "            splits = json.load(f)\n",
    "        print(f\"\\n splits.json exists\")\n",
    "        print(f\"  Train: {splits['counts']['train']}\")\n",
    "        print(f\"  Val: {splits['counts']['val']}\")\n",
    "        print(f\"  Test: {splits['counts']['test']}\")\n",
    "        print(f\"  Total: {splits['counts']['total']}\")\n",
    "    else:\n",
    "        print(\"\\n splits.json missing!\")\n",
    "    \n",
    "    # Check label_map.json\n",
    "    label_map_file = PROCESSED_DIR / \"label_map.json\"\n",
    "    if label_map_file.exists():\n",
    "        with open(label_map_file, 'r') as f:\n",
    "            label_map = json.load(f)\n",
    "        print(f\"\\n label_map.json exists\")\n",
    "        print(f\"  Labels: {label_map['label_to_int']}\")\n",
    "    else:\n",
    "        print(\"\\n label_map.json missing!\")\n",
    "    \n",
    "    # Check labels.npy\n",
    "    labels_file = PROCESSED_DIR / \"labels.npy\"\n",
    "    if labels_file.exists():\n",
    "        labels = np.load(labels_file, allow_pickle=True)\n",
    "        print(f\"\\n labels.npy exists\")\n",
    "        print(f\"  Labels: {list(labels)}\")\n",
    "    else:\n",
    "        print(\"\\n labels.npy missing!\")\n",
    "    \n",
    "    # Check manifest.jsonl\n",
    "    manifest_file = PROCESSED_DIR / \"manifest.jsonl\"\n",
    "    if manifest_file.exists():\n",
    "        with open(manifest_file, 'r') as f:\n",
    "            manifest = [json.loads(line) for line in f if line.strip()]\n",
    "        print(f\"\\n manifest.jsonl exists\")\n",
    "        print(f\"  Entries: {len(manifest)}\")\n",
    "    else:\n",
    "        print(\"\\n manifest.jsonl missing!\")\n",
    "    \n",
    "    # Check record files\n",
    "    record_files = list(RECORDS_DIR.glob(\"*.npy\"))\n",
    "    print(f\"\\n Found {len(record_files)} .npy files in records/\")\n",
    "    \n",
    "    # Load and verify 5 random records\n",
    "    if record_files:\n",
    "        print(\"\\nVerifying 5 random records...\")\n",
    "        samples = random.sample(record_files, min(5, len(record_files)))\n",
    "    \n",
    "        for i, npy_file in enumerate(samples, 1):\n",
    "            try:\n",
    "                # Load signal\n",
    "                signal = np.load(npy_file, allow_pickle=False)\n",
    "    \n",
    "                # Load label\n",
    "                label_file = npy_file.with_suffix('.label')\n",
    "                with open(label_file, 'r') as f:\n",
    "                    label = int(f.read().strip())\n",
    "    \n",
    "                # Load metadata\n",
    "                meta_file = npy_file.parent / f\"{npy_file.stem}.meta.json\"\n",
    "                with open(meta_file, 'r') as f:\n",
    "                    meta = json.load(f)\n",
    "    \n",
    "                print(f\"\\n  {i}. {npy_file.name}\")\n",
    "                print(f\"     Signal shape: {signal.shape}\")\n",
    "                print(f\"     Label: {label} ({meta.get('mapped_label', 'unknown')})\")\n",
    "                print(f\"     Dataset: {meta.get('dataset', 'unknown')}\")\n",
    "    \n",
    "                # Verify shape\n",
    "                assert signal.shape == (1, 5000), f\"Expected (1, 5000), got {signal.shape}\"\n",
    "                assert 0 <= label <= 4, f\"Invalid label: {label}\"\n",
    "    \n",
    "                print(f\"      Valid\")\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"\\n  {i}. {npy_file.name}\")\n",
    "                print(f\"      Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VERIFICATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as _e:\n",
    "    print('Warning: inlined module', 'verify_smoke_test.py', 'raised', _e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae54d7b46bbcf4c",
   "metadata": {},
   "source": [
    "## Quick: Generate/Load unified mapping (run this cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed306eeb7a03517a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:07.311649Z",
     "start_time": "2025-12-01T15:29:07.304629Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.693288Z",
     "iopub.status.busy": "2025-12-02T04:21:17.693288Z",
     "iopub.status.idle": "2025-12-02T04:21:17.712582Z",
     "shell.execute_reply": "2025-12-02T04:21:17.710577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate mapping exists: True Prod file exists: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate unified mapping (if you have script)\n",
    "candidate = Path('logs/unified_label_mapping.candidate.csv')\n",
    "prod = Path('logs/unified_label_mapping.csv')\n",
    "if (Path('scripts/generate_unified_mapping.py')).exists() and not candidate.exists():\n",
    "    print('Generating candidate mapping...')\n",
    "    os.system(f'python \"{str(Path(\"scripts/generate_unified_mapping.py\"))}\"')\n",
    "else:\n",
    "    print('Candidate mapping exists:', candidate.exists(), 'Prod file exists:', prod.exists())\n",
    "# If you have a candidate and want to promote it, uncomment:\n",
    "# if candidate.exists(): candidate.replace(prod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fab709735e2724",
   "metadata": {},
   "source": [
    "## Preprocessing (streaming, memory-safe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3714aceb0b5926c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:07.339772Z",
     "start_time": "2025-12-01T15:29:07.333851Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.719994Z",
     "iopub.status.busy": "2025-12-02T04:21:17.717984Z",
     "iopub.status.idle": "2025-12-02T04:21:17.729139Z",
     "shell.execute_reply": "2025-12-02T04:21:17.729139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No preprocess_streaming.py found. Implement preprocessing in this notebook or inline alternate script.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run streaming preprocessing from scripts/preprocess_streaming.py if present\n",
    "proc_script = Path('scripts/preprocess_streaming.py')\n",
    "if proc_script.exists():\n",
    "    print('Launching streaming preprocessing (script)...')\n",
    "    # recommend using environment var ECG_PREPROCESS_LIMIT to test\n",
    "    os.system(f'python \"{proc_script}\"')\n",
    "else:\n",
    "    print('No preprocess_streaming.py found. Implement preprocessing in this notebook or inline alternate script.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a9563ec6f28d7",
   "metadata": {},
   "source": [
    "## Training (run this after preprocessing finishes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c26ee3251b56b46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:07.369441Z",
     "start_time": "2025-12-01T15:29:07.362452Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.733861Z",
     "iopub.status.busy": "2025-12-02T04:21:17.733861Z",
     "iopub.status.idle": "2025-12-02T04:21:17.748130Z",
     "shell.execute_reply": "2025-12-02T04:21:17.745488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training script detected. Use in-notebook training cells or create scripts/training.py and link it.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Launch training script if present\n",
    "train_script = Path('scripts/train_pipeline.py')  # optional\n",
    "if train_script.exists():\n",
    "    print('Running training script...')\n",
    "    os.system(f'python \"{train_script}\"')\n",
    "else:\n",
    "    print('No training script detected. Use in-notebook training cells or create scripts/training.py and link it.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57481068ade4179c",
   "metadata": {},
   "source": [
    "## Evaluation and Visuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfde8129dd34cc5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:07.383353Z",
     "start_time": "2025-12-01T15:29:07.369441Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.755219Z",
     "iopub.status.busy": "2025-12-02T04:21:17.750063Z",
     "iopub.status.idle": "2025-12-02T04:21:17.763211Z",
     "shell.execute_reply": "2025-12-02T04:21:17.763211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No evaluate.py. Use notebook cells to visualize artifacts/figures/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run evaluation if script exists\n",
    "eval_script = Path('scripts/evaluate.py')\n",
    "if eval_script.exists():\n",
    "    os.system(f'python \"{eval_script}\"')\n",
    "else:\n",
    "    print('No evaluate.py. Use notebook cells to visualize artifacts/figures/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1b0641e56671c",
   "metadata": {},
   "source": [
    "## Smoke tests and quick validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e5bae11fefdd4b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T15:29:08.959841Z",
     "start_time": "2025-12-01T15:29:07.408954Z"
    },
    "execution": {
     "iopub.execute_input": "2025-12-02T04:21:17.765928Z",
     "iopub.status.busy": "2025-12-02T04:21:17.765928Z",
     "iopub.status.idle": "2025-12-02T04:21:21.478158Z",
     "shell.execute_reply": "2025-12-02T04:21:21.474405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No smoke-test script. Manual checks:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Count files: 98127\n",
      " - Check splits: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run smoke tests\n",
    "smoke = Path('scripts/verify_smoke_test.py')\n",
    "if smoke.exists():\n",
    "    os.system(f'python \"{smoke}\"')\n",
    "else:\n",
    "    print('No smoke-test script. Manual checks:')\n",
    "    print(' - Count files:', len(list((PROCESSED_DIR/'records').glob('*.npz'))))\n",
    "    print(' - Check splits:', (PROCESSED_DIR/'splits.json').exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39990c55fd636",
   "metadata": {},
   "source": [
    "## Final: Notebook control\n",
    "You can now run cells in order. Long-running steps are executed as external scripts to avoid kernel timeouts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
