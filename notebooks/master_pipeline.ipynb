{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7566aa",
   "metadata": {},
   "source": [
    "# ECG Master Pipeline\n",
    "\n",
    "Single notebook with preprocessing, training, evaluation and smoke tests.\n",
    "\n",
    "How to run:\n",
    "\n",
    "1. Open interactively: `jupyter notebook notebooks/master_pipeline.ipynb`\n",
    "2. Or run headless (recommended for full preprocessing):\n",
    "   `jupyter nbconvert --to notebook --execute notebooks/master_pipeline.ipynb --output logs/preprocess_run.ipynb`\n",
    "\n",
    "Notes:\n",
    "- If you are on Windows and see asyncio warnings, add at top of kernel: `import asyncio, sys; asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())`.\n",
    "\n",
    "---\n",
    "Cells are grouped: Environment, Config, Utilities, Mapping Load, Preprocessing, Dataset, Model, Training, Evaluation, Smoke Tests, Orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "id": "002dec38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T06:16:16.006839Z",
     "start_time": "2025-12-02T06:16:15.988052Z"
    }
   },
   "source": [
    "# Environment checks and directory setup\n",
    "import os, sys, asyncio\n",
    "if sys.platform == \"win32\":\n",
    "    try:\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "from pathlib import Path\n",
    "# Find project root by looking for Dataset folder or going up from cwd\n",
    "ROOT = Path.cwd().resolve()\n",
    "initial_root = ROOT\n",
    "# If we're in notebooks/ subdirectory, go up one level\n",
    "if ROOT.name == 'notebooks' and (ROOT.parent / 'Dataset').exists():\n",
    "    ROOT = ROOT.parent\n",
    "    print(f'[Adjusted] ROOT from {initial_root} -> {ROOT}')\n",
    "# If still no Dataset found, try going up one more level\n",
    "elif not (ROOT / 'Dataset').exists() and (ROOT.parent / 'Dataset').exists():\n",
    "    ROOT = ROOT.parent\n",
    "    print(f'[Adjusted] ROOT from {initial_root} -> {ROOT}')\n",
    "DATASET_DIR = (ROOT / \"Dataset\")\n",
    "ARTIFACTS_DIR = (ROOT / \"artifacts\")\n",
    "PROCESSED_DIR = ARTIFACTS_DIR / \"processed\"\n",
    "FIGURES_DIR = ARTIFACTS_DIR / \"figures\"\n",
    "LOGS_DIR = ROOT / \"logs\"\n",
    "for p in [ARTIFACTS_DIR, PROCESSED_DIR, PROCESSED_DIR / \"records\", FIGURES_DIR, LOGS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('ROOT:', ROOT)\n",
    "print('DATASET_DIR:', DATASET_DIR)\n",
    "print('DATASET_DIR exists:', DATASET_DIR.exists())\n",
    "if DATASET_DIR.exists():\n",
    "    subdirs = [d.name for d in DATASET_DIR.iterdir() if d.is_dir()]\n",
    "    print(f'Dataset subdirectories: {subdirs}')\n",
    "print('ARTIFACTS_DIR:', ARTIFACTS_DIR)\n",
    "print('PROCESSED_DIR:', PROCESSED_DIR)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adjusted] ROOT from D:\\ecg-research\\notebooks -> D:\\ecg-research\n",
      "ROOT: D:\\ecg-research\n",
      "DATASET_DIR: D:\\ecg-research\\Dataset\n",
      "DATASET_DIR exists: True\n",
      "Dataset subdirectories: ['PTB_Diagnostic', 'CinC2017', 'Chapman_Shaoxing', 'ptb-xl']\n",
      "ARTIFACTS_DIR: D:\\ecg-research\\artifacts\n",
      "PROCESSED_DIR: D:\\ecg-research\\artifacts\\processed\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4249b5be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T06:16:19.821652Z",
     "start_time": "2025-12-02T06:16:19.188993Z"
    }
   },
   "source": [
    "# Imports, device, seeds\n",
    "import os, random, json, time, math\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device ->', DEVICE)\n",
    "\n",
    "SEED = int(os.environ.get('ECG_SEED', 42))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device -> cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3fa1b2ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T06:16:22.771259Z",
     "start_time": "2025-12-02T06:16:22.764018Z"
    }
   },
   "source": [
    "# Configuration constants\n",
    "TARGET_FS = 500\n",
    "TARGET_SAMPLES = 5000   # 10s @ 500Hz\n",
    "LABEL_ORDER = ['MI','AF','BBB','NORM','OTHER']\n",
    "LABEL_TO_INT = {l:i for i,l in enumerate(LABEL_ORDER)}\n",
    "BATCH_SIZE = 8 if DEVICE.type=='cpu' else int(os.environ.get('ECG_BATCH_SIZE', 32))\n",
    "EPOCHS = int(os.environ.get('ECG_EPOCHS', 2))\n",
    "LR = float(os.environ.get('ECG_LR', 1e-3))\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "print('BATCH_SIZE', BATCH_SIZE, 'EPOCHS', EPOCHS, 'AMP', USE_AMP)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE 8 EPOCHS 2 AMP False\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "2ff10f19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T06:16:33.062429Z",
     "start_time": "2025-12-02T06:16:31.859203Z"
    }
   },
   "source": [
    "# Utilities: IO, normalization, resample, safe save/load\n",
    "import json, gzip\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from pathlib import Path\n",
    "\n",
    "def zscore_norm(x, eps=1e-6):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    m = x.mean(axis=-1, keepdims=True)\n",
    "    s = x.std(axis=-1, keepdims=True)\n",
    "    s[s < eps] = 1.0\n",
    "    return (x - m) / s\n",
    "\n",
    "def pad_or_truncate(x, target_len):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if x.ndim == 1:\n",
    "        if x.shape[0] >= target_len:\n",
    "            return x[:target_len]\n",
    "        else:\n",
    "            pad = target_len - x.shape[0]\n",
    "            return np.pad(x, (0, pad), mode='constant')\n",
    "    elif x.ndim == 2:\n",
    "        # assume shape (leads, samples)\n",
    "        if x.shape[1] >= target_len:\n",
    "            return x[:, :target_len]\n",
    "        else:\n",
    "            pad = target_len - x.shape[1]\n",
    "            return np.pad(x, ((0,0),(0,pad)), mode='constant')\n",
    "    else:\n",
    "        raise ValueError('Unexpected signal shape')\n",
    "\n",
    "def safe_save_npz(path: Path, signal_array, label:int, metadata=None):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "    np.savez_compressed(path, signal=signal_array.astype(np.float32), label=int(label), metadata=json.dumps(metadata))\n",
    "\n",
    "def load_npz(path:Path):\n",
    "    with np.load(path, allow_pickle=True) as d:\n",
    "        sig = d['signal'].astype(np.float32)\n",
    "        lbl = int(d['label'])\n",
    "        meta = json.loads(d['metadata'].tolist() if hasattr(d['metadata'],'tolist') else d['metadata'])\n",
    "    return sig, lbl, meta\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "f8d351e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T06:16:41.272274Z",
     "start_time": "2025-12-02T06:16:38.856386Z"
    }
   },
   "source": [
    "# Load unified mapping if present; else load candidate, else fallback\n",
    "from collections import Counter\n",
    "UNIFIED_CSV = LOGS_DIR / \"unified_label_mapping.csv\"\n",
    "CANDIDATE_CSV = LOGS_DIR / \"unified_label_mapping.candidate.csv\"\n",
    "\n",
    "mapping_index = {}\n",
    "if UNIFIED_CSV.exists() and UNIFIED_CSV.stat().st_size>0:\n",
    "    df_map = pd.read_csv(UNIFIED_CSV, dtype=str).fillna('')\n",
    "    print('Loaded unified mapping:', UNIFIED_CSV, len(df_map))\n",
    "else:\n",
    "    if CANDIDATE_CSV.exists() and CANDIDATE_CSV.stat().st_size>0:\n",
    "        df_map = pd.read_csv(CANDIDATE_CSV, dtype=str).fillna('')\n",
    "        print('Loaded candidate mapping:', CANDIDATE_CSV, len(df_map))\n",
    "    else:\n",
    "        df_map = pd.DataFrame(columns=['dataset','record_id','mapped_label'])\n",
    "        print('No mapping CSV found; will default to OTHER')\n",
    "\n",
    "# Build mapping index (dataset -> key -> label)\n",
    "for _, row in df_map.iterrows():\n",
    "    ds = str(row.get('dataset','')).strip()\n",
    "    rid = str(row.get('record_id','')).strip().replace('\\\\','/').strip('/')\n",
    "    lab = str(row.get('mapped_label','')).strip().upper()\n",
    "    if not ds or not rid:\n",
    "        continue\n",
    "    mapping_index.setdefault(ds, {})[rid] = lab\n",
    "\n",
    "print('Datasets in mapping:', list(mapping_index.keys())[:10])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded unified mapping: D:\\ecg-research\\logs\\unified_label_mapping.csv 84556\n",
      "Datasets in mapping: ['ptb-xl', 'CinC2017', 'PTB_Diagnostic', 'Chapman_Shaoxing']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "da10f658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T06:16:48.709116Z",
     "start_time": "2025-12-02T06:16:48.702658Z"
    }
   },
   "source": [
    "# label lookup utility used during preprocessing\n",
    "def lookup_mapped_label(dataset_name, record_id):\n",
    "    idx = mapping_index.get(dataset_name, {})\n",
    "    if record_id in idx:\n",
    "        lab = idx[record_id].upper()\n",
    "        return lab if lab in LABEL_TO_INT else 'OTHER'\n",
    "    # try basename\n",
    "    base = record_id.split('/')[-1]\n",
    "    if base in idx:\n",
    "        lab = idx[base].upper()\n",
    "        return lab if lab in LABEL_TO_INT else 'OTHER'\n",
    "    return 'OTHER'\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "6ee9f5d5",
   "metadata": {},
   "source": [
    "## Preprocessing (streaming). This cell scans supported datasets and writes per-record .npz files into artifacts/processed/records. It is I/O-heavy and may take hours for full dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "388039c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T16:47:32.451428Z",
     "start_time": "2025-12-02T06:16:54.357772Z"
    }
   },
   "source": [
    "# Preprocessing: very conservative memory-safe loop\n",
    "import wfdb\n",
    "import scipy.io\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "RECORDS_DIR = PROCESSED_DIR / \"records\"\n",
    "RECORDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# helper to read recordings (WFDB .hea/.dat or .mat)\n",
    "def read_record_generic(full_path: Path):\n",
    "    # returns (signal (n_leads, n_samples), fs, meta_dict)\n",
    "    try:\n",
    "        if full_path.suffix.lower() == '.mat':\n",
    "            data = scipy.io.loadmat(str(full_path))\n",
    "            # try several common keys\n",
    "            for k in ['val','data','sig','ecg']:\n",
    "                if k in data:\n",
    "                    arr = data[k]\n",
    "                    arr = np.asarray(arr, dtype=np.float32)\n",
    "                    if arr.ndim==2 and arr.shape[0] > arr.shape[1]:\n",
    "                        # ensure shape (leads, samples)\n",
    "                        return arr, int(data.get('fs', TARGET_FS)), {'source':'mat','path':str(full_path)}\n",
    "            # fallback - find first numeric\n",
    "            arr = None\n",
    "            for v in data.values():\n",
    "                if isinstance(v, np.ndarray) and v.ndim==2:\n",
    "                    arr = v.astype(np.float32)\n",
    "                    break\n",
    "            if arr is None:\n",
    "                raise RuntimeError('No 2D array found in mat')\n",
    "            return arr, int(data.get('fs', TARGET_FS)), {'source':'mat','path':str(full_path)}\n",
    "        else:\n",
    "            # WFDB read using record name without .hea\n",
    "            rec_dir = full_path.parent\n",
    "            rec_name = full_path.stem\n",
    "            record = wfdb.rdrecord(str(full_path.with_suffix('')))\n",
    "            sig = np.asarray(record.p_signal.T, dtype=np.float32)  # shape (leads, samples)\n",
    "            fs = int(getattr(record, 'fs', TARGET_FS))\n",
    "            return sig, fs, {'source':'wfdb','path':str(full_path)}\n",
    "    except Exception as e:\n",
    "        # bubble up\n",
    "        raise\n",
    "\n",
    "# iterate datasets (supported minimal set)\n",
    "candidates = []\n",
    "if DATASET_DIR.exists():\n",
    "    for ds in sorted(DATASET_DIR.iterdir()):\n",
    "        if ds.is_dir():\n",
    "            candidates.append(ds)\n",
    "print('Datasets found:', [p.name for p in candidates])\n",
    "\n",
    "# We'll process with a limit if provided\n",
    "LIMIT = int(os.environ.get('ECG_PREPROCESS_LIMIT', 0))\n",
    "print('Processing limit (0 means all):', LIMIT)\n",
    "\n",
    "manifest = []\n",
    "skipped = 0\n",
    "processed = 0\n",
    "\n",
    "# For speed and safety, define file patterns per dataset (common)\n",
    "patterns = {\n",
    "    'ptb-xl': ['**/*.dat','**/*.hea','**/*_hr.mat','**/*_lr.mat'],\n",
    "    'CinC2017': ['**/*.mat','**/*.hea','**/*.atr','training/*.mat'],\n",
    "    'PTB_Diagnostic': ['**/*.dat','**/*.hea'],\n",
    "    'Chapman_Shaoxing': ['**/*.dat','**/*.hea','**/*.mat']\n",
    "}\n",
    "\n",
    "# If wfdb package missing, fallback to synthetic creation\n",
    "if not candidates:\n",
    "    print('No dataset folders – generating synthetic samples for quick smoke tests')\n",
    "    t = np.linspace(0, 10, TARGET_SAMPLES, dtype=np.float32)\n",
    "    for i in range(200):\n",
    "        s = np.sin(2*np.pi*(1+i*0.1)*t).astype(np.float32)\n",
    "        out = RECORDS_DIR / f\"SYNTH_{i:05d}.npz\"\n",
    "        safe_save_npz(out, s, i%len(LABEL_ORDER), {'dataset':'SYNTH'})\n",
    "        manifest.append({'path': f\"records/{out.name}\", 'label': int(i%len(LABEL_ORDER))})\n",
    "    processed = len(manifest)\n",
    "else:\n",
    "    # iterate dataset folders and patterns\n",
    "    for ds in candidates:\n",
    "        ds_name = ds.name\n",
    "        pat_list = patterns.get(ds_name, ['**/*.hea','**/*.mat','**/*.dat'])\n",
    "        files = []\n",
    "        for pat in pat_list:\n",
    "            files.extend(list(ds.rglob(pat)))\n",
    "        # prefer .hea as index entries: convert to unique set\n",
    "        files = sorted(set(files))\n",
    "        if LIMIT and processed >= LIMIT:\n",
    "            break\n",
    "        for fpath in tqdm(files, desc=f\"Processing {ds_name}\", unit='file'):\n",
    "            try:\n",
    "                # simple TRY: read using wfdb or mat loader; if fails, skip\n",
    "                try:\n",
    "                    sig, fs, meta = read_record_generic(fpath)\n",
    "                except Exception:\n",
    "                    # if WFDB read fails try reading .hea by name\n",
    "                    try:\n",
    "                        rec = wfdb.rdrecord(str(fpath.with_suffix('')))\n",
    "                        sig = np.asarray(rec.p_signal.T, dtype=np.float32)\n",
    "                        fs = int(getattr(rec, 'fs', TARGET_FS))\n",
    "                        meta = {'source':'wfdb'}\n",
    "                    except Exception as e:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "\n",
    "                # resample if needed\n",
    "                if fs != TARGET_FS:\n",
    "                    # resample each lead\n",
    "                    num = int(round(sig.shape[1] * (TARGET_FS / float(fs))))\n",
    "                    sig = signal.resample(sig, num, axis=1).astype(np.float32)\n",
    "                    fs = TARGET_FS\n",
    "\n",
    "                # normalize and pad/truncate\n",
    "                if sig.ndim == 1:\n",
    "                    sig = np.expand_dims(sig, 0)\n",
    "                sig = zscore_norm(sig)\n",
    "                sig = pad_or_truncate(sig, TARGET_SAMPLES)\n",
    "\n",
    "                # build record id relative to dataset root\n",
    "                try:\n",
    "                    rel = fpath.relative_to(DATASET_DIR).as_posix()\n",
    "                except Exception:\n",
    "                    rel = fpath.name\n",
    "                # lookup mapped label\n",
    "                mapped = lookup_mapped_label(ds_name, rel)\n",
    "                label_int = LABEL_TO_INT.get(mapped, LABEL_TO_INT['OTHER'])\n",
    "\n",
    "                out_file = RECORDS_DIR / f\"{ds_name}__{rel.replace('/','__').replace('.','_')}.npz\"\n",
    "                safe_save_npz(out_file, sig, label_int, {'dataset': ds_name, 'src': rel})\n",
    "                manifest.append({'path': f\"records/{out_file.name}\", 'label': label_int})\n",
    "                processed += 1\n",
    "\n",
    "                if LIMIT and processed >= LIMIT:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                # write short log entry\n",
    "                with open(LOGS_DIR / \"preprocess_errors.log\", \"a\", encoding=\"utf-8\") as fh:\n",
    "                    fh.write(f\"{fpath} -> {repr(e)}\\n\")\n",
    "                continue\n",
    "\n",
    "print('Done. processed:', processed, 'skipped:', skipped)\n",
    "# persist manifest and splits\n",
    "import json\n",
    "with open(PROCESSED_DIR / \"manifest.jsonl\", \"w\", encoding=\"utf-8\") as fh:\n",
    "    for rec in manifest:\n",
    "        fh.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "# build simple stratified splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "paths = [m['path'] for m in manifest]\n",
    "labels = [m['label'] for m in manifest]\n",
    "if paths:\n",
    "    train_p, test_p, y_train, y_test = train_test_split(paths, labels, test_size=0.2, stratify=labels, random_state=SEED)\n",
    "    val_p, test_p, y_val, y_test = train_test_split(test_p, y_test, test_size=0.5, stratify=y_test, random_state=SEED)\n",
    "    splits = {'paths': {'train': train_p, 'val': val_p, 'test': test_p}}\n",
    "    with open(PROCESSED_DIR / \"splits.json\", \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(splits, fh, indent=2)\n",
    "    print('Splits saved. Train:', len(train_p), 'Val:', len(val_p), 'Test:', len(test_p))\n",
    "else:\n",
    "    print('No manifest entries – nothing to split.')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets found: ['Chapman_Shaoxing', 'CinC2017', 'ptb-xl', 'PTB_Diagnostic']\n",
      "Processing limit (0 means all): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chapman_Shaoxing: 100%|██████████| 90304/90304 [4:02:02<00:00,  6.22file/s]   \n",
      "Processing CinC2017: 100%|██████████| 17656/17656 [48:34<00:00,  6.06file/s] \n",
      "Processing ptb-xl: 100%|██████████| 87196/87196 [5:33:57<00:00,  4.35file/s]   \n",
      "Processing PTB_Diagnostic: 100%|██████████| 1098/1098 [05:26<00:00,  3.37file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. processed: 196252 skipped: 2\n",
      "Splits saved. Train: 157001 Val: 19625 Test: 19626\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "de0afe8b",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader (lazy loading)"
   ]
  },
  {
   "cell_type": "code",
   "id": "92f01117",
   "metadata": {},
   "source": [
    "# PyTorch Dataset reading .npz files lazily\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, entries, base_dir):\n",
    "        self.entries = entries\n",
    "        self.base_dir = Path(base_dir)\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.entries[idx]\n",
    "        sig, label, meta = load_npz(self.base_dir / p.split('records/')[-1])\n",
    "        # ensure shape (1, samples)\n",
    "        if sig.ndim == 2:\n",
    "            # use mean across leads for single-lead baseline\n",
    "            sig = sig.mean(axis=0, keepdims=True)\n",
    "        tensor = torch.from_numpy(sig).float()\n",
    "        return tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# quick loader constructor\n",
    "def build_loaders(limit=None):\n",
    "    import json\n",
    "    with open(PROCESSED_DIR / 'splits.json','r') as fh:\n",
    "        splits = json.load(fh)\n",
    "    train_list = splits['paths']['train']\n",
    "    val_list = splits['paths']['val']\n",
    "    test_list = splits['paths']['test']\n",
    "    if limit:\n",
    "        train_list = train_list[:limit]\n",
    "        val_list = val_list[:int(limit*0.2)]\n",
    "        test_list = test_list[:int(limit*0.2)]\n",
    "    train_ds = ECGDataset(train_list, PROCESSED_DIR / 'records')\n",
    "    val_ds = ECGDataset(val_list, PROCESSED_DIR / 'records')\n",
    "    test_ds = ECGDataset(test_list, PROCESSED_DIR / 'records')\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# show example batch if available\n",
    "try:\n",
    "    tr, va, te = build_loaders(limit=16)\n",
    "    xb, yb = next(iter(tr))\n",
    "    print('example batch:', xb.shape, yb.shape)\n",
    "except Exception as e:\n",
    "    print('build_loaders failed:', e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "580180f4",
   "metadata": {},
   "source": [
    "## Model (compact 1D ResNet-like). GPU intensive: forward/backward, mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "id": "d43c85b1",
   "metadata": {},
   "source": [
    "# Simple 1D CNN with residual blocks\n",
    "import torch.nn as nn\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=7, s=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=s, padding=k//2)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self,x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class SmallResNet1D(nn.Module):\n",
    "    def __init__(self, in_ch=1, num_classes=len(LABEL_ORDER)):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBlock(in_ch, 16, k=11, s=2),\n",
    "            ConvBlock(16, 32, k=9, s=2),\n",
    "        )\n",
    "        self.res1 = nn.Sequential(\n",
    "            ConvBlock(32, 32, k=7, s=1),\n",
    "            ConvBlock(32, 32, k=5, s=1),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.stem(x)\n",
    "        r = self.res1(x)\n",
    "        x = x + r\n",
    "        return self.head(x)\n",
    "\n",
    "model = SmallResNet1D().to(DEVICE)\n",
    "print(model)\n",
    "print('num params:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d60d6f66",
   "metadata": {},
   "source": [
    "## Training loop (uses AMP when available). Logs metrics and saves checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "id": "27af72be",
   "metadata": {},
   "source": [
    "# Training loop with evaluation function\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import json, os\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys, ypreds = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            preds = logits.argmax(dim=1).cpu().tolist()\n",
    "            ys.extend(yb.cpu().tolist())\n",
    "            ypreds.extend(preds)\n",
    "    report = {\n",
    "        'acc': accuracy_score(ys, ypreds),\n",
    "        'f1_macro': f1_score(ys, ypreds, average='macro'),\n",
    "        'confusion': confusion_matrix(ys, ypreds).tolist()\n",
    "    }\n",
    "    return report\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    best_val = -1.0\n",
    "    history = {'train_loss':[], 'val_f1':[]}\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for xb,yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            losses.append(loss.item())\n",
    "        avg_loss = float(np.mean(losses)) if losses else 0.0\n",
    "        val_report = evaluate(model, val_loader) if val_loader is not None else {}\n",
    "        val_f1 = val_report.get('f1_macro', 0.0)\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        print(f\"Epoch {ep+1}/{epochs} loss={avg_loss:.4f} val_f1={val_f1:.4f}\")\n",
    "        # save checkpoint\n",
    "        ckpt = PROCESSED_DIR / f\"checkpoint_ep{ep+1}.pth\"\n",
    "        torch.save({'model_state': model.state_dict(), 'opt_state': opt.state_dict(), 'epoch': ep+1, 'history': history}, ckpt)\n",
    "        if val_f1 > best_val:\n",
    "            best_val = val_f1\n",
    "            torch.save({'model_state': model.state_dict(), 'opt_state': opt.state_dict(), 'epoch': ep+1}, PROCESSED_DIR / \"best_model.pth\")\n",
    "    # final history\n",
    "    with open(PROCESSED_DIR / \"training_history.json\",\"w\",encoding=\"utf-8\") as fh:\n",
    "        json.dump(history, fh, indent=2)\n",
    "    return history\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "823cd39e",
   "metadata": {},
   "source": [
    "## Evaluation & Plots"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9fa4ff5",
   "metadata": {},
   "source": [
    "# Plot training curves and confusion matrix if evaluation available\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history, savepath=FIGURES_DIR/'training_curves.png'):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(history.get('train_loss',[]), label='train_loss')\n",
    "    plt.plot(history.get('val_f1',[]), label='val_f1')\n",
    "    plt.legend()\n",
    "    plt.title('Training history')\n",
    "    plt.savefig(savepath)\n",
    "    plt.close()\n",
    "    print('Saved', savepath)\n",
    "\n",
    "# Confusion matrix plotting helper\n",
    "def plot_confusion(cm, labels=LABEL_ORDER, savepath=FIGURES_DIR/'confusion.png'):\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(np.array(cm), annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "    plt.savefig(savepath)\n",
    "    plt.close()\n",
    "    print('Saved', savepath)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b9bbcbf",
   "metadata": {},
   "source": [
    "## Smoke tests — quick checks to ensure pipeline integrity"
   ]
  },
  {
   "cell_type": "code",
   "id": "09fcd648",
   "metadata": {},
   "source": [
    "# Basic smoke tests: manifest existence, ability to load one record, model forward pass\n",
    "errors = []\n",
    "if not (PROCESSED_DIR / 'manifest.jsonl').exists():\n",
    "    errors.append('manifest.jsonl missing')\n",
    "else:\n",
    "    # try to load first manifest entry\n",
    "    import json\n",
    "    with open(PROCESSED_DIR / 'manifest.jsonl','r',encoding='utf-8') as fh:\n",
    "        first = fh.readline().strip()\n",
    "    if not first:\n",
    "        errors.append('manifest empty')\n",
    "    else:\n",
    "        rec = json.loads(first)\n",
    "        path = PROCESSED_DIR / 'records' / Path(rec['path']).name\n",
    "        try:\n",
    "            sig, lbl, meta = load_npz(path)\n",
    "            print('Loaded sample shape', sig.shape, 'label', lbl)\n",
    "        except Exception as e:\n",
    "            errors.append(f'load_npz failed: {e}')\n",
    "\n",
    "# model forward test\n",
    "try:\n",
    "    m = model.to(DEVICE)\n",
    "    m.eval()\n",
    "    dummy = torch.randn(2,1,TARGET_SAMPLES).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = m(dummy)\n",
    "    print('Model forward ok, out shape', out.shape)\n",
    "except Exception as e:\n",
    "    errors.append(f'model forward failed: {e}')\n",
    "\n",
    "if errors:\n",
    "    print('SMOKE TESTS FOUND ISSUES:')\n",
    "    for e in errors:\n",
    "        print('-', e)\n",
    "else:\n",
    "    print('SMOKE TESTS PASSED')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6c584ad1",
   "metadata": {},
   "source": [
    "## Orchestrator: run preprocessing -> build loaders -> train -> evaluate\n",
    "Use this cell to run the full pipeline (careful: preprocessing may take long)"
   ]
  },
  {
   "cell_type": "code",
   "id": "df19e199",
   "metadata": {},
   "source": [
    "# Orchestrator. Set env var ECG_PREPROCESS_LIMIT to test quickly.\n",
    "import os\n",
    "def run_full(limit=None, do_preprocess=True, do_train=True):\n",
    "    if do_preprocess:\n",
    "        print('Run preprocessing cell above or set ECG_PREPROCESS_LIMIT and re-run notebook headless')\n",
    "        # We included a full preprocessing cell earlier; simply re-run the cell if needed.\n",
    "    # Build loaders\n",
    "    try:\n",
    "        tr, va, te = build_loaders(limit=limit)\n",
    "        print('Loaders ready. sizes:', len(tr.dataset), len(va.dataset), len(te.dataset))\n",
    "    except Exception as e:\n",
    "        print('Failed to build loaders:', e)\n",
    "        return\n",
    "    if do_train:\n",
    "        hist = train(model, tr, va, epochs=EPOCHS)\n",
    "        plot_history(hist)\n",
    "    # final eval\n",
    "    rep = evaluate(model, te)\n",
    "    print('Test eval:', rep)\n",
    "    # confusion matrix plot\n",
    "    if 'confusion' in rep:\n",
    "        plot_confusion(rep['confusion'])\n",
    "    return rep\n",
    "\n",
    "# Example usage:\n",
    "# run_full(limit=200, do_preprocess=False, do_train=False)\n",
    "print('Orchestrator ready. To run: run_full(limit=500, do_preprocess=False, do_train=True)')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b4eec90",
   "metadata": {},
   "source": [
    "## Final notes\n",
    "\n",
    "- For a quick smoke run set `ECG_PREPROCESS_LIMIT=5000` in your environment and run the preprocessing cell.\n",
    "- For full production, run headless overnight: `jupyter nbconvert --to notebook --execute notebooks/master_pipeline.ipynb --output logs/preprocess_run.ipynb`\n",
    "- If you want me to generate a variant that uses TFRecords, ONNX export, MLflow logging, or multi-label training — say which and I'll produce it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
